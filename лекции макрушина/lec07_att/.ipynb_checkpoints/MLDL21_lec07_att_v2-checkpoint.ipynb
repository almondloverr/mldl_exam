{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 7: Механизм внимания (attention)\n",
    "\n",
    "__Автор: Сергей Вячеславович Макрушин__ e-mail: SVMakrushin@fa.ru \n",
    "\n",
    "Финансовый универсиет, 2021 г. \n",
    "\n",
    "При подготовке лекции использованы материалы:\n",
    "* https://dlcourse.ai/\n",
    "* https://neerc.ifmo.ru/wiki/index.php?title=Механизм_внимания\n",
    "\n",
    "v 0.1 02.05.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в механизм внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Изначально механизм внимания был представлен в контексте рекуррентных Seq2seq[1] сетей [2] для \"обращения внимания\" блоков декодеров на скрытые состояния RNN для любой итерации энкодера, а не только последней.\n",
    "\n",
    "После успеха этой методики в __машинном переводе__ последовали ее внедрения в:\n",
    "* других задачах обработки естественного языка\n",
    "* задачах генерации описания изображения (в применении к CNN) \n",
    "* в порождающих состязательных сетях (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/seq_to_seq_v1.png\" alt=\"\" style=\"width: 200px;\"/>\n",
    "    <b>Рекуррентные сети Seq2seq (приниципиальная схема)</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/enc_dec_2.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>Пример использования RNN для задачи машинного перевода</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/enc_dec_2.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>Пример использования RNN для задачи машинного перевода</b> <br/>\n",
    "</center>\n",
    "\n",
    "__Специфика__:\n",
    "* Для энкодера и декодера используются различные веса сети (RNN модуля)\n",
    "* Скрытое состояние энкодера на последнем шаге обработки предложения (последовательности) является ключевым т.к. по сути кодирует все предложение. Затем декодер использует именно это состояние для того, чтобы сгенерировать перевод предложения на другом языке.\n",
    "* Результат сэмплинга декодера на предыдущем шаге передается на следующий блок декодера в добавок к внутреннему состянию RNN декодера.\n",
    "\n",
    "__Проблемы__:\n",
    "* Расстояние между местом кодирования слова и местом его декодирования большое.\n",
    "* Последовательность слов в исходном и целевом предложении часто должна быть разная.\n",
    "* Часто количество слов в исходном и целевом предложении различается.\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "__Приемы, направленные на преодоление проблемы__ :\n",
    "\n",
    "* Для языков, в которых последовательность слов в эквивалентных передложениях примерно одинакова, может быть полезно декодеру __гененрировать предложение в обратном порядке__. Это дает возможность иметь небольшую последовательность преобразований (_\"небольшое расстояние\"_ ) от энкодинга слов (последних слов в исходном предложении) до места их декодирования.\n",
    "* Может быть полезно скрытое состояние энкодера на последнем шаге передавать на вход каждому блоку декодера, чтобы каждый декодер имел прямой доступ к энкодингу всего предложения (этот подход тоже сокращает сложность передачи информации об исходном предложение \"сквозь\" последовательность в декодере):\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/enc_dec_3.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>RNN в машинном переводе: использование ключевого состояния энкодера</b> <br/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Базовая архитектура Seq2seq__\n",
    "\n",
    "_Seq2seq_ состоит из двух рекуррентных нейронных сетей (RNN): __Энкодера__ и __Декодера__.\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/enc_dec_1.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "    <b>Рекуррентные сети Seq2seq</b> <br/>\n",
    "</center>\n",
    "\n",
    "* __Энкодер__ - принимает последовательность (например: предложение на языке _A_) и сжимает его в вектор скрытого состояния.\n",
    "    * $x_i$ слова в предложении на языке _A_\n",
    "    * $h_i$ скрытое состояние энкодера\n",
    "    * __блоки энкодера__ (зеленый): получают на вход $x_i$ и передают скрытое состояние $h_i$ на следующую итерацию    \n",
    "* __Декодер__ - выдает слово на языке _B_, принимает последнее скрытое состояние энкодера (для первого слова) / предыдущее скрытое состояние декодера (последующие слова) и предыдущее предсказанное слово.\n",
    "    * $d_i$ скрытое состояние декодера\n",
    "    * $y_i$ слова в предложении на языке _B_\n",
    "    * Блоки декодера (фиолетовый) блоки декодера получающие на вход $y_{i-1}$ или специальный токен `<start>` (в случае первой итерации) и возвращаюшие на следующую итерацию:\n",
    "        * $y_i$ - слова в предложении на языке _B_ \n",
    "        * $d_i$ - скрытое состояние декодера \n",
    "        * Перевод считается завершенным при возвращении $y_i$, равного специальному токену `<end>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Применение механизма внимания для Seq2seq: базовый принцип__\n",
    "\n",
    "* Использование механизма внимания позволяет решать задачу нахождения закономерности между словами находящимися __на большом расстоянии друг от друга__. \n",
    "* LSTM, GRU и аналогичные блоки используются для улучшения передачи информации на большое количество итераций по сравнению с базовыми RNN, несмотря на это сохраняется проблема: влияние предыдущих состояний на текущее уменьшается экспоненциально от расстояния между словами\n",
    "    * В классическом применении RNN результатом является только последнее скрытое состояние $h_m$, где $m$ - длина последовательности входных данных.\n",
    "* __Механизм внимания__ улучшает этот показатель до __линейного__.\n",
    "    * Использование механизма внимания позволяет использовать информацию, полученную не только из последнего скрытого состояния, но и из любого скрытого состояния $h_t$ для любого $t \\in 1 \\dotso m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Устройство слоя механизма внимания__\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_4.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "    <b>Обобщенный механизм внимания в RNN</b> <br/>\n",
    "</center>  \n",
    "\n",
    "1. Слой механизма внимания представляет собой обычную, чаще всего однослойную, нейронную сеть на вход которой подаются $h_t, t = 1 \\ldots m$ (скрытые состояния на последовательных шагах RNN), а также вектор $d$ в котором содержится некий контекст, зависящий от конкретной задачи.\n",
    "    * В случае _Seq2seq_ сетей вектором $d$ будет скрытое состояние предыдущей итерации декодера $d_{i-1}$ .\n",
    "\n",
    "2. Выходом данного слоя будет является вектор $s$ (от _score_ ) - оценки на основании которых на скрытое состояние $h_i$ будет \"обращено внимание\".\n",
    "3. Для нормализации значений $s$ используется $softmax$: $e = softmax(s)$\n",
    "    * $\\forall s\\colon\\  \\sum_{i=1}^n softmax(e)_i = 1$\n",
    "    * $\\forall s,\\ i\\colon \\ softmax(e)_i >= 0 $\n",
    "\n",
    "4. Результатом работы слоя внимания является $c$ (__context vector__) который, содержит в себе информацию обо всех скрытых состояниях $h_i$ пропорционально нормализованному вниманию $e_i$ :\n",
    "\n",
    "    * $с = \\sum_{i=1}^m e_i h_i$\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Применение механизма внимания к базовой Seq2seq архитектуре__\n",
    "\n",
    "При добавлении слоя механизма внимания в базовую архитектуру _Seq2seq_ между RNN _Энкодером_ и _Декодером_ получится следующая схема:\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_5.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "    <b>Пример схемы работы Seq2seq сети с механизмом внимания</b> <br/>\n",
    "</center>  \n",
    "\n",
    "* $x_i, h_i, d_i, y_i$ имеют те же назначения, что и в базовом варианте без механизма внимания.\n",
    "* _Агрегатор скрытых состояний энкодера (желтый)_ - агрегирует всю последовательность векторов $h = [h_1, h_2, h_3, h_4]$.\n",
    "* _Блоки механизма внимания (красный)_ - принимают $h$ и $d_{i - 1}$ (_контекст для выбора параметров_ для блока $i$-го блока декодера (сиреневый цвет) ), возвращает $c_i$ - контекст для $i$-го блока декодера\n",
    "* _Блоки декодера (фиолетовый)_ - по сравнению с обычной _Seq2seq_ сетью меняются входные данные. Теперь, вместо $y_{i-1}$  на $i$ итерации на вход подается: конкатенация $y_{i-1}$ и $c_i$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи механизма внимания достигается __\"фокусирование\" декодера__ на определенных скрытых состояниях энкодера, на любом из шагов энкодера. \n",
    "* В случаях машинного перевода эта возможность помогает декодеру предсказывать на какие скрытые состояния при исходных определенных словах на языке _A_ необходимо обратить больше внимания при генерации очередного слова перевода на язык _B_.\n",
    "    * При этом расстояние (в шагах последовательности) между генерируемым словом и влияющими на него словами из исходной последовательности никак не влияют на возможность использование скрытых состояний, сформированных под их воздействием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/att_6.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "    <b>Пример весов механизма внимания при решении задачи машинного перевода</b> <br/>\n",
    "</center>  \n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_7.png\" alt=\"\" style=\"width: 450px;\"/>\n",
    "    <b>Эффект использования механизма внимания при решении задачи машинного перевода для предложений разной длины</b> <br/>\n",
    "</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Использование механизмов внимания для других задач__\n",
    "\n",
    "Задача _Image Captioning_ : на вход подается изображение, а на выходе создается текстовое предложение, описывающее визуальное содержание картинки.\n",
    "* Классическая нейростевая модель для решения задачи Image Captioning состоит из:\n",
    "    * сверточной нейронной сети (CNN) для извлечения визуальных характеристик из картинки\n",
    "    * и рекуррентной нейронной сети (RNN) для перевода результата работы сверточной сети в текст.\n",
    "    \n",
    "* __Сверточный модуль внимания__ (сonvolutional block attention module) — модуль внимания для сверточных нейросетей. Применяется для задач детектирования обьектов на изображениях и классификации с входными данными больших размерностей. \n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_8.png\" alt=\"\" style=\"width: 450px;\"/>\n",
    "    <b>Логика работы архитектуры, использующей attention model для задачи Image Captioning</b> <br/>\n",
    "</center>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_9.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "    <b>Пример использования attention model для генерации существительных в описании картинок</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Архитектура Transformer__\n",
    "\n",
    "* Рассматриваем задачу машинного перевода _Seq2seq_\n",
    "* новая архитектура для решения этой задачи, которая не является ни RNN, ни CNN\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_12.png\" alt=\"\" style=\"width: 250px;\"/>\n",
    "    <b>Принципиальная архитектуры модели Transformer из статьи \"Attention Is All You Need\"*</b> <br/>\n",
    "</center>\n",
    "\n",
    "\\*https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Encoder, Multi-head attention__\n",
    "\n",
    "__Multi-head attention__ - слой, который дает возможность каждому входному вектору (эмбеддингу слова) взаимодействовать с любыми другими словами предложения через attention mechanism, вместо:\n",
    "* последовательной передачи hidden state, как в RNN\n",
    "* свертки эмбеддингов соседних слов, как в CNN\n",
    "    \n",
    "<center>         \n",
    "    <img src=\"./img/text_2.png\" alt=\"\" style=\"width: 450px;\"/>\n",
    "    <b>Пример иерархическая структуры предложения </b> <br/>\n",
    "</center>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_11.png\" alt=\"\" style=\"width: 250px;\"/>\n",
    "    <b>Организация Multi-head attention</b> <br/>\n",
    "</center>    \n",
    "\n",
    "В Multi-head attention:\n",
    "1. на вход даются вектора Query (Q), и несколько пар Key (K) и Value (V) (на практике, Key и Value это всегда один и тот же вектор)\n",
    "2. каждый из них преобразуется обучаемым линейным преобразованием\n",
    "3. вычисляется скалярное произведение Q со всеми K по очереди\n",
    "4. результат этих скалярных произведений прходит через softmax\n",
    "5. с полученными весами все вектора V суммируются в единый вектор\n",
    "6. __Ключевое отличие от классического attention__: результат всех этих параллельных attention'ов (на картинке их $h$) конкатенируется, прогоняется через обучаемое линейное преобразование и идет на выход (получается вектор того же размера, что и каждый из входов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенности:\n",
    "\n",
    "* Q: в чем смысл множества одинаковых attention слоев?\n",
    "    * Логика стандартного attention: сеть пытается выдать соответствие одного слова другому в предложении, если они близки чем-то\n",
    "    * Не обучатся ли разныеслои  Multi-head attention в точности одному и тому же?\n",
    "        * Нет. Технически, разных начальных случайных весов хватает, чтобы толкать разные слои в разные стороны.\n",
    "        * Разные слои позволяют сети обращать внимание на различные аспекты слов (например семантические (смысловые) и грамматические аспекты слов)\n",
    "\n",
    "* Так как на выход такой блок выдает вектор того же размера, что и был на входе, то этот блок можно вставлять в сеть несколько раз, добавляя сети глубину (в рассмотренной работе этот блок использовался с глубиной 6).\n",
    "* Одной из фич каждого слова является __positional encoding__ — т.е. его позиция в предложении. Например, в процессе обработки слова это позволяет легко \"обращать внимание\" на соседние слова, если они важны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/att__.gif\" alt=\"\" style=\"width: 450px;\"/>\n",
    "    <b>Пример работы архитектуры Transformer </b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Декодер__\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_13.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "    <b>Организация декодера</b> <br/>\n",
    "</center>\n",
    "\n",
    "* декодер запускается по слову за раз, получает на вход:\n",
    "    * вектора всех исходных слов, полученные при энкодинге\n",
    "    * все прошлые слова и должен выдать следующее (на первой итерации получает специальный токен <start>).\n",
    "* соответственно, декодер состоит из двух блоков:\n",
    "    * первый — работает с векторами предыдущих декодированных слов, аналогичный использованному в процессе encoding (но может обращаться не ко всем слвам, а только к уже декодированным).\n",
    "    * второй — работает с выходом энкодера. B этом случае Query — это вектор входа в декодере, а пары Key/Value — это финальные эмбеддинги энкодера\n",
    "* все это снова повторяется 6 раз (глубокое обучение!), где выход предыдущего блока идет на вход следующему\n",
    "* в конце сети стоит softmax, который выдает вероятности слов. Сэмплирование из него и есть результат, то есть следующее слово в предложении.\n",
    "* Результат семплирования подается на вход следующему запуску декодера и процесс повторяется, пока декодер не выдаст токен `<EOS>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Преимущества архитектуры Transformer__\n",
    "\n",
    "(над RNN и CNN)\n",
    "\n",
    "* Все слова в предложении одинаково доступны при кодировании/декодировании (расстояние не важно)\n",
    "* Может эффективно выполняться параллельно\n",
    "* Эффективная глубина сети меньше по сравнению с RNN (RNN \"разматывается\" вдоль последовательности), в Transformer глубина графа не зависит от длины последовательности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BERT__\n",
    "\n",
    "__Bidirectional Encoder Representations from Transformers (BERT)__ - языковая модель, основанная на архитектуре трансформер, предназначенная для предобучения языковых представлений с целью их последующего применения в широком спектре задач обработки естественного языка. BERT был создан и опубликован в 2018 году Якобом Девлином и его коллегами из Google.\n",
    "* BERT является автокодировщиком\n",
    "* BERT использует трансформер-архитектуру\n",
    "    * В каждом слое кодировщика применяется двустороннее внимание, что позволяет учитывать контекст с обеих сторон от рассматриваемого токена\n",
    "* В каждом слое кодировщика применяется двустороннее внимание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "__Языковое моделирование__ (language modelling, LM) - использование различных статистических и вероятностных методов для определения вероятности того, что данная последовательность слов встречается в предложении.\n",
    "* Для обеспечения достоверности предсказаний языковые модели строятся на основе анализа массивов текстовых данных. \n",
    "* Языковые модели используются в приложениях обработки естественного языка (natural language processing, NLP). В частности:\n",
    "   * Задачах машинного перевода.\n",
    "   * Задачах ответа на вопросы (question answering).\n",
    "   * Других задачах, особенно, требующих генерацию текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "       \n",
    "__Автокодировщик__ (autoencoder) — специальная архитектура искусственных нейронных сетей, позволяющая применять __обучение без учителя__ с использованием метода обратного распространения ошибки. \n",
    "\n",
    "Автокодировщик состоит из двух частей:\n",
    "* __энкодера__ $g$, он переводит входной сигнал в его представление (код): $h = g(x)$\n",
    "* __декодера__ $f$, он восстанавливает сигнал по его коду: $x = f(h)$\n",
    "\n",
    "При этом семейства функций энкодера $g$ и декодера $f$ определенным образом ограничены, чтобы автоэнкодер был вынужден отбирать наиболее важные свойства сигнала.\n",
    "\n",
    "Автокодировщик, изменяя (обучая) $f$ и $g$, стремится выучить тождественную функцию $x = f(g(x))$, минимизируя заданный функционал ошибки: $L(x, f(g(x)))$.\n",
    "\n",
    "Автокодировщик можно использовать:\n",
    "* для предобучения, например, когда стоит задача классификации, а размеченных пар слишком мало\n",
    "* понижения размерности в данных, например для последующей визуализации\n",
    "* когда надо научиться различать полезные свойства входного сигнала       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Обучение модели BERT__\n",
    "\n",
    "BERT-модель __предварительно обучена без учителя__ на на двух NLP-задачах:\n",
    "* генерации пропущенного токена (masked language modeling)\n",
    "    * На вход BERT подаются токенизированные пары предложений, в которых __некоторые токены скрыты__\n",
    "    * Модель обучается генерировать пропущенные токены (слова или отдельные токены)\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_14.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "    <b>Логика задачи генерации пропущенного токена</b> <br/>\n",
    "</center>\n",
    "\n",
    "* предсказание следующего предложения:\n",
    "    * Задача же предсказания следующего предложения есть задача бинарной классификации — является ли второе предложение продолжением первого\n",
    "    * Благодаря этой задаче сеть обучается различать связь между предложениями в тексте\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/att_15.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "    <b>Логика задачи генерации пропущенного токена</b> <br/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/att_16.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "    <b>Представление входных данных модели</b> <br/>\n",
    "</center>\n",
    "\n",
    "* Токенизация в BERT: \n",
    "    * Токенами служат слова, доступные в словаре, или их составные части\n",
    "    * Если слово отсутствует в словаре, оно разбивается на части, которые в словаре присутствуют\n",
    "    * Словарь является составляющей модели\n",
    "        * Например в BERT-Base используется словарь примерно из 30K слов. \n",
    "* В нейронной сети BERT __токены кодируются своими эмбеддингами__, а именно: соединяются представления самого токена (предобученные), номера его предложения, а также позиции токена внутри своего предложения.\n",
    "\n",
    "* Предобучение проведено на больших корпусах текстов (есть многоязычные варианты модели BERT).\n",
    "    * В частности, многоязыковая модель BERT-Base, поддерживающая 104 языка, состоит из 12 слоев, 768 узлов, 12 выходов и 110M параметров\n",
    "    \n",
    "* Предобученные модели можно подстраивать под решение конкретных NLP-задач:\n",
    "    * Дообучаем модель на данных, специфичных задаче: знание языка уже получено на этапе предобучения языковой модели, необходима лишь коррекция сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Данные и оценка качества__\n",
    "\n",
    "Исходные данные: Предобучение ведётся на текстовых данных корпуса BooksCorpus (800 млн. слов), а также на текстах англоязычной Википедии (2.5 млрд. слов). \n",
    "\n",
    "\n",
    "Качество модели авторы оценивают на популярном для обучения моделей обработки естественного языка наборе задач GLUE ( https://gluebenchmark.com/tasks )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Гиперпараметры модели__\n",
    "\n",
    "Гиперпараметрами модели являются:\n",
    "* $H$ — размерность скрытого пространства кодировщика\n",
    "* $L$ — количество слоёв-кодировщиков\n",
    "* $A$ — количество голов в механизме внимания.\n",
    "\n",
    "\n",
    "В репозитории Google Research доступны для загрузки и использования несколько вариантов обученной сети в формате контрольных точек обучения модели популярного фреймворка TensorFlow[5]. В таблице в репозитории приведено соответствие параметров L и H и моделей. Использование моделей с малыми значениями гиперпараметров на устройствах с меньшей вычислительной мощностью позволяет сохранять баланс между производительностью и потреблением ресурсов. Также представлены модели с различным типом скрытия токенов при обучении, доступны два варианта: скрытие слова целиком (англ. whole word masking) или скрытие составных частей слов (англ. WordPiece masking).\n",
    "\n",
    "Также модель доступна для использования с помощью популярной библиотеки PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Точная настройка (Fine-tuning)__\n",
    "\n",
    "Этот этап обучения зависит от задачи, и выход сети, полученной на этапе предобучения, может использоваться как вход для решаемой задачи. \n",
    "\n",
    "_Пример:_ если решаем задачу построения вопросно-ответной системы, можем использовать в качестве ответа последовательность токенов, следующую за разделителем предложений. В общем случае дообучаем модель на данных, специфичных задаче: знание языка уже получено на этапе предобучения, необходима лишь коррекция сети.\n",
    "\n",
    "Интерпретация этапа fine-tuning — обучение решению конкретной задачи при уже имеющейся общей модели языка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример использования__\n",
    "\n",
    "Пример предказания пропущенного токена при помощи BERT в составе PyTorch. Скрытый токен — первое слово второго предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка токенизатора и входные данные\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n",
    "text_1 = \"Who was Jim Henson ?\"\n",
    "text_2 = \"Jim Henson was a puppeteer\"\n",
    "\n",
    "# Токенизация ввода, также добавляются специальные токены начала и конца предложения.\n",
    "indexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Конвертирвание ввода в формат тензоров PyTorch\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "\n",
    "# Выбираем токен, который будет скрыт и позднее предсказан моделью\n",
    "masked_index = 8\n",
    "indexed_tokens[masked_index] = tokenizer.mask_token_id\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Загрузка модели\n",
    "masked_lm_model = torch.hub.load('huggingface/pytorch-transformers', 'modelWithLMHead', 'bert-base-cased')\n",
    "predictions = masked_lm_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "\n",
    "# Предсказание скрытого токена\n",
    "predicted_index = torch.argmax(predictions[0][0], dim=1)[masked_index].item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'Jim'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
