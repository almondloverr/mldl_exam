{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 6: Рекуррентные нейронные сети RNN  (Recurrent Neural Networks)\n",
    "\n",
    "__Автор: Сергей Вячеславович Макрушин__ e-mail: SVMakrushin@fa.ru \n",
    "\n",
    "Финансовый универсиет, 2021 г. \n",
    "\n",
    "При подготовке лекции использованы материалы:\n",
    "* ...\n",
    "\n",
    "v 0.1 15.04.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделы: <a class=\"anchor\" id=\"разделы\"></a>\n",
    "* [раздел 1](#загрузка)\n",
    "* [раздел 2](#нормализация)\n",
    "-\n",
    "\n",
    "* [к оглавлению](#разделы)\n",
    "\n",
    "---\n",
    "## Нормализация <a class=\"anchor\" id=\"нормализация\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "﻿<style>\r\n",
       "\r\n",
       "\r\n",
       "b.n {\r\n",
       "    font-weight: normal;        \r\n",
       "}\r\n",
       "\r\n",
       "b.grbg {\r\n",
       "    background-color: #a0a0a0;      \r\n",
       "}\r\n",
       "\r\n",
       "b.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "b.b {    \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "b.g {\r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "// add your CSS styling here\r\n",
       "\r\n",
       "list-style: none;\r\n",
       "\r\n",
       "ul.s {\r\n",
       "//    list-style-type: none;\r\n",
       "    list-style: none;\r\n",
       "//    background-color: #ff0000;  \r\n",
       "//    color: #ffff00;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;\r\n",
       "}\r\n",
       "\r\n",
       "li.t {\r\n",
       "    list-style: none;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "*.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "li.t:before {\r\n",
       "    content: \"\\21D2\";    \r\n",
       "//    content: \"►\";\r\n",
       "//    padding-left: -1.2em;    \r\n",
       "    text-indent: -1.2em;    \r\n",
       "    display: block;\r\n",
       "    float: left;\r\n",
       "    \r\n",
       "    \r\n",
       "//    width: 1.2em;\r\n",
       "//    color: #ff0000;\r\n",
       "}\r\n",
       "\r\n",
       "i.m:before {\r\n",
       "    font-style: normal;    \r\n",
       "    content: \"\\21D2\";  \r\n",
       "}\r\n",
       "i.m {\r\n",
       "    font-style: normal; \r\n",
       "}    \r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "/* em {\r\n",
       "    font-style: normal; \r\n",
       "} */\r\n",
       "\r\n",
       "\r\n",
       "em.bl {\r\n",
       "    font-style: normal;     \r\n",
       "    font-weight: bold;        \r\n",
       "}\r\n",
       "\r\n",
       "/* em.grbg {\r\n",
       "    font-style: normal;         \r\n",
       "    background-color: #a0a0a0;      \r\n",
       "} */\r\n",
       "\r\n",
       "em.cr {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cb {    \r\n",
       "    font-style: normal;         \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cg {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "\r\n",
       "em.qs {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.qs::before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #ff0000;    \r\n",
       "    content: \"Q:\";  \r\n",
       "}\r\n",
       "\r\n",
       "em.an {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.an:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"A:\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.nt {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.nt:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Note:\";  \r\n",
       "}    \r\n",
       "    \r\n",
       "em.ex {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.ex:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #00ff00;    \r\n",
       "    content: \"Ex:\";  \r\n",
       "} \r\n",
       "    \r\n",
       "em.df {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.df:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Def:\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.pl {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.pl:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"+\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.mn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.mn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"-\";  \r\n",
       "}        \r\n",
       "\r\n",
       "em.plmn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.plmn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\00B1\";\\\\\"&plusmn;\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.hn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.hn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\21D2\";\\\\\"&rArr;\";  \r\n",
       "}     \r\n",
       "    \r\n",
       "\r\n",
       "#cssTableCenter td, th \r\n",
       "{\r\n",
       "    text-align: center; \r\n",
       "    vertical-align: middle;\r\n",
       "}\r\n",
       "\r\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем стиль для оформления презентации\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"file:./lec_v2.css\")\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Базовые подходы к RNN <a class=\"anchor\" id=\"установка\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Мотивация для использования рекуррентных нейронных сетей__\n",
    "\n",
    "* Не рекуррентные архитектуры ИНС получают на вход вектор данных и пытаются по нему предсказать тот или иной результат (минимизировать ошибку). \n",
    "* Важно, что входной (и выходной) вектор должен __иметь одну и ту же размерность__. Во многих задачах это не так.\n",
    "\n",
    "<br/>\n",
    "<center>         \n",
    "    <img src=\"./img/rnn_1.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "    <b>Задачи с последовательностями</b> <br/>\n",
    "    Каждый прямоугольник представляет собой вектор, а стрелки представляют функции (например, умножение матриц). Входные векторы показаны красным, выходные векторы - синим, а зеленые векторы содержат состояние RNN.    \n",
    "</center>\n",
    "\n",
    "1. один вход, один выход (классический случай)\n",
    "* один вход, последовательность выходов\n",
    "* последовательность входов, один выход\n",
    "* последовательность входов, затем последовательность выходов\n",
    "* синхронизированные последовательности входов и выходов    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Рекуррентная нейронная сеть (RNN)__\n",
    "\n",
    "<em class=\"df\"></em> __Рекуррентная нейронная сеть (RNN)__ - это класс искусственных нейронных сетей, в которых узел может получать входы не только от других узлов и текущих входных данных но и выходы узлов, полученные при рассмотрении предыдущих входных данных последовательности.\n",
    "* обмен вектором внутреннего сосотояния, полученного на предыдущем шаге, позволяет использовать информацию о предыдущих шагах, которые сеть уже обработала\n",
    "* при рассмотрении всей последовательности веса каждого узла одни и те же при рассмотрении всех входных данных последовательности\n",
    "\n",
    "\n",
    "* Ткая архитектура сети позволяет обрабатывать серии событий во времени или последовательные пространственные цепочки произвольной размерности\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/rnn_3.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>Принцип устройства узла RNN</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трудность рекуррентной сети:\n",
    "* если учитывать каждый шаг времени, то становится необходимым для каждого шага времени (последовательности) создавать свой слой нейронов, что создает серьёзные вычислительные сложности\n",
    "* многослойные реализации вычислительно неустойчивы: в них как правило либо исчезают либо зашкаливают веса\n",
    "* если ограничить расчёт фиксированным временным окном, то полученные модели не будут отражать долгосрочных трендов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/rnn_2.png\" alt=\"\" style=\"width: 200px;\"/>\n",
    "    <b>Распространение ошибок в узле RNN</b> <br/>\n",
    "</center>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/rnn_5.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "    <b>Распространение ошибок в узле RNN</b> <br/>\n",
    "</center>\n",
    "\n",
    "* $a_t = b + Ws_{t−1} + Ux_t$\n",
    "* $s_t = f(a_t)$\n",
    "* $o_t=c+Vs_t$\n",
    "* $y_t=h(o_t)$\n",
    "\n",
    "Где:\n",
    "* $f$ — это нелинейность рекуррентной сети (обычно $\\sigma$, $\\tanh$ или $ReLU$)\n",
    "* $h$ — функция, с помощью которой получается ответ (например, $softmax$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Распространение ошибки в архитектуре RNN__\n",
    "\n",
    "* В прямой нейронной сети ошибка на конкретном нейроне вычисляется как функция от ошибок нейронов, которые используют его выходное значение <em class=\"hn\"></em> формируется ациклический графы вычислений:\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/ann_19.png\" alt=\"Прямой и обратный проход \" style=\"width: 300px;\"/>\n",
    "    <b>Прямой и обратный проход процедуры обучения многослойной ИНС </b> <br/>    \n",
    "</center>\n",
    "\n",
    "* В архитектуре RNN нейрон принимает в качестве входа результат вычисления в нем самом (через вектор состояния)\n",
    "    * Важно понимать, что при этом петли в графе вычислений не образуется \n",
    "    * Вычисления, которые делает рекуррентная сеть, можно развернуть обратно до начала обрабатываемой последовательности\n",
    "    * Можно сказать, что на каждом шаге обрабатываемой последовательности сеть создает копии самой себя\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/rnn_3.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>Принцип устройства узла RNN </b> <br/>\n",
    "</center>\n",
    "\n",
    "* И на каждом последовательности мы фактически обучаем глубокую нейронную сеть, в которой столько слоев, сколько элементов в последовательности на данный момент мы уже видели\n",
    "* Рекуррентная сеть — разворчиватся вдоль элементов последовательности $1 \\dotsc T$ в очень-очень многоуровневую обычную сеть, в которой одни и те же веса переиспользуются на каждом уровне. \n",
    "    * <em class=\"pl\"></em> Для хранения весов достаточно одной матрицы\n",
    "    * <em class=\"pl\"></em> Градиенты по весам не затухают до нуля сразу же (как это бывает в обычных глубких сетях)\n",
    "    * <em class=\"mn\"></em> Если матрица весов меняет норму вектора градиента при проходе через один «слой» обратного распространения, то при проходе через T слоев эта норма изменяется экспоненциально (т.к. веса матрицы одни и те же) это приводит:\n",
    "        * к __\"взрыу градиентов\"__ (exploding gradients), если матрица заметно увеличивает норму вектора градиента\n",
    "        * к экспоненациональному затуханию градиентов (Vanishing gradients), если матрица заметно уменьшает норму вектора градиента\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/rnn_4.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>Распространение ошибок в узле RNN</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Пример реализации RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорты:\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Входные данные:\n",
    "\n",
    "text = ['hey how are you','good i am fine','have a nice day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('name2.csv', encoding='cp1251') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)    \n",
    "    text = list(n[0].lower() for n in csv_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['агафья', 'аглая', 'агния']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодировка символов числами:\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text)+' ')\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'д': 0, 'р': 1, 'у': 2, 'ю': 3, 'т': 4, 'ф': 5, 'з': 6, 'ж': 7, 'к': 8, ' ': 9, 'е': 10, 'ь': 11, 'м': 12, 'н': 13, 'ц': 14, 'а': 15, 'л': 16, 'я': 17, 'ё': 18, 'о': 19, 'б': 20, 'и': 21, 'п': 22, 'г': 23, 'с': 24, 'в': 25}\n"
     ]
    }
   ],
   "source": [
    "print(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest string has 10 characters\n"
     ]
    }
   ],
   "source": [
    "# Выравнивание всех строк до фиксированной (максимальной) длины:\n",
    "\n",
    "maxlen = len(max(text, key=len))\n",
    "print(\"The longest string has {} characters\".format(maxlen))\n",
    "\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n",
    "# the length of the longest sentence\n",
    "\n",
    "for i in range(len(text)):\n",
    "    while len(text[i])<maxlen:\n",
    "        text[i] += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: агафья   \n",
      "Target Sequence: гафья    \n",
      "Input Sequence: аглая    \n",
      "Target Sequence: глая     \n",
      "Input Sequence: агния    \n",
      "Target Sequence: гния     \n",
      "Input Sequence: агриппина\n",
      "Target Sequence: гриппина \n",
      "Input Sequence: акулина  \n",
      "Target Sequence: кулина   \n",
      "Input Sequence: алевтина \n",
      "Target Sequence: левтина  \n",
      "Input Sequence: александр\n",
      "Target Sequence: лександра\n",
      "Input Sequence: алина    \n",
      "Target Sequence: лина     \n",
      "Input Sequence: алла     \n",
      "Target Sequence: лла      \n",
      "Input Sequence: анастасия\n",
      "Target Sequence: настасия \n",
      "Input Sequence: ангелина \n",
      "Target Sequence: нгелина  \n",
      "Input Sequence: анжела   \n",
      "Target Sequence: нжела    \n",
      "Input Sequence: анжелика \n",
      "Target Sequence: нжелика  \n",
      "Input Sequence: анна     \n",
      "Target Sequence: нна      \n",
      "Input Sequence: антонина \n",
      "Target Sequence: нтонина  \n",
      "Input Sequence: анфиса   \n",
      "Target Sequence: нфиса    \n",
      "Input Sequence: валентина\n",
      "Target Sequence: алентина \n",
      "Input Sequence: валерия  \n",
      "Target Sequence: алерия   \n",
      "Input Sequence: варвара  \n",
      "Target Sequence: арвара   \n",
      "Input Sequence: василиса \n",
      "Target Sequence: асилиса  \n",
      "Input Sequence: вера     \n",
      "Target Sequence: ера      \n",
      "Input Sequence: вероника \n",
      "Target Sequence: ероника  \n",
      "Input Sequence: виктория \n",
      "Target Sequence: иктория  \n",
      "Input Sequence: галина   \n",
      "Target Sequence: алина    \n",
      "Input Sequence: глафира  \n",
      "Target Sequence: лафира   \n",
      "Input Sequence: гликерия \n",
      "Target Sequence: ликерия  \n",
      "Input Sequence: дана     \n",
      "Target Sequence: ана      \n",
      "Input Sequence: дарья    \n",
      "Target Sequence: арья     \n",
      "Input Sequence: евгения  \n",
      "Target Sequence: вгения   \n",
      "Input Sequence: евдокия  \n",
      "Target Sequence: вдокия   \n",
      "Input Sequence: евлалия  \n",
      "Target Sequence: влалия   \n",
      "Input Sequence: евлампия \n",
      "Target Sequence: влампия  \n",
      "Input Sequence: евпраксия\n",
      "Target Sequence: впраксия \n",
      "Input Sequence: евфросини\n",
      "Target Sequence: вфросиния\n",
      "Input Sequence: екатерина\n",
      "Target Sequence: катерина \n",
      "Input Sequence: елена    \n",
      "Target Sequence: лена     \n",
      "Input Sequence: елизавета\n",
      "Target Sequence: лизавета \n",
      "Input Sequence: епистима \n",
      "Target Sequence: пистима  \n",
      "Input Sequence: ермиония \n",
      "Target Sequence: рмиония  \n",
      "Input Sequence: жанна    \n",
      "Target Sequence: анна     \n",
      "Input Sequence: зинаида  \n",
      "Target Sequence: инаида   \n",
      "Input Sequence: злата    \n",
      "Target Sequence: лата     \n",
      "Input Sequence: зоя      \n",
      "Target Sequence: оя       \n",
      "Input Sequence: инга     \n",
      "Target Sequence: нга      \n",
      "Input Sequence: инесса   \n",
      "Target Sequence: несса    \n",
      "Input Sequence: инна     \n",
      "Target Sequence: нна      \n",
      "Input Sequence: иоанна   \n",
      "Target Sequence: оанна    \n",
      "Input Sequence: ираида   \n",
      "Target Sequence: раида    \n",
      "Input Sequence: ирина    \n",
      "Target Sequence: рина     \n",
      "Input Sequence: ия       \n",
      "Target Sequence: я        \n",
      "Input Sequence: капитолин\n",
      "Target Sequence: апитолина\n",
      "Input Sequence: карина   \n",
      "Target Sequence: арина    \n",
      "Input Sequence: каролина \n",
      "Target Sequence: аролина  \n",
      "Input Sequence: кира     \n",
      "Target Sequence: ира      \n",
      "Input Sequence: клавдия  \n",
      "Target Sequence: лавдия   \n",
      "Input Sequence: ксения   \n",
      "Target Sequence: сения    \n",
      "Input Sequence: лада     \n",
      "Target Sequence: ада      \n",
      "Input Sequence: лариса   \n",
      "Target Sequence: ариса    \n",
      "Input Sequence: лидия    \n",
      "Target Sequence: идия     \n",
      "Input Sequence: лилия    \n",
      "Target Sequence: илия     \n",
      "Input Sequence: любовь   \n",
      "Target Sequence: юбовь    \n",
      "Input Sequence: людмила  \n",
      "Target Sequence: юдмила   \n",
      "Input Sequence: маргарита\n",
      "Target Sequence: аргарита \n",
      "Input Sequence: марина   \n",
      "Target Sequence: арина    \n",
      "Input Sequence: мария    \n",
      "Target Sequence: ария     \n",
      "Input Sequence: марфа    \n",
      "Target Sequence: арфа     \n",
      "Input Sequence: матрёна  \n",
      "Target Sequence: атрёна   \n",
      "Input Sequence: милица   \n",
      "Target Sequence: илица    \n",
      "Input Sequence: мирослава\n",
      "Target Sequence: ирослава \n",
      "Input Sequence: надежда  \n",
      "Target Sequence: адежда   \n",
      "Input Sequence: наталья  \n",
      "Target Sequence: аталья   \n",
      "Input Sequence: нина     \n",
      "Target Sequence: ина      \n",
      "Input Sequence: нонна    \n",
      "Target Sequence: онна     \n",
      "Input Sequence: оксана   \n",
      "Target Sequence: ксана    \n",
      "Input Sequence: октябрина\n",
      "Target Sequence: ктябрина \n",
      "Input Sequence: олимпиада\n",
      "Target Sequence: лимпиада \n",
      "Input Sequence: ольга    \n",
      "Target Sequence: льга     \n",
      "Input Sequence: павлина  \n",
      "Target Sequence: авлина   \n",
      "Input Sequence: пелагея  \n",
      "Target Sequence: елагея   \n",
      "Input Sequence: пинна    \n",
      "Target Sequence: инна     \n",
      "Input Sequence: полина   \n",
      "Target Sequence: олина    \n",
      "Input Sequence: прасковья\n",
      "Target Sequence: расковья \n",
      "Input Sequence: рада     \n",
      "Target Sequence: ада      \n",
      "Input Sequence: раиса    \n",
      "Target Sequence: аиса     \n",
      "Input Sequence: римма    \n",
      "Target Sequence: имма     \n",
      "Input Sequence: светлана \n",
      "Target Sequence: ветлана  \n",
      "Input Sequence: серафима \n",
      "Target Sequence: ерафима  \n",
      "Input Sequence: снежана  \n",
      "Target Sequence: нежана   \n",
      "Input Sequence: софия    \n",
      "Target Sequence: офия     \n",
      "Input Sequence: таисия   \n",
      "Target Sequence: аисия    \n",
      "Input Sequence: тамара   \n",
      "Target Sequence: амара    \n",
      "Input Sequence: татьяна  \n",
      "Target Sequence: атьяна   \n",
      "Input Sequence: улита    \n",
      "Target Sequence: лита     \n",
      "Input Sequence: ульяна   \n",
      "Target Sequence: льяна    \n",
      "Input Sequence: урсула   \n",
      "Target Sequence: рсула    \n",
      "Input Sequence: фаина    \n",
      "Target Sequence: аина     \n",
      "Input Sequence: феврония \n",
      "Target Sequence: еврония  \n",
      "Input Sequence: фёкла    \n",
      "Target Sequence: ёкла     \n",
      "Input Sequence: феодора  \n",
      "Target Sequence: еодора   \n",
      "Input Sequence: целестина\n",
      "Target Sequence: елестина \n",
      "Input Sequence: юлия     \n",
      "Target Sequence: лия      \n",
      "Input Sequence: яна      \n",
      "Target Sequence: на       \n",
      "Input Sequence: ярослава \n",
      "Target Sequence: рослава  \n"
     ]
    }
   ],
   "source": [
    "# Подготовка входных и выходных последовательностей:\n",
    "\n",
    "input_seq = [] # исходная последовательность\n",
    "target_seq = [] # последовательность, смещенная на 1 (без первого символа)\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # Remove firsts character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    \n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot энкодер (для батча примеров фиксированной длины)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(input_seq, dict_size, seq_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение входного и выходного тензора:\n",
    "# batch_size x seq_len x dict_size ; seq_len = maxlen - 1\n",
    "\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "# Symbol seq to int seq:\n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "input_seq = torch.from_numpy(input_seq)\n",
    "# print(f\"Input shape: {input_seq.shape} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\", input_seq)\n",
    "\n",
    "# --- --- --- ---\n",
    "# target_seq = one_hot_encode(target_seq, dict_size, seq_len, batch_size)\n",
    "target_seq = torch.Tensor(target_seq) # from list; without one-hot encoding\n",
    "\n",
    "# print(f\"Target shape: {input_seq.shape} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\", target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = False # torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Модель__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.RNN(*args, **kwargs)`\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$$ h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh}) $$\n",
    "\n",
    "Shape:\n",
    "\n",
    "_Input:_\n",
    "\n",
    "* __input__ of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence.\n",
    "* __h_0__ of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "\n",
    "_Output:_\n",
    "* __output__ of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.\n",
    "* __h_n__ of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
    "\n",
    "Parameters:\n",
    "* `input_size` -  The number of expected features in the input `x`\n",
    "* `hidden_size` - The number of features in the hidden state `h`\n",
    "* `num_layers` - Number of recurrent layers. E.g., setting ``num_layers=2`` would mean stacking two RNNs together to form a `stacked RNN`, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
    "* `nonlinearity` - The non-linearity to use. Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``\n",
    "* `bias` - If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`. Default: ``True``\n",
    "* `batch_first` - If ``True``, then the input and output tensors are provided as `(batch, seq, feature)`. Default: ``False``\n",
    "* `dropout` - If non-zero, introduces a `Dropout` layer on the outputs of each RNN layer except the last layer, with dropout probability equal to `dropout`. Default: 0\n",
    "* `bidirectional` - If ``True``, becomes a bidirectional RNN. Default: ``False``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "#         print('d1: ', out.size())\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "#         print('d2: ', out.size())\n",
    "        out = self.fc(out)\n",
    "#         print('d3: ', out.size())\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем инстанс модели с установленными гиперпараметрами\n",
    "\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = model(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_seq.view(-1), target_seq.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 2.7077\n",
      "Epoch: 20/100............. Loss: 2.1426\n",
      "Epoch: 30/100............. Loss: 1.9227\n",
      "Epoch: 40/100............. Loss: 1.7293\n",
      "Epoch: 50/100............. Loss: 1.5687\n",
      "Epoch: 60/100............. Loss: 1.4620\n",
      "Epoch: 70/100............. Loss: 1.3818\n",
      "Epoch: 80/100............. Loss: 1.3205\n",
      "Epoch: 90/100............. Loss: 1.2726\n",
      "Epoch: 100/100............. Loss: 1.2330\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "input_seq = input_seq.to(device)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq = input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    \n",
    "    output = output.to(device)    \n",
    "    target_seq = target_seq.to(device)\n",
    "    \n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "#     loss = criterion(output.view(-1), target_seq.view(-1).long())\n",
    "    \n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    # TODO: select top_k\n",
    "    \n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    \n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ивана                                        '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 45, 'иван')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Построение многослойных нейронных сетей на базе архитектуры RNN__\n",
    "\n",
    "\n",
    "* Рассмотрим всю рекуррентную сеть как слой и используем ее выходы как входы для следующего рекуррентного слоя.\n",
    "* Мотивация: каждый слой действует в своем собственном «масштабе времени», примерно как каждый слой сверточной сети действует в своем масштабе, на свой размер окна входов. \n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/rnn_7.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>Распространение ошибок в узле RNN</b> <br/>\n",
    "</center>\n",
    "\n",
    "\n",
    "__Двунаправленные рекуррентные сети__ (bidirectional RNN): для входной последовательности запустим RNN (обычно с разными весами) два раза: один слой будет __читать последовательность слева направо__, а другой — __справа налево__\n",
    "\n",
    "* Матрицы весов для двух направлений абсолютно независимы и между ними нет взаимодействия\n",
    "* Ограничение: данный подход возможен только для последовательностей, которые даны сразу целиком (например для предложений естественного языка).\n",
    "* Мотивация в том, чтобы получить состояние, отражающее контекст и слева, и справа для каждого элемента последовательности (например для отнесения слова к части речи т.к. важно анализировать все предложение, и слева, и справа от слова)\n",
    "* Вместо классической рекуррентной сети из трех матриц, может использоваться любая другая конструкция, например LSTM или GRU.\n",
    "    \n",
    "<center>         \n",
    "    <img src=\"./img/rnn_6.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>Распространение ошибок в узле RNN</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LSTM__\n",
    "\n",
    "LSTM (Long Short-Term Memory) \n",
    "\n",
    "* обычные рекуррентные сети очень плохо справляются с ситуациями, когда нужно что-то «запомнить» надолго: влияние скрытого состояния или входа с шага t на последующие состояния рекуррентной сети экспоненциально затухает\n",
    "* LSTM хорошо приспособлена к обучению на задачах классификации, обработки и прогнозирования временных рядов в случаях, когда важные события разделены временными лагами с неопределённой продолжительностью и границами\n",
    "\n",
    "* вместо одного-единственного числа, на которое влияют все последующие состояния, используется специального вида ячейка моделирующая \"долгую память\"\n",
    "    * LSTM моделирует процессы записи и чтения из этой \"ячейки памяти\" \n",
    "    * у ячейки не один набор весов, как у обычного нейрона, а сразу несколько\n",
    "\n",
    "В LSTM есть три основных вида узлов, которые называются гейтами:\n",
    "* входной (input gate)\n",
    "* забывающий (forget gate)\n",
    "* выходной (output gate)\n",
    "* рекуррентная ячейка со скрытым состоянием\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/rnn_8.png\" alt=\"\" style=\"width: 350px;\"/>\n",
    "    <b>Структура LSTM</b> <br/>\n",
    "</center>\n",
    "\n",
    "Переменные:\n",
    "* $x_t$ — входной вектор во время t\n",
    "* $h_t$ — вектор скрытого состояния во время t\n",
    "* $c_t$ — вектор ячейки состояния во время t\n",
    "* $W_i$ — матрицы весов, применяющиеся ко входу\n",
    "* $W_h$ — матрицы весов, в рекуррентных соединениях; b - векторы свободных членов\n",
    "\n",
    "* candidate cell state: $c_t^{'}= \\tanh (W_{xc}x_t + W_{hc}h_{t-1}+b_{c^{'}})$\n",
    "* input gate: $i_t=\\sigma (W_{xi}x_t + W_{hi}h_{t-1}+b_{i})$\n",
    "* forget gate: $f_t=\\sigma (W_{xf}x_t + W_{hf}h_{t-1}+b_{f})$\n",
    "* output gate: $o_t=\\sigma (W_{xo}x_t + W_{ho}h_{t-1}+b_{o})$\n",
    "* cell state: $c_t=f_t \\odot c_{t-1} + i_t \\odot c_t^{'}$\n",
    "* block output: $h_t=o_t \\odot \\tanh (c_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Специфика ячейки памяти в LSTM__\n",
    "\n",
    "* Вход в LSTM:\n",
    "    * входные данные $x_t$\n",
    "    * скрытое состояние $h_{t-1}$\n",
    "    * вектор \"ячейки памяти\" (cell) $c_t$:\n",
    "        \n",
    "* Кандидат на новое значение памяти, полученный из входа и предыдущего скрытого состояния вектор: $c_t^{'}$ \n",
    "$$c_t=f_t \\odot c_{t-1} + i_t \\odot c_t^{'}$$\n",
    "\n",
    "* Новое значение $c_t$ получается как линейная комбинация из старого с коэффициентами из забывающего гейта $f_t$ и нового кандидата $c_t^{'}$ с коэффициентами из входного гейта $i_t$.\n",
    "* Покомпонентное умножение приводит к тому, что на очередном шаге может быть перезаписана только часть \"памяти\" LSTM-ячейки и какая это будет часть, тоже определяет сама ячейка.\n",
    "    * LSTM-ячейка может не просто выбрать, записать новое значение или выкинуть его, а еще и сохранить любую линейную комбинацию старого и нового значения, причем коэффициенты могут быть разными в разных компонентах вектора.\n",
    "    * Решения ячейка принимает в зависимости от конкретного входа.\n",
    "\n",
    "Так распространяется градиент ошибки для $c_t$, если рассматривать ее без забывающего гейта:\n",
    "$$\\frac{\\partial c_t}{\\partial c_{t-1}}=1$$\n",
    "\n",
    "* в рекурсивном вычислении состояния ячейки нет никакой нелинейности: т.е. ошибки в сети из LSTM пропагируются без изменений, и скрытые состояния LSTM могут, если сама ячейка не решит их перезаписать, сохранять свои значения неограниченно долго\n",
    "* __Важно__: хотя обычно веса нейронной сети инициализируются маленькими случайными числами свободный член забывающего гейта $b_f$\n",
    "    * все LSTM-ячейки изначально будут иметь значение $f_t$ около 1/2\n",
    "    * ошибки и память будут затухать экспоненциально. Поэтому свободный член $b_f$ нужно инициализировать большими значениями, около 1 или даже 2: тогда значения забывающих гейтов ft в начале обучения будут близки к нулю и градиенты будут свободно распространяться вдоль всей последовательности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует много разных вариантов LSTM: \n",
    "\n",
    "* LSTM без входного гейта $i_t$\n",
    "* LSTM без забывающего гейта $f_t$\n",
    "* LSTM без выходного гейта $o_t$\n",
    "* LSTM без функции активации $\\sigma$ на входном гейте\n",
    "* LSTM без функции активации $\\sigma$ на выходном гейте\n",
    "* LSTM без замочных скважин\n",
    "* LSTM со связанными входным и забывающим гейтом\n",
    "* LSTM с дополнительными рекуррентными связями на каждом гейте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* LSTM требует довольно значительных ресурсов\n",
    "    * В обычном RNN каждая ячейка имеет один вектор скрытого состояния $h$, а веса представлены тремя матрицами (плюс свободные члены)\n",
    "    * В LSTM-ячейке даже в базовой модели участвует сразу восемь матриц весов\n",
    "* Цель: добиться того же эффекта долгосрочной памяти и решить проблему затухающих градиентов более эффективно\n",
    "\n",
    "* Критически важными компонентами для успешной работы LSTM выступают\n",
    "    * два гейта: выходной и забывающий\n",
    "    * «память» $c_t$ и константная ошибка, которая позволяет состоянию LSTM сохраняться надолго\n",
    "\n",
    "* Архитектура GRU использует идею совмещения выходного и забывающего гейта, а скрытое состояние $h_t$ совмещает со значением памяти $c_t$.\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/gru_1.png\" alt=\"\" style=\"width: 350px;\"/>\n",
    "    <b>Структура GRU</b> <br/>\n",
    "</center>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/gru_2.png\" alt=\"\" style=\"width: 350px;\"/>\n",
    "    <b>Структура GRU</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from: https://trungtran.io/2019/02/08/text-generation-with-pytorch/\n",
    "\n",
    "# set parameters:\n",
    "\n",
    "# flags = Namespace(\n",
    "#     train_file='oliver.txt',\n",
    "#     seq_size=32,\n",
    "#     batch_size=16,\n",
    "#     embedding_size=64,\n",
    "#     lstm_size=64,\n",
    "#     gradients_norm=5,\n",
    "#     initial_words=['I', 'am'],\n",
    "#     predict_top_k=5,\n",
    "#     checkpoint_path='checkpoint',\n",
    "# )\n",
    "\n",
    "Flags = collections.namedtuple('Flags', 'train_file seq_size batch_size embedding_size lstm_size gradients_norm initial_words predict_top_k checkpoint_path learning_rate')\n",
    "# print with_class._fields\n",
    "\n",
    "flags = Flags(\n",
    "    train_file='AnnaKarenina__.txt',\n",
    "    seq_size=32,\n",
    "    batch_size=16,\n",
    "    embedding_size=64,\n",
    "    lstm_size=64,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['Анна', 'не'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint',\n",
    "    learning_rate = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    text = text.split()\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    print(f'num_batches: {num_batches}')\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
    "\n",
    "# TODO: lower case ??\n",
    "# TODO: DataLoader ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "        return logits, state       \n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Модель:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.LSTM(*args, **kwargs)`\n",
    "\n",
    "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n",
    "\n",
    "In a multilayer LSTM, the input $x^{(l)}_t$ of the l-th layer (l >= 2) is the hidden state $h^{(l-1)}_t$ of the previous layer multiplied by dropout $\\delta^{(l-1)}_t$ where each $\\delta^{(l-1)}_t$ is a Bernoulli random variable which is 0 with probability dropout.\n",
    "\n",
    "Shape:\n",
    "\n",
    "_Input:_\n",
    "* __input__ of shape (seq_len, batch, input_size): tensor containing the features of the input sequence.\n",
    "    * The input can also be a packed variable length sequence. \n",
    "* __h_0__ of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch.\n",
    "    * If the LSTM is bidirectional, num_directions should be 2, else it should be 1. \n",
    "* __c_0__ of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch.\n",
    "* _If (h_0, c_0) is not provided, both h_0 and c_0 default to zero_\n",
    "\n",
    "_Output:_\n",
    "* __output__ of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. \n",
    "* __h_n__ of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. \n",
    "* __c_n__ of shape (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t = seq_len.\n",
    "\n",
    "Parameters:\n",
    "* `input_size` – The number of expected features in the input x\n",
    "* `hidden_size` – The number of features in the hidden state h\n",
    "* `num_layers` – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "* `bias` – If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`\n",
    "* `batch_first` – If `True`, then the input and output tensors are provided as (batch, seq, feature). Default: `False`\n",
    "* `dropout` – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "* `bidirectional` – If True, becomes a bidirectional LSTM. Default: False\n",
    "* `proj_size` – If > 0, will use LSTM with projections of corresponding size. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, model, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    model.eval()\n",
    "\n",
    "    state_h, state_c = model.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    \n",
    "    for w in (w for wl in words for w in wl):\n",
    "        ix = torch.tensor([[vocab_to_int[w]]], dtype=torch.long).to(device)\n",
    "        output, (state_h, state_c) = model(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "    \n",
    "    words_new = list()\n",
    "    words_new.append(int_to_vocab[choice])\n",
    "    \n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]], dtype=torch.long).to(device)\n",
    "        output, (state_h, state_c) = model(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words_new.append(int_to_vocab[choice])\n",
    "\n",
    "    words.append(words_new)\n",
    "    \n",
    "    print('\\n\\n'.join(' '.join(s) for s in words))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_loss_and_train_op(model, lr=0.001):\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 55567\n",
      "num_batches: 576\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
    "    flags.train_file, flags.batch_size, flags.seq_size)\n",
    "\n",
    "model = RNNModule(n_vocab, flags.seq_size,\n",
    "                flags.embedding_size, flags.lstm_size)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=flags.learning_rate)\n",
    "\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'себе.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab[422]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной цикл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/200 Iteration: 50 Loss: 8.682318687438965\n",
      "Epoch: 0/200 Iteration: 100 Loss: 8.617399215698242\n",
      "Epoch: 0/200 Iteration: 150 Loss: 8.296709060668945\n",
      "Epoch: 0/200 Iteration: 200 Loss: 7.892895221710205\n",
      "Epoch: 0/200 Iteration: 250 Loss: 7.927113056182861\n",
      "Epoch: 0/200 Iteration: 300 Loss: 8.42678451538086\n",
      "Epoch: 0/200 Iteration: 350 Loss: 8.1193265914917\n",
      "Epoch: 0/200 Iteration: 400 Loss: 8.304059982299805\n",
      "Epoch: 0/200 Iteration: 450 Loss: 7.96299409866333\n",
      "Epoch: 0/200 Iteration: 500 Loss: 7.650730609893799\n",
      "Epoch: 0/200 Iteration: 550 Loss: 7.475924015045166\n",
      "Epoch: 1/200 Iteration: 600 Loss: 6.885001182556152\n",
      "Epoch: 1/200 Iteration: 650 Loss: 7.147774696350098\n",
      "Epoch: 1/200 Iteration: 700 Loss: 7.100455284118652\n",
      "Epoch: 1/200 Iteration: 750 Loss: 7.00004243850708\n",
      "Epoch: 1/200 Iteration: 800 Loss: 6.786848545074463\n",
      "Epoch: 1/200 Iteration: 850 Loss: 6.651426792144775\n",
      "Epoch: 1/200 Iteration: 900 Loss: 6.785276889801025\n",
      "Epoch: 1/200 Iteration: 950 Loss: 6.836668968200684\n",
      "Epoch: 1/200 Iteration: 1000 Loss: 7.031463623046875\n",
      "Анна не\n",
      "\n",
      "мог любить этого и все в своем же положении. – сказала она, как будто не может быть, не может видеть его в своем положении, что это так же, когда ему на него на него, она так как только не мог понять, как будто было понять ей, но не мог понять. Но не могла решиться не могла понять в том, чего он был не могла решиться не могли видеть это и с ним о своем была все было не могла решиться не только что они любить и что они и не могли видеть его в этом ужасном и он был и все\n",
      "Epoch: 1/200 Iteration: 1050 Loss: 6.68926477432251\n",
      "Epoch: 1/200 Iteration: 1100 Loss: 6.527917385101318\n",
      "Epoch: 1/200 Iteration: 1150 Loss: 6.4937567710876465\n",
      "Epoch: 2/200 Iteration: 1200 Loss: 6.473074436187744\n",
      "Epoch: 2/200 Iteration: 1250 Loss: 6.3288187980651855\n",
      "Epoch: 2/200 Iteration: 1300 Loss: 5.937457084655762\n",
      "Epoch: 2/200 Iteration: 1350 Loss: 5.881008625030518\n",
      "Epoch: 2/200 Iteration: 1400 Loss: 5.701354503631592\n",
      "Epoch: 2/200 Iteration: 1450 Loss: 5.89992094039917\n",
      "Epoch: 2/200 Iteration: 1500 Loss: 5.4892401695251465\n",
      "Epoch: 2/200 Iteration: 1550 Loss: 5.7687296867370605\n",
      "Epoch: 2/200 Iteration: 1600 Loss: 5.68658971786499\n",
      "Epoch: 2/200 Iteration: 1650 Loss: 5.633321762084961\n",
      "Epoch: 2/200 Iteration: 1700 Loss: 5.607697010040283\n",
      "Epoch: 3/200 Iteration: 1750 Loss: 5.876126766204834\n",
      "Epoch: 3/200 Iteration: 1800 Loss: 5.443840980529785\n",
      "Epoch: 3/200 Iteration: 1850 Loss: 5.316812992095947\n",
      "Epoch: 3/200 Iteration: 1900 Loss: 5.201122760772705\n",
      "Epoch: 3/200 Iteration: 1950 Loss: 5.09616756439209\n",
      "Epoch: 3/200 Iteration: 2000 Loss: 4.860184669494629\n",
      "Анна не\n",
      "\n",
      "могла изгнать ли ему казалось, надетом с Марьей и грация, которые он был нынче; но не только была испуганная, любить. Он в котором он услыхал, чтоб избавить ее его сообщалась и, главное, кроме того, что, если будто он не только остатком их, и он мог оторваться с ним более всего причем в чем он мог понять, как он мог выговорить его. Он в особенности в ней делается?» когда он услыхал, чтоб она разумела ее его накормили дышавший в особенности его заблестели – Ну, что же делать? – Я с вашею тетушкой, И он хочет перенесся ничего так как он услыхал, что\n",
      "Epoch: 3/200 Iteration: 2050 Loss: 4.86093807220459\n",
      "Epoch: 3/200 Iteration: 2100 Loss: 4.784378528594971\n",
      "Epoch: 3/200 Iteration: 2150 Loss: 4.625822067260742\n",
      "Epoch: 3/200 Iteration: 2200 Loss: 4.586594581604004\n",
      "Epoch: 3/200 Iteration: 2250 Loss: 4.869402885437012\n",
      "Epoch: 3/200 Iteration: 2300 Loss: 4.818678379058838\n",
      "Epoch: 4/200 Iteration: 2350 Loss: 5.045168399810791\n",
      "Epoch: 4/200 Iteration: 2400 Loss: 4.716988563537598\n",
      "Epoch: 4/200 Iteration: 2450 Loss: 4.736148357391357\n",
      "Epoch: 4/200 Iteration: 2500 Loss: 4.575717926025391\n",
      "Epoch: 4/200 Iteration: 2550 Loss: 4.33756685256958\n",
      "Epoch: 4/200 Iteration: 2600 Loss: 4.393008232116699\n",
      "Epoch: 4/200 Iteration: 2650 Loss: 4.39669132232666\n",
      "Epoch: 4/200 Iteration: 2700 Loss: 4.372436046600342\n",
      "Epoch: 4/200 Iteration: 2750 Loss: 4.25816011428833\n",
      "Epoch: 4/200 Iteration: 2800 Loss: 4.364677429199219\n",
      "Epoch: 4/200 Iteration: 2850 Loss: 4.235698223114014\n",
      "Epoch: 5/200 Iteration: 2900 Loss: 4.5270771980285645\n",
      "Epoch: 5/200 Iteration: 2950 Loss: 4.2982072830200195\n",
      "Epoch: 5/200 Iteration: 3000 Loss: 4.244172096252441\n",
      "Анна не\n",
      "\n",
      "могла воздержаться, не может существовать. Дарья Он не мог бы понимать, а на минуту и она была так как можно быть оскорблены то тема лоб, в том, но что они разумели механическую не только сходством, а на ее головой, как бы нарушило и что они разрушают, были и так и не было противостоять и не могла понять про его. Но это был сторонником матери с нею слишком, и в своем кабинете, он уже давно ли не в силах поднять не только и потому не может быть тайна его. Он хотел закрыть речи. Он хотел отвлечь посетителей и он к жене. Она\n",
      "Epoch: 5/200 Iteration: 3050 Loss: 4.327160835266113\n",
      "Epoch: 5/200 Iteration: 3100 Loss: 4.097737789154053\n",
      "Epoch: 5/200 Iteration: 3150 Loss: 4.194820880889893\n",
      "Epoch: 5/200 Iteration: 3200 Loss: 4.076470851898193\n",
      "Epoch: 5/200 Iteration: 3250 Loss: 4.141369819641113\n",
      "Epoch: 5/200 Iteration: 3300 Loss: 3.987783193588257\n",
      "Epoch: 5/200 Iteration: 3350 Loss: 3.788259506225586\n",
      "Epoch: 5/200 Iteration: 3400 Loss: 4.084507465362549\n",
      "Epoch: 5/200 Iteration: 3450 Loss: 3.8182668685913086\n",
      "Epoch: 6/200 Iteration: 3500 Loss: 4.146830081939697\n",
      "Epoch: 6/200 Iteration: 3550 Loss: 3.949707269668579\n",
      "Epoch: 6/200 Iteration: 3600 Loss: 3.97001576423645\n",
      "Epoch: 6/200 Iteration: 3650 Loss: 4.028846740722656\n",
      "Epoch: 6/200 Iteration: 3700 Loss: 4.087593078613281\n",
      "Epoch: 6/200 Iteration: 3750 Loss: 4.007614612579346\n",
      "Epoch: 6/200 Iteration: 3800 Loss: 3.9264700412750244\n",
      "Epoch: 6/200 Iteration: 3850 Loss: 3.5498125553131104\n",
      "Epoch: 6/200 Iteration: 3900 Loss: 3.8135733604431152\n",
      "Epoch: 6/200 Iteration: 3950 Loss: 3.8270134925842285\n",
      "Epoch: 6/200 Iteration: 4000 Loss: 3.716113805770874\n",
      "Анна не\n",
      "\n",
      "мог видеть этого. Он чувствовал, чтобы получить гордиться, как он решился ручку, что они кончили свои крепкие белые молодой и с гордым улыбкой удовольствия, и, подняв брата в котором было не мог быть диким. Она знала, что ему казалось себя недостойным полную на станцию новою Алексеем Александровичем, – сказала Анна. И, оглянувшись, с иллюстрациями французского критика заработной отношений. И потом, если бы сходство не нужна? Он не в этот переходный, с отражениями до малейших служанок русского рабочего за границей, – это искусство? или пить? и Левин встал и Левин в ту сторону в гостиной народа Лидии Ивановны с красным обязанности, узнанные\n",
      "Epoch: 7/200 Iteration: 4050 Loss: 3.833950996398926\n",
      "Epoch: 7/200 Iteration: 4100 Loss: 3.885650634765625\n",
      "Epoch: 7/200 Iteration: 4150 Loss: 4.009477615356445\n",
      "Epoch: 7/200 Iteration: 4200 Loss: 3.8464252948760986\n",
      "Epoch: 7/200 Iteration: 4250 Loss: 3.7870068550109863\n",
      "Epoch: 7/200 Iteration: 4300 Loss: 3.8585000038146973\n",
      "Epoch: 7/200 Iteration: 4350 Loss: 3.718935489654541\n",
      "Epoch: 7/200 Iteration: 4400 Loss: 3.555428981781006\n",
      "Epoch: 7/200 Iteration: 4450 Loss: 3.4098565578460693\n",
      "Epoch: 7/200 Iteration: 4500 Loss: 3.411998987197876\n",
      "Epoch: 7/200 Iteration: 4550 Loss: 3.6981942653656006\n",
      "Epoch: 7/200 Iteration: 4600 Loss: 3.603067398071289\n",
      "Epoch: 8/200 Iteration: 4650 Loss: 3.8299813270568848\n",
      "Epoch: 8/200 Iteration: 4700 Loss: 3.8134605884552\n",
      "Epoch: 8/200 Iteration: 4750 Loss: 3.479776620864868\n",
      "Epoch: 8/200 Iteration: 4800 Loss: 3.7994277477264404\n",
      "Epoch: 8/200 Iteration: 4850 Loss: 3.6490330696105957\n",
      "Epoch: 8/200 Iteration: 4900 Loss: 3.5843546390533447\n",
      "Epoch: 8/200 Iteration: 4950 Loss: 3.5812230110168457\n",
      "Epoch: 8/200 Iteration: 5000 Loss: 3.4532523155212402\n",
      "Анна не\n",
      "\n",
      "могла не может выздороветь, Левин видел, и мускулы с ней им употреблять Она не знала иногда, как бы не спала, а на ее соперник, Он провинился, как он приехал к жене, не было никакой всей души, давшее к этому существу и с которою Степан Аркадьич вышел на совершенное равнодушие. Это нечестно, была жива, жизни, он делал предложение и материальными зашевелила из своего собеседника. – Ну, так же в лиловом? – и я бы перебил его: – Я только не смеялась! В кандидаты глазах сестры. Когда Левин покраснел и, увидав выбегавших мужа на извозчике, и уважение к ней. – А рубашка! как\n",
      "Epoch: 8/200 Iteration: 5050 Loss: 3.295600414276123\n",
      "Epoch: 8/200 Iteration: 5100 Loss: 3.3508615493774414\n",
      "Epoch: 8/200 Iteration: 5150 Loss: 3.371246337890625\n",
      "Epoch: 9/200 Iteration: 5200 Loss: 3.4464797973632812\n",
      "Epoch: 9/200 Iteration: 5250 Loss: 3.701490879058838\n",
      "Epoch: 9/200 Iteration: 5300 Loss: 3.46051025390625\n",
      "Epoch: 9/200 Iteration: 5350 Loss: 3.3670973777770996\n",
      "Epoch: 9/200 Iteration: 5400 Loss: 3.3494362831115723\n",
      "Epoch: 9/200 Iteration: 5450 Loss: 3.5550999641418457\n",
      "Epoch: 9/200 Iteration: 5500 Loss: 3.3187875747680664\n",
      "Epoch: 9/200 Iteration: 5550 Loss: 3.274777412414551\n",
      "Epoch: 9/200 Iteration: 5600 Loss: 2.9973299503326416\n",
      "Epoch: 9/200 Iteration: 5650 Loss: 3.298952579498291\n",
      "Epoch: 9/200 Iteration: 5700 Loss: 3.324040412902832\n",
      "Epoch: 9/200 Iteration: 5750 Loss: 3.1262476444244385\n",
      "Epoch: 10/200 Iteration: 5800 Loss: 3.6126906871795654\n",
      "Epoch: 10/200 Iteration: 5850 Loss: 3.6530611515045166\n",
      "Epoch: 10/200 Iteration: 5900 Loss: 3.4918267726898193\n",
      "Epoch: 10/200 Iteration: 5950 Loss: 3.581866979598999\n",
      "Epoch: 10/200 Iteration: 6000 Loss: 3.264866352081299\n",
      "Анна не\n",
      "\n",
      "умела себе внимания. Если тема кого-нибудь подбородком энергическое лицо ее от себя тридцать лет, он не мог бы привыкнуть, в соединении с ловкою взгляды в комнате «Все-таки задержат; от росту настолько примирились, тотчас же, не имели ни того, чтобы доказать прийти в своем воображении постели на солнце, к Вареньке в калошках на кипящий блестящими глазами и спотыкаясь по комнате. «Так что это делалось, не женился и с тем более что он решит о себе, не может быть. Надо, что, не будем говорить. формы, и я последую ее в погибель. Зачем я докажу занимались чуть и смешных. Но с этою пиесой нужно\n",
      "Epoch: 10/200 Iteration: 6050 Loss: 3.3448548316955566\n",
      "Epoch: 10/200 Iteration: 6100 Loss: 3.3457908630371094\n",
      "Epoch: 10/200 Iteration: 6150 Loss: 3.3205173015594482\n",
      "Epoch: 10/200 Iteration: 6200 Loss: 3.114924669265747\n",
      "Epoch: 10/200 Iteration: 6250 Loss: 3.257934331893921\n",
      "Epoch: 10/200 Iteration: 6300 Loss: 3.2026009559631348\n",
      "Epoch: 11/200 Iteration: 6350 Loss: 2.814459800720215\n",
      "Epoch: 11/200 Iteration: 6400 Loss: 3.1786646842956543\n",
      "Epoch: 11/200 Iteration: 6450 Loss: 3.238440752029419\n",
      "Epoch: 11/200 Iteration: 6500 Loss: 3.298950433731079\n",
      "Epoch: 11/200 Iteration: 6550 Loss: 3.289571762084961\n",
      "Epoch: 11/200 Iteration: 6600 Loss: 3.364015817642212\n",
      "Epoch: 11/200 Iteration: 6650 Loss: 3.2330875396728516\n",
      "Epoch: 11/200 Iteration: 6700 Loss: 3.0054428577423096\n",
      "Epoch: 11/200 Iteration: 6750 Loss: 3.037858486175537\n",
      "Epoch: 11/200 Iteration: 6800 Loss: 3.2184526920318604\n",
      "Epoch: 11/200 Iteration: 6850 Loss: 3.293436050415039\n",
      "Epoch: 11/200 Iteration: 6900 Loss: 3.119536876678467\n",
      "Epoch: 12/200 Iteration: 6950 Loss: 2.9451112747192383\n",
      "Epoch: 12/200 Iteration: 7000 Loss: 3.231874942779541\n",
      "Анна не\n",
      "\n",
      "могла выговорить. – Но ты вспомни, что это не добро; я стою за честь, я еду, только не люблю раньше, чтоб я нечестный семейной рукою в моем тебе… Хотя я… – сказала Анна, поднимаясь к столу. Анна Аркадьевна послала просто, что он хотел не быть благодарна за обедом, что она не быть любовь. Но теперь он прошел с такими же местах». Долли с ними встретил пред нею на одинаковом слове в верхних потной того, чтобы не читалось. а ночью потекло в выучиванье мысли и удобен, и задерживаемой с ним. Он хотел быть иначе». И, как это сознание целей, они несправедливы; он\n",
      "Epoch: 12/200 Iteration: 7050 Loss: 3.2843239307403564\n",
      "Epoch: 12/200 Iteration: 7100 Loss: 3.170234203338623\n",
      "Epoch: 12/200 Iteration: 7150 Loss: 3.157946825027466\n",
      "Epoch: 12/200 Iteration: 7200 Loss: 3.1194396018981934\n",
      "Epoch: 12/200 Iteration: 7250 Loss: 3.169311761856079\n",
      "Epoch: 12/200 Iteration: 7300 Loss: 3.0677618980407715\n",
      "Epoch: 12/200 Iteration: 7350 Loss: 2.9790380001068115\n",
      "Epoch: 12/200 Iteration: 7400 Loss: 2.982882022857666\n",
      "Epoch: 12/200 Iteration: 7450 Loss: 2.677231788635254\n",
      "Epoch: 13/200 Iteration: 7500 Loss: 3.0200586318969727\n",
      "Epoch: 13/200 Iteration: 7550 Loss: 3.285144329071045\n",
      "Epoch: 13/200 Iteration: 7600 Loss: 3.3007407188415527\n",
      "Epoch: 13/200 Iteration: 7650 Loss: 3.0984368324279785\n",
      "Epoch: 13/200 Iteration: 7700 Loss: 2.848121166229248\n",
      "Epoch: 13/200 Iteration: 7750 Loss: 2.9962944984436035\n",
      "Epoch: 13/200 Iteration: 7800 Loss: 3.1761934757232666\n",
      "Epoch: 13/200 Iteration: 7850 Loss: 2.813314437866211\n",
      "Epoch: 13/200 Iteration: 7900 Loss: 3.1329381465911865\n",
      "Epoch: 13/200 Iteration: 7950 Loss: 2.8513176441192627\n",
      "Epoch: 13/200 Iteration: 8000 Loss: 2.7149171829223633\n",
      "Анна не\n",
      "\n",
      "была приглашена до глубины жизни, и не могла придумать этого, ему было легче, что в глубине основания овес, своей коротенькой жакетке с кажущимися взглядом на этом обеде с листком бы отвлечь с ним и в периоде газетных хрониках этого шагом. И действительно, как бы не умел делать, и сердился. Наконец застрелиться. Одно мои чудеса. как он искал лишен глубины ревности, чем страсть, чтоб обмануть княгини Мягкой, удивляясь нежности самого себя. Только в первый раз он хотел говорить; как зверек, оглядываясь и на любимого себя за буйство. тысяч с ним познакомиться. которое она кончила службы и недоумевающий разговор. Вронский взглянул подле нее.\n",
      "Epoch: 13/200 Iteration: 8050 Loss: 2.875990629196167\n",
      "Epoch: 14/200 Iteration: 8100 Loss: 2.7144415378570557\n",
      "Epoch: 14/200 Iteration: 8150 Loss: 2.918644905090332\n",
      "Epoch: 14/200 Iteration: 8200 Loss: 3.1632487773895264\n",
      "Epoch: 14/200 Iteration: 8250 Loss: 2.826634645462036\n",
      "Epoch: 14/200 Iteration: 8300 Loss: 3.0248379707336426\n",
      "Epoch: 14/200 Iteration: 8350 Loss: 2.964581251144409\n",
      "Epoch: 14/200 Iteration: 8400 Loss: 3.157541036605835\n",
      "Epoch: 14/200 Iteration: 8450 Loss: 2.931691884994507\n",
      "Epoch: 14/200 Iteration: 8500 Loss: 2.935457706451416\n",
      "Epoch: 14/200 Iteration: 8550 Loss: 2.639432668685913\n",
      "Epoch: 14/200 Iteration: 8600 Loss: 2.8390228748321533\n",
      "Epoch: 15/200 Iteration: 8650 Loss: 2.7728898525238037\n",
      "Epoch: 15/200 Iteration: 8700 Loss: 2.9565963745117188\n",
      "Epoch: 15/200 Iteration: 8750 Loss: 2.9417855739593506\n",
      "Epoch: 15/200 Iteration: 8800 Loss: 3.2109615802764893\n",
      "Epoch: 15/200 Iteration: 8850 Loss: 2.5764851570129395\n",
      "Epoch: 15/200 Iteration: 8900 Loss: 3.0371413230895996\n",
      "Epoch: 15/200 Iteration: 8950 Loss: 2.84837007522583\n",
      "Epoch: 15/200 Iteration: 9000 Loss: 2.863959312438965\n",
      "Анна не\n",
      "\n",
      "только начинается. Алексей от нее семейною трех бекасов. – Как я никогда еще нетвердо, у тебя не ездить, потому не серьезное, так неприятно, что он так хорошо помню. В то ли мы завалены исчезло, я знаю, не могу. Ты можешь иметь принимаемые меры. И это ужасно! – Нет, нет, есть так надо. Я хотела сказать про себя виноват, и не отходили от нее. И покрытое разочарованием, во мне или, бы высшим неприятно, что ты хочешь сиденья в этом вертепе? а с выражением испуга взглядывая Я буду скажу, чтобы сказать свои ласки. – Дмитрия мировой литературе»,[4] Во всех водах, и раздаривал объяснить\n",
      "Epoch: 15/200 Iteration: 9050 Loss: 2.8071625232696533\n",
      "Epoch: 15/200 Iteration: 9100 Loss: 2.64180588722229\n",
      "Epoch: 15/200 Iteration: 9150 Loss: 2.7574689388275146\n",
      "Epoch: 15/200 Iteration: 9200 Loss: 2.736119031906128\n",
      "Epoch: 16/200 Iteration: 9250 Loss: 2.912357807159424\n",
      "Epoch: 16/200 Iteration: 9300 Loss: 3.196399211883545\n",
      "Epoch: 16/200 Iteration: 9350 Loss: 2.898688793182373\n",
      "Epoch: 16/200 Iteration: 9400 Loss: 2.986158847808838\n",
      "Epoch: 16/200 Iteration: 9450 Loss: 2.958204507827759\n",
      "Epoch: 16/200 Iteration: 9500 Loss: 2.7227869033813477\n",
      "Epoch: 16/200 Iteration: 9550 Loss: 2.6796982288360596\n",
      "Epoch: 16/200 Iteration: 9600 Loss: 2.594172716140747\n",
      "Epoch: 16/200 Iteration: 9650 Loss: 2.7981576919555664\n",
      "Epoch: 16/200 Iteration: 9700 Loss: 2.8497378826141357\n",
      "Epoch: 16/200 Iteration: 9750 Loss: 2.6357860565185547\n",
      "Epoch: 17/200 Iteration: 9800 Loss: 2.838327646255493\n",
      "Epoch: 17/200 Iteration: 9850 Loss: 3.2115118503570557\n",
      "Epoch: 17/200 Iteration: 9900 Loss: 3.0086095333099365\n",
      "Epoch: 17/200 Iteration: 9950 Loss: 3.000120162963867\n",
      "Epoch: 17/200 Iteration: 10000 Loss: 2.7553200721740723\n",
      "Анна не\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "могла быть), потому он перехватился взглядом. – Ну, и он видает – сказала она, с улыбкой пожав руку и про него; это что не сказано ни в книгах он обвенчан, как бы то ни стало напряжение счастья Степану Аркадьичу и убить не поддавался ему. В одиннадцатом часу с доме в общественных обязанностей. но он представлял себе робость. в том, что она уже… Мысль его это не изменившаяся, того, чтобы не мешают семье. Они так же, как на выборы. И она увидала ему справки его письмо, которое он, выходя, о ней было бледно и вышла. «Неужели они в чем, предоставлял ли, чтобы\n",
      "Epoch: 17/200 Iteration: 10050 Loss: 3.333803176879883\n",
      "Epoch: 17/200 Iteration: 10100 Loss: 2.727761745452881\n",
      "Epoch: 17/200 Iteration: 10150 Loss: 2.623095989227295\n",
      "Epoch: 17/200 Iteration: 10200 Loss: 2.708463668823242\n",
      "Epoch: 17/200 Iteration: 10250 Loss: 2.441995143890381\n",
      "Epoch: 17/200 Iteration: 10300 Loss: 2.800102949142456\n",
      "Epoch: 17/200 Iteration: 10350 Loss: 2.82179594039917\n",
      "Epoch: 18/200 Iteration: 10400 Loss: 2.604034662246704\n",
      "Epoch: 18/200 Iteration: 10450 Loss: 2.7963826656341553\n",
      "Epoch: 18/200 Iteration: 10500 Loss: 3.03944993019104\n",
      "Epoch: 18/200 Iteration: 10550 Loss: 2.9357330799102783\n",
      "Epoch: 18/200 Iteration: 10600 Loss: 2.7296290397644043\n",
      "Epoch: 18/200 Iteration: 10650 Loss: 2.7502801418304443\n",
      "Epoch: 18/200 Iteration: 10700 Loss: 2.683638334274292\n",
      "Epoch: 18/200 Iteration: 10750 Loss: 2.7075986862182617\n",
      "Epoch: 18/200 Iteration: 10800 Loss: 2.8757920265197754\n",
      "Epoch: 18/200 Iteration: 10850 Loss: 2.681535005569458\n",
      "Epoch: 18/200 Iteration: 10900 Loss: 2.588897228240967\n",
      "Epoch: 19/200 Iteration: 10950 Loss: 2.634068727493286\n",
      "Epoch: 19/200 Iteration: 11000 Loss: 2.850020170211792\n",
      "Анна не\n",
      "\n",
      "хочет возможности внимания. Со времени которых он был не мог заснуть. «Что я прошу всякого в духе – все становится для России, и все равно как можно быть дети. Она встала и Песцовым и независимый, не понимал про то, что, расспрашивая его домой человек откуда бы и всегда не было, как бы желая сказать ему нехорош. «Буду меньше быть в другой день, потому что, подписывая с одной жизни. И он чувствовал, что, когда он вошел к нему из дома и молчал. Он уже лежал. ее. И действительно, она ревновала себя смущенною последний раз с таким видом, на свете, и вечного надеясь\n",
      "Epoch: 19/200 Iteration: 11050 Loss: 2.9835002422332764\n",
      "Epoch: 19/200 Iteration: 11100 Loss: 2.7400524616241455\n",
      "Epoch: 19/200 Iteration: 11150 Loss: 2.626239538192749\n",
      "Epoch: 19/200 Iteration: 11200 Loss: 2.6586875915527344\n",
      "Epoch: 19/200 Iteration: 11250 Loss: 2.3262526988983154\n",
      "Epoch: 19/200 Iteration: 11300 Loss: 2.241539716720581\n",
      "Epoch: 19/200 Iteration: 11350 Loss: 2.4941928386688232\n",
      "Epoch: 19/200 Iteration: 11400 Loss: 2.852213144302368\n",
      "Epoch: 19/200 Iteration: 11450 Loss: 2.738616466522217\n",
      "Epoch: 19/200 Iteration: 11500 Loss: 2.6120195388793945\n",
      "Epoch: 20/200 Iteration: 11550 Loss: 2.5615522861480713\n",
      "Epoch: 20/200 Iteration: 11600 Loss: 2.6084117889404297\n",
      "Epoch: 20/200 Iteration: 11650 Loss: 2.761975049972534\n",
      "Epoch: 20/200 Iteration: 11700 Loss: 2.5255746841430664\n",
      "Epoch: 20/200 Iteration: 11750 Loss: 2.71004581451416\n",
      "Epoch: 20/200 Iteration: 11800 Loss: 2.6180977821350098\n",
      "Epoch: 20/200 Iteration: 11850 Loss: 2.6665291786193848\n",
      "Epoch: 20/200 Iteration: 11900 Loss: 2.525512933731079\n",
      "Epoch: 20/200 Iteration: 11950 Loss: 2.4127588272094727\n",
      "Epoch: 20/200 Iteration: 12000 Loss: 2.6454594135284424\n",
      "Анна не\n",
      "\n",
      "знала, написала мысли фигуры. – Я устала ли не говори, у нас не будет. Только думают. Но как ни Ну, что вы приехали, не оскорбить меня, оглядываясь к ней, но теперь ее к тебе, что он сделал… Сережу его. И как бы вы приехали к нам. Я все простил – отвечал Левин, не глядя на свою жизнь Анну, если б я не могу быть католичнее и не глядел за встретившемуся Левин? – Ну, позволь… У Пушкина: Он назвал несут все сто тысяч матушки. к Кити. – Ну, как это все это и не имеет говорить но я рада, что, когда мы\n",
      "Epoch: 20/200 Iteration: 12050 Loss: 1.9956955909729004\n",
      "Epoch: 21/200 Iteration: 12100 Loss: 2.735252857208252\n",
      "Epoch: 21/200 Iteration: 12150 Loss: 2.253478527069092\n",
      "Epoch: 21/200 Iteration: 12200 Loss: 2.711315155029297\n",
      "Epoch: 21/200 Iteration: 12250 Loss: 2.9228951930999756\n",
      "Epoch: 21/200 Iteration: 12300 Loss: 2.7492880821228027\n",
      "Epoch: 21/200 Iteration: 12350 Loss: 2.537280797958374\n",
      "Epoch: 21/200 Iteration: 12400 Loss: 2.5482048988342285\n",
      "Epoch: 21/200 Iteration: 12450 Loss: 2.529860734939575\n",
      "Epoch: 21/200 Iteration: 12500 Loss: 2.5489978790283203\n",
      "Epoch: 21/200 Iteration: 12550 Loss: 2.74658465385437\n",
      "Epoch: 21/200 Iteration: 12600 Loss: 2.6324191093444824\n",
      "Epoch: 21/200 Iteration: 12650 Loss: 2.494694948196411\n",
      "Epoch: 22/200 Iteration: 12700 Loss: 2.307013511657715\n",
      "Epoch: 22/200 Iteration: 12750 Loss: 2.418116569519043\n",
      "Epoch: 22/200 Iteration: 12800 Loss: 3.014488935470581\n",
      "Epoch: 22/200 Iteration: 12850 Loss: 2.4165146350860596\n",
      "Epoch: 22/200 Iteration: 12900 Loss: 2.506117582321167\n",
      "Epoch: 22/200 Iteration: 12950 Loss: 2.5440752506256104\n",
      "Epoch: 22/200 Iteration: 13000 Loss: 2.5742945671081543\n",
      "Анна не\n",
      "\n",
      "было на станцию, начала большое улыбкой лошадей, фыркавших ее был своею материнскою, семейною послышались людей, – все пропало! И действительно, он передал его за свою досаду, к ней холоднее, – сказал Левин. «К лиц, – что ж, пускай едут. – Нет, мы не можешь понять… и не влюблен – сказал он. Пухлый занятия, – отвечал помещик. И как тривиально она, как ни тяжело будет дом, но как только кто к себе ему. Все, что у нее голова и и потом подражали нам. Старый Степан Аркадьич не в ту минуту, но когда они все казалось, надетом женской малою занять дня не только\n",
      "Epoch: 22/200 Iteration: 13050 Loss: 2.5243821144104004\n",
      "Epoch: 22/200 Iteration: 13100 Loss: 2.566542625427246\n",
      "Epoch: 22/200 Iteration: 13150 Loss: 2.424234390258789\n",
      "Epoch: 22/200 Iteration: 13200 Loss: 2.266512632369995\n",
      "Epoch: 23/200 Iteration: 13250 Loss: 2.587085485458374\n",
      "Epoch: 23/200 Iteration: 13300 Loss: 2.318469762802124\n",
      "Epoch: 23/200 Iteration: 13350 Loss: 2.695009231567383\n",
      "Epoch: 23/200 Iteration: 13400 Loss: 2.6207540035247803\n",
      "Epoch: 23/200 Iteration: 13450 Loss: 2.743345022201538\n",
      "Epoch: 23/200 Iteration: 13500 Loss: 2.832732915878296\n",
      "Epoch: 23/200 Iteration: 13550 Loss: 2.254593849182129\n",
      "Epoch: 23/200 Iteration: 13600 Loss: 2.634408473968506\n",
      "Epoch: 23/200 Iteration: 13650 Loss: 2.187166929244995\n",
      "Epoch: 23/200 Iteration: 13700 Loss: 2.5110161304473877\n",
      "Epoch: 23/200 Iteration: 13750 Loss: 2.5768086910247803\n",
      "Epoch: 23/200 Iteration: 13800 Loss: 2.521782398223877\n",
      "Epoch: 24/200 Iteration: 13850 Loss: 2.4728376865386963\n",
      "Epoch: 24/200 Iteration: 13900 Loss: 2.5844054222106934\n",
      "Epoch: 24/200 Iteration: 13950 Loss: 2.858792304992676\n",
      "Epoch: 24/200 Iteration: 14000 Loss: 2.437131643295288\n",
      "Анна не\n",
      "\n",
      "могла преодолеть. XXIV Ночь, проведенная ее Вронским. Обед, весело блестящими головой. И она чувствовала, чтобы дать прежде. Теперь же, если тело не признать, что недостатки ее, он не желал, если она видела ей душевное настроение в этом положении, что не одетый, об Алексее Ребенок к начатому ими поверены, или жены. – Пора, ужасно около нас для меня и погибла. Теперь Степан Аркадьич, мне отказано, или что у вас не такой, а propos ваш развода, три какие-то другие люди, и погибла. Она видела также, твоей небольшую положенному достояние, во власти Степана Аркадьича: Шураев снял между ними толпы за выстрелами. «Ах, и она\n",
      "Epoch: 24/200 Iteration: 14050 Loss: 2.4377644062042236\n",
      "Epoch: 24/200 Iteration: 14100 Loss: 2.5953731536865234\n",
      "Epoch: 24/200 Iteration: 14150 Loss: 2.4795334339141846\n",
      "Epoch: 24/200 Iteration: 14200 Loss: 2.610865354537964\n",
      "Epoch: 24/200 Iteration: 14250 Loss: 2.5116422176361084\n",
      "Epoch: 24/200 Iteration: 14300 Loss: 2.5462746620178223\n",
      "Epoch: 24/200 Iteration: 14350 Loss: 2.3443233966827393\n",
      "Epoch: 24/200 Iteration: 14400 Loss: 2.427337169647217\n",
      "Epoch: 25/200 Iteration: 14450 Loss: 2.386251926422119\n",
      "Epoch: 25/200 Iteration: 14500 Loss: 2.735656499862671\n",
      "Epoch: 25/200 Iteration: 14550 Loss: 2.492405891418457\n",
      "Epoch: 25/200 Iteration: 14600 Loss: 2.5937843322753906\n",
      "Epoch: 25/200 Iteration: 14650 Loss: 2.7688181400299072\n",
      "Epoch: 25/200 Iteration: 14700 Loss: 2.2543935775756836\n",
      "Epoch: 25/200 Iteration: 14750 Loss: 2.091012954711914\n",
      "Epoch: 25/200 Iteration: 14800 Loss: 2.52994441986084\n",
      "Epoch: 25/200 Iteration: 14850 Loss: 2.501646041870117\n",
      "Epoch: 25/200 Iteration: 14900 Loss: 2.4273762702941895\n",
      "Epoch: 25/200 Iteration: 14950 Loss: 2.6328299045562744\n",
      "Epoch: 26/200 Iteration: 15000 Loss: 2.6720144748687744\n",
      "Анна не\n",
      "\n",
      "думая о своей Константину старалась, и просит ее желания. И ему оскорбительно. товарища железные маленький, стола, пред поворотом она благодарит красотой и о деле стал просить в его голове. Самые ряд, а в его белые зубы, где Вронский провел его. едут Левина на грязной комнату. И опять крепко-крепко к дамам и не показывать, как на это дело на него молодого человека. Весь мир другом нельзя отложить дожидавшуюся так, что же это значит? к удивлению рублей. Николай статья для нее тонкий были деньги, дешевле, и заказать это раздражило Левина куда, в старом пальто ему и, зная у него не любил и ни\n",
      "Epoch: 26/200 Iteration: 15050 Loss: 2.316833734512329\n",
      "Epoch: 26/200 Iteration: 15100 Loss: 2.5949652194976807\n",
      "Epoch: 26/200 Iteration: 15150 Loss: 2.4649057388305664\n",
      "Epoch: 26/200 Iteration: 15200 Loss: 2.4144973754882812\n",
      "Epoch: 26/200 Iteration: 15250 Loss: 2.3748106956481934\n",
      "Epoch: 26/200 Iteration: 15300 Loss: 2.60439133644104\n",
      "Epoch: 26/200 Iteration: 15350 Loss: 2.165653705596924\n",
      "Epoch: 26/200 Iteration: 15400 Loss: 2.114004373550415\n",
      "Epoch: 26/200 Iteration: 15450 Loss: 2.282752513885498\n",
      "Epoch: 26/200 Iteration: 15500 Loss: 2.6196846961975098\n",
      "Epoch: 26/200 Iteration: 15550 Loss: 2.3384077548980713\n",
      "Epoch: 27/200 Iteration: 15600 Loss: 2.3303890228271484\n",
      "Epoch: 27/200 Iteration: 15650 Loss: 2.422093391418457\n",
      "Epoch: 27/200 Iteration: 15700 Loss: 2.4331090450286865\n",
      "Epoch: 27/200 Iteration: 15750 Loss: 2.2425827980041504\n",
      "Epoch: 27/200 Iteration: 15800 Loss: 2.765685558319092\n",
      "Epoch: 27/200 Iteration: 15850 Loss: 2.355884075164795\n",
      "Epoch: 27/200 Iteration: 15900 Loss: 2.399610996246338\n",
      "Epoch: 27/200 Iteration: 15950 Loss: 2.6030662059783936\n",
      "Epoch: 27/200 Iteration: 16000 Loss: 2.356410026550293\n",
      "Анна не\n",
      "\n",
      "может жить в своем длинном пальто свет в коляске в этом для них то душевное лиловом, как разговор затронул ее в дом, чтобы застраховать приближение и тактом, взять, Сергеем отчаянным встретиться, которую ставили ей Бетси. – О чем же восемьдесят знаете скучно? Ну, – и не то же, – говорила она, с ее Долли под лампой, было тихо, как и все более для того, чего она, когда она недовольна и не поймет; не могло быть проще бы менее, как он тяготится, времени и решилась сделать любить его. Согласиться за руку на рукав своего бывшего в кабинете, ее здесь. – Со смешанным\n",
      "Epoch: 27/200 Iteration: 16050 Loss: 2.263503074645996\n",
      "Epoch: 27/200 Iteration: 16100 Loss: 2.204502582550049\n",
      "Epoch: 28/200 Iteration: 16150 Loss: 2.1572494506835938\n",
      "Epoch: 28/200 Iteration: 16200 Loss: 2.1565616130828857\n",
      "Epoch: 28/200 Iteration: 16250 Loss: 2.6186788082122803\n",
      "Epoch: 28/200 Iteration: 16300 Loss: 2.4637980461120605\n",
      "Epoch: 28/200 Iteration: 16350 Loss: 1.8606263399124146\n",
      "Epoch: 28/200 Iteration: 16400 Loss: 2.3405487537384033\n",
      "Epoch: 28/200 Iteration: 16450 Loss: 2.331167459487915\n",
      "Epoch: 28/200 Iteration: 16500 Loss: 2.3392937183380127\n",
      "Epoch: 28/200 Iteration: 16550 Loss: 2.453887462615967\n",
      "Epoch: 28/200 Iteration: 16600 Loss: 2.178581953048706\n",
      "Epoch: 28/200 Iteration: 16650 Loss: 2.439383029937744\n",
      "Epoch: 28/200 Iteration: 16700 Loss: 2.4966495037078857\n",
      "Epoch: 29/200 Iteration: 16750 Loss: 2.013110876083374\n",
      "Epoch: 29/200 Iteration: 16800 Loss: 2.228062629699707\n",
      "Epoch: 29/200 Iteration: 16850 Loss: 2.586228847503662\n",
      "Epoch: 29/200 Iteration: 16900 Loss: 2.412450075149536\n",
      "Epoch: 29/200 Iteration: 16950 Loss: 2.1185007095336914\n",
      "Epoch: 29/200 Iteration: 17000 Loss: 2.084057569503784\n",
      "Анна не\n",
      "\n",
      "существует формы, всех так же отыскивал и не верить был особенно удивительно любовью, но он слышал похвалу сказанные не шло на то, чего же не могли жить при каждой бедной и даже даже его лица, а в карете? Славная танцует. Левин подбежал к столу и, видимо, смотрел к ней, и даже элегантности, эту и не той, не получавшие его в столовую. Ему любить. Левин понял, не знал, как она сомневалась в себе его воображении приятеля эти слова и он сказал себе: «Если я поеду в лицо Анны и отнесся глядя в ее лицо. – Не хочу. – подумал он и опять\n",
      "Epoch: 29/200 Iteration: 17050 Loss: 2.1424317359924316\n",
      "Epoch: 29/200 Iteration: 17100 Loss: 2.2693707942962646\n",
      "Epoch: 29/200 Iteration: 17150 Loss: 2.239215612411499\n",
      "Epoch: 29/200 Iteration: 17200 Loss: 2.163919448852539\n",
      "Epoch: 29/200 Iteration: 17250 Loss: 2.2282590866088867\n",
      "Epoch: 30/200 Iteration: 17300 Loss: 2.443441390991211\n",
      "Epoch: 30/200 Iteration: 17350 Loss: 2.109182596206665\n",
      "Epoch: 30/200 Iteration: 17400 Loss: 2.1176698207855225\n",
      "Epoch: 30/200 Iteration: 17450 Loss: 2.7194974422454834\n",
      "Epoch: 30/200 Iteration: 17500 Loss: 1.8789262771606445\n",
      "Epoch: 30/200 Iteration: 17550 Loss: 2.468266010284424\n",
      "Epoch: 30/200 Iteration: 17600 Loss: 2.238640308380127\n",
      "Epoch: 30/200 Iteration: 17650 Loss: 2.473874568939209\n",
      "Epoch: 30/200 Iteration: 17700 Loss: 2.067589521408081\n",
      "Epoch: 30/200 Iteration: 17750 Loss: 2.075427770614624\n",
      "Epoch: 30/200 Iteration: 17800 Loss: 2.396362781524658\n",
      "Epoch: 30/200 Iteration: 17850 Loss: 2.384512186050415\n",
      "Epoch: 31/200 Iteration: 17900 Loss: 2.229004383087158\n",
      "Epoch: 31/200 Iteration: 17950 Loss: 2.17171573638916\n",
      "Epoch: 31/200 Iteration: 18000 Loss: 2.4588799476623535\n",
      "Анна не\n",
      "\n",
      "прошла вперед и торопилась легче иметь положение, в котором бас с занятиями обруч глаз. Ребенок хватал так как они не высказал к ней. – Ну, что? Но мне не имеют не на рубеже, потому я так мало результатов. А я тогда, как ему так полны, что добрался один за адвокатом, не могу ревновать свою мысль… и весела. За процесс 25). Но он всегда с Алексеем Александровичем, что в «Анне но считают помощник, меня пред холодною сейчас лошади слабы выводил сказала Дарья – Нет, ты думаешь, что, я не могу прятаться», и не могу погубить человека. Что наше знакомство не мужик на\n",
      "Epoch: 31/200 Iteration: 18050 Loss: 2.4844164848327637\n",
      "Epoch: 31/200 Iteration: 18100 Loss: 2.535482168197632\n",
      "Epoch: 31/200 Iteration: 18150 Loss: 2.3195958137512207\n",
      "Epoch: 31/200 Iteration: 18200 Loss: 2.6830637454986572\n",
      "Epoch: 31/200 Iteration: 18250 Loss: 2.358999013900757\n",
      "Epoch: 31/200 Iteration: 18300 Loss: 2.4853737354278564\n",
      "Epoch: 31/200 Iteration: 18350 Loss: 2.379950761795044\n",
      "Epoch: 31/200 Iteration: 18400 Loss: 2.141813278198242\n",
      "Epoch: 32/200 Iteration: 18450 Loss: 2.244086265563965\n",
      "Epoch: 32/200 Iteration: 18500 Loss: 2.0320675373077393\n",
      "Epoch: 32/200 Iteration: 18550 Loss: 2.5617740154266357\n",
      "Epoch: 32/200 Iteration: 18600 Loss: 2.6178789138793945\n",
      "Epoch: 32/200 Iteration: 18650 Loss: 2.2860822677612305\n",
      "Epoch: 32/200 Iteration: 18700 Loss: 2.3930704593658447\n",
      "Epoch: 32/200 Iteration: 18750 Loss: 2.2813780307769775\n",
      "Epoch: 32/200 Iteration: 18800 Loss: 2.2355754375457764\n",
      "Epoch: 32/200 Iteration: 18850 Loss: 2.1549782752990723\n",
      "Epoch: 32/200 Iteration: 18900 Loss: 2.200038433074951\n",
      "Epoch: 32/200 Iteration: 18950 Loss: 2.4461452960968018\n",
      "Epoch: 32/200 Iteration: 19000 Loss: 2.2149622440338135\n",
      "Анна не\n",
      "\n",
      "только не в своей неверности, а только что в припадке этом виновата. – Что ж ты не устала? Это я у тебя ужасного в «Эрмитаж»? Первое в деле мужчина и буду следующее: для выгоды делаю. И и вызвала и желая быть слушать и взглядывая – сказала она с ним, имеющий только вместе! русского двух вечером. Несмотря и покорно на его значение. Он взглянул на свою спальню, он перечитал и быстрым шагом, смеясь его. И он верил в его и тогда как он должен углубляться в воде, Вронский был не знал о назначении колебания: почти другою. На этот день был самый мыслей\n",
      "Epoch: 33/200 Iteration: 19050 Loss: 2.432133913040161\n",
      "Epoch: 33/200 Iteration: 19100 Loss: 2.292448043823242\n",
      "Epoch: 33/200 Iteration: 19150 Loss: 2.288980722427368\n",
      "Epoch: 33/200 Iteration: 19200 Loss: 2.3902292251586914\n",
      "Epoch: 33/200 Iteration: 19250 Loss: 2.293668746948242\n",
      "Epoch: 33/200 Iteration: 19300 Loss: 2.1699142456054688\n",
      "Epoch: 33/200 Iteration: 19350 Loss: 2.1959619522094727\n",
      "Epoch: 33/200 Iteration: 19400 Loss: 2.356477737426758\n",
      "Epoch: 33/200 Iteration: 19450 Loss: 2.1764941215515137\n",
      "Epoch: 33/200 Iteration: 19500 Loss: 2.2091867923736572\n",
      "Epoch: 33/200 Iteration: 19550 Loss: 2.080047845840454\n",
      "Epoch: 34/200 Iteration: 19600 Loss: 2.09011173248291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/200 Iteration: 19650 Loss: 2.4556026458740234\n",
      "Epoch: 34/200 Iteration: 19700 Loss: 2.3193788528442383\n",
      "Epoch: 34/200 Iteration: 19750 Loss: 2.2997324466705322\n",
      "Epoch: 34/200 Iteration: 19800 Loss: 2.3531463146209717\n",
      "Epoch: 34/200 Iteration: 19850 Loss: 2.431978464126587\n",
      "Epoch: 34/200 Iteration: 19900 Loss: 2.190730094909668\n",
      "Epoch: 34/200 Iteration: 19950 Loss: 2.26235294342041\n",
      "Epoch: 34/200 Iteration: 20000 Loss: 1.9443612098693848\n",
      "Анна не\n",
      "\n",
      "могла нравиться, но как только не убедит брать их. Кити и что обычные условия жизни. Ты не поверишь, хотя я добродушно, образования: Долли думала о том, что все это лето 1876 г. Дарья с парным Облонских и все оглядывались, ожидая его. Он спросил, что надо не могло ли граница в случае подпустить Кити прежде? сил людей, провожавших в один из жизни Дарьи Александровны, оттого, чего это не казалось, не дворянство. Это чувство надевал в себе глазах. – Вы бы и очень рад, но ты знаешь? Au не хочу хоть не сказала ему только я от старика та господа у других, я\n",
      "Epoch: 34/200 Iteration: 20050 Loss: 2.250332832336426\n",
      "Epoch: 34/200 Iteration: 20100 Loss: 2.1505489349365234\n",
      "Epoch: 34/200 Iteration: 20150 Loss: 2.203740358352661\n",
      "Epoch: 35/200 Iteration: 20200 Loss: 2.425750255584717\n",
      "Epoch: 35/200 Iteration: 20250 Loss: 2.4803576469421387\n",
      "Epoch: 35/200 Iteration: 20300 Loss: 2.5360801219940186\n",
      "Epoch: 35/200 Iteration: 20350 Loss: 2.6188433170318604\n",
      "Epoch: 35/200 Iteration: 20400 Loss: 2.453423023223877\n",
      "Epoch: 35/200 Iteration: 20450 Loss: 2.2662534713745117\n",
      "Epoch: 35/200 Iteration: 20500 Loss: 2.3164310455322266\n",
      "Epoch: 35/200 Iteration: 20550 Loss: 2.3518829345703125\n",
      "Epoch: 35/200 Iteration: 20600 Loss: 2.2492287158966064\n",
      "Epoch: 35/200 Iteration: 20650 Loss: 2.1589863300323486\n",
      "Epoch: 35/200 Iteration: 20700 Loss: 2.2426676750183105\n",
      "Epoch: 36/200 Iteration: 20750 Loss: 1.9011608362197876\n",
      "Epoch: 36/200 Iteration: 20800 Loss: 2.2256221771240234\n",
      "Epoch: 36/200 Iteration: 20850 Loss: 2.354468584060669\n",
      "Epoch: 36/200 Iteration: 20900 Loss: 2.64605975151062\n",
      "Epoch: 36/200 Iteration: 20950 Loss: 2.2970876693725586\n",
      "Epoch: 36/200 Iteration: 21000 Loss: 2.363646984100342\n",
      "Анна не\n",
      "\n",
      "жалела его. Кити не видала Кити, не получавшие ему о своем пении под горою гнев уже ласка, слегка улыбаясь жене. – сказала Кити, подавая ей в то поле уже, которую вы хороши! ее дорогой ценой, и она с кем всегда теперь совсем будешь спокойна, глядя в окно, и, верно, вчера вечером так там, как бы то ты не размышляя. Впрочем, тебе, – сказала она Маше. Он спустил мысли. Но Степан Аркадьич. Вчера он, взял его не в глубине души не было, что Левин вошел с ней и опять веселый улыбкой. С изуродованным положение, и не садясь, стоял с нею получало мнения\n",
      "Epoch: 36/200 Iteration: 21050 Loss: 2.288146495819092\n",
      "Epoch: 36/200 Iteration: 21100 Loss: 2.2232813835144043\n",
      "Epoch: 36/200 Iteration: 21150 Loss: 2.033796548843384\n",
      "Epoch: 36/200 Iteration: 21200 Loss: 2.3717496395111084\n",
      "Epoch: 36/200 Iteration: 21250 Loss: 2.281980514526367\n",
      "Epoch: 36/200 Iteration: 21300 Loss: 2.233893632888794\n",
      "Epoch: 37/200 Iteration: 21350 Loss: 1.9818971157073975\n",
      "Epoch: 37/200 Iteration: 21400 Loss: 2.2392959594726562\n",
      "Epoch: 37/200 Iteration: 21450 Loss: 2.2446200847625732\n",
      "Epoch: 37/200 Iteration: 21500 Loss: 2.380120277404785\n",
      "Epoch: 37/200 Iteration: 21550 Loss: 2.183100700378418\n",
      "Epoch: 37/200 Iteration: 21600 Loss: 2.1054141521453857\n",
      "Epoch: 37/200 Iteration: 21650 Loss: 2.325430393218994\n",
      "Epoch: 37/200 Iteration: 21700 Loss: 2.355050563812256\n",
      "Epoch: 37/200 Iteration: 21750 Loss: 2.1501986980438232\n",
      "Epoch: 37/200 Iteration: 21800 Loss: 2.158324718475342\n",
      "Epoch: 37/200 Iteration: 21850 Loss: 1.7949202060699463\n",
      "Epoch: 38/200 Iteration: 21900 Loss: 2.1370208263397217\n",
      "Epoch: 38/200 Iteration: 21950 Loss: 2.43422794342041\n",
      "Epoch: 38/200 Iteration: 22000 Loss: 2.4793574810028076\n",
      "Анна не\n",
      "\n",
      "думая о нем. У его усов все знакомые и, сказав ее смягчить связи названные его руках Левин чувствовал себя выбитым рубашку и Спинозу, рад уже, что Левин видел за то, что, несмотря и устремлены за него. В поиске он видел еще яснее о том, чтобы ничего достигнуть своей был помолодевший их было ждать представлялись ей в их время для нее одной, чтоб она видела, что это был слишком большой Левину в том, чтобы он не знал, кроме Еноха, она сама поняла о жене, которая противодействие была виновата, я оставил все в тех пор не то, как леди взглядах от няни, ее\n",
      "Epoch: 38/200 Iteration: 22050 Loss: 2.2717113494873047\n",
      "Epoch: 38/200 Iteration: 22100 Loss: 2.019745349884033\n",
      "Epoch: 38/200 Iteration: 22150 Loss: 2.093184232711792\n",
      "Epoch: 38/200 Iteration: 22200 Loss: 2.202275276184082\n",
      "Epoch: 38/200 Iteration: 22250 Loss: 2.048495292663574\n",
      "Epoch: 38/200 Iteration: 22300 Loss: 2.2935538291931152\n",
      "Epoch: 38/200 Iteration: 22350 Loss: 2.222691059112549\n",
      "Epoch: 38/200 Iteration: 22400 Loss: 2.031552314758301\n",
      "Epoch: 38/200 Iteration: 22450 Loss: 2.206374406814575\n",
      "Epoch: 39/200 Iteration: 22500 Loss: 1.9897605180740356\n",
      "Epoch: 39/200 Iteration: 22550 Loss: 2.061138868331909\n",
      "Epoch: 39/200 Iteration: 22600 Loss: 2.516150951385498\n",
      "Epoch: 39/200 Iteration: 22650 Loss: 2.144592046737671\n",
      "Epoch: 39/200 Iteration: 22700 Loss: 2.158635377883911\n",
      "Epoch: 39/200 Iteration: 22750 Loss: 2.2287755012512207\n",
      "Epoch: 39/200 Iteration: 22800 Loss: 2.3385705947875977\n",
      "Epoch: 39/200 Iteration: 22850 Loss: 2.2145557403564453\n",
      "Epoch: 39/200 Iteration: 22900 Loss: 2.238903284072876\n",
      "Epoch: 39/200 Iteration: 22950 Loss: 2.0662691593170166\n",
      "Epoch: 39/200 Iteration: 23000 Loss: 2.006486654281616\n",
      "Анна не\n",
      "\n",
      "то, чего хочет. – Так вели, высоко выражения в этих крыльце все с взволнованною тою обои пред вязанья, большую на лестницу, вечно лесу молодой человек в поддевке, В Ергушове же года и не только не нравилась, которые необходимо в одно чувство с которым она с тем, что она готова к себе обедать, попросить на руку Анны. Все это так приятно. Он не приготовил письмо Анны какое-нибудь словцо, когда он, когда выйдут члены, улыбкой, когда приехал теперь был не было. Не вышел, не мог понять, злится Голенищев докончила его умерла. Многие за роскошь и жизни. Так и не только далека в помещик,\n",
      "Epoch: 40/200 Iteration: 23050 Loss: 2.1863136291503906\n",
      "Epoch: 40/200 Iteration: 23100 Loss: 2.237628221511841\n",
      "Epoch: 40/200 Iteration: 23150 Loss: 2.257437229156494\n",
      "Epoch: 40/200 Iteration: 23200 Loss: 2.6531012058258057\n",
      "Epoch: 40/200 Iteration: 23250 Loss: 1.927252173423767\n",
      "Epoch: 40/200 Iteration: 23300 Loss: 2.3839991092681885\n",
      "Epoch: 40/200 Iteration: 23350 Loss: 2.030977964401245\n",
      "Epoch: 40/200 Iteration: 23400 Loss: 2.2005889415740967\n",
      "Epoch: 40/200 Iteration: 23450 Loss: 2.020803213119507\n",
      "Epoch: 40/200 Iteration: 23500 Loss: 2.0245532989501953\n",
      "Epoch: 40/200 Iteration: 23550 Loss: 2.169724464416504\n",
      "Epoch: 40/200 Iteration: 23600 Loss: 2.0294723510742188\n",
      "Epoch: 41/200 Iteration: 23650 Loss: 2.2497072219848633\n",
      "Epoch: 41/200 Iteration: 23700 Loss: 2.2327194213867188\n",
      "Epoch: 41/200 Iteration: 23750 Loss: 2.3169937133789062\n",
      "Epoch: 41/200 Iteration: 23800 Loss: 2.4010353088378906\n",
      "Epoch: 41/200 Iteration: 23850 Loss: 2.211268186569214\n",
      "Epoch: 41/200 Iteration: 23900 Loss: 2.1907291412353516\n",
      "Epoch: 41/200 Iteration: 23950 Loss: 1.9527403116226196\n",
      "Epoch: 41/200 Iteration: 24000 Loss: 1.9453082084655762\n",
      "Анна не\n",
      "\n",
      "могла понять, я придумаю голубчик. – сказал Сергей Иванович ожидал, как Степан Аркадьич не могла понять этого найти в состоянии верить не были для борьбы почти движение и спокоен в себе хозяйства законы[135]; не так страшна и жизнью, но она не знала… Вернувшись на морозе она забыла в которой он находился, чувствуя все выходило очень приятно, и его службу, но когда, видимо, ожидал ей поражен странною делают, в вагоне, и соборовали. Что? в деревню в своих страданиях этого свидания в садик, напоминает себе труда было тысячи. Когда доктора вышла за план Левина для свадьбы посланника прекратился, удивительные об общей воинской повинности[87].\n",
      "Epoch: 41/200 Iteration: 24050 Loss: 2.2044200897216797\n",
      "Epoch: 41/200 Iteration: 24100 Loss: 2.3548054695129395\n",
      "Epoch: 41/200 Iteration: 24150 Loss: 1.935323715209961\n",
      "Epoch: 42/200 Iteration: 24200 Loss: 2.3492257595062256\n",
      "Epoch: 42/200 Iteration: 24250 Loss: 2.648214817047119\n",
      "Epoch: 42/200 Iteration: 24300 Loss: 2.201263189315796\n",
      "Epoch: 42/200 Iteration: 24350 Loss: 2.4392290115356445\n",
      "Epoch: 42/200 Iteration: 24400 Loss: 2.2754006385803223\n",
      "Epoch: 42/200 Iteration: 24450 Loss: 2.7074928283691406\n",
      "Epoch: 42/200 Iteration: 24500 Loss: 2.165989875793457\n",
      "Epoch: 42/200 Iteration: 24550 Loss: 2.0215203762054443\n",
      "Epoch: 42/200 Iteration: 24600 Loss: 2.05523681640625\n",
      "Epoch: 42/200 Iteration: 24650 Loss: 2.030714273452759\n",
      "Epoch: 42/200 Iteration: 24700 Loss: 2.3276336193084717\n",
      "Epoch: 42/200 Iteration: 24750 Loss: 2.236710786819458\n",
      "Epoch: 43/200 Iteration: 24800 Loss: 2.023282289505005\n",
      "Epoch: 43/200 Iteration: 24850 Loss: 2.2107584476470947\n",
      "Epoch: 43/200 Iteration: 24900 Loss: 2.5236597061157227\n",
      "Epoch: 43/200 Iteration: 24950 Loss: 2.418473720550537\n",
      "Epoch: 43/200 Iteration: 25000 Loss: 2.1741487979888916\n",
      "Анна не\n",
      "\n",
      "спросила это, не только того, когда она так видела в нем свою улыбку, то на нее очень важное еще сообщить свою прежнюю большим достоинством; но он почти выстрелил по комнате с полным положении Дарья Он заехал по уху, с Левиным XXX Да, встретив ли к этому, – отвечала Варенька. Когда Левин вошел на последнюю меру. стоя и опять вспомнил к нему с нею в жизни Левин, как будто не знал, что в душе своей дочь и что она готовилась в железе и в расположение и с M. Canut и его поцелуи и, содрогаясь, мучалась движения направления от тому, как они упали\n",
      "Epoch: 43/200 Iteration: 25050 Loss: 2.219782829284668\n",
      "Epoch: 43/200 Iteration: 25100 Loss: 2.1666688919067383\n",
      "Epoch: 43/200 Iteration: 25150 Loss: 2.024961233139038\n",
      "Epoch: 43/200 Iteration: 25200 Loss: 2.2497105598449707\n",
      "Epoch: 43/200 Iteration: 25250 Loss: 2.10780668258667\n",
      "Epoch: 43/200 Iteration: 25300 Loss: 2.1420793533325195\n",
      "Epoch: 44/200 Iteration: 25350 Loss: 2.0972609519958496\n",
      "Epoch: 44/200 Iteration: 25400 Loss: 2.2658472061157227\n",
      "Epoch: 44/200 Iteration: 25450 Loss: 2.4628360271453857\n",
      "Epoch: 44/200 Iteration: 25500 Loss: 2.3311378955841064\n",
      "Epoch: 44/200 Iteration: 25550 Loss: 2.186275005340576\n",
      "Epoch: 44/200 Iteration: 25600 Loss: 2.128413677215576\n",
      "Epoch: 44/200 Iteration: 25650 Loss: 2.013599395751953\n",
      "Epoch: 44/200 Iteration: 25700 Loss: 1.9027444124221802\n",
      "Epoch: 44/200 Iteration: 25750 Loss: 1.8824567794799805\n",
      "Epoch: 44/200 Iteration: 25800 Loss: 2.363088846206665\n",
      "Epoch: 44/200 Iteration: 25850 Loss: 2.2213168144226074\n",
      "Epoch: 44/200 Iteration: 25900 Loss: 2.1807048320770264\n",
      "Epoch: 45/200 Iteration: 25950 Loss: 2.180722713470459\n",
      "Epoch: 45/200 Iteration: 26000 Loss: 1.991576910018921\n",
      "Анна не\n",
      "\n",
      "могла продолжать. – Не знаю. Я честолюбив, я должен пожалеть писать ко мне не может, и он! И, несмотря его внимание на стук Левина. И как он вор, есть только средство иметь на место. – Я сообщу вам свое дело в наше самое время, – сказал он, как будто испугался статьи говорил, как это было? и мы не можем доказывать, – как же? Я говорю по-французски, Очень богат, тяжело, которая свободный весь его нынче переехала в шесть. и холодным улыбку тем, чтобы иметь влияния без меня и встретила меня прелесть, но я думаю, что у нас дети. И мы Левин. –\n",
      "Epoch: 45/200 Iteration: 26050 Loss: 2.340276002883911\n",
      "Epoch: 45/200 Iteration: 26100 Loss: 2.1133317947387695\n",
      "Epoch: 45/200 Iteration: 26150 Loss: 2.2468128204345703\n",
      "Epoch: 45/200 Iteration: 26200 Loss: 2.1183950901031494\n",
      "Epoch: 45/200 Iteration: 26250 Loss: 2.2725093364715576\n",
      "Epoch: 45/200 Iteration: 26300 Loss: 1.9876670837402344\n",
      "Epoch: 45/200 Iteration: 26350 Loss: 2.0946874618530273\n",
      "Epoch: 45/200 Iteration: 26400 Loss: 2.271368980407715\n",
      "Epoch: 45/200 Iteration: 26450 Loss: 1.659244179725647\n",
      "Epoch: 46/200 Iteration: 26500 Loss: 2.301814317703247\n",
      "Epoch: 46/200 Iteration: 26550 Loss: 1.8952229022979736\n",
      "Epoch: 46/200 Iteration: 26600 Loss: 2.106760025024414\n",
      "Epoch: 46/200 Iteration: 26650 Loss: 2.505143404006958\n",
      "Epoch: 46/200 Iteration: 26700 Loss: 2.4395668506622314\n",
      "Epoch: 46/200 Iteration: 26750 Loss: 2.0200936794281006\n",
      "Epoch: 46/200 Iteration: 26800 Loss: 2.084364414215088\n",
      "Epoch: 46/200 Iteration: 26850 Loss: 2.097602367401123\n",
      "Epoch: 46/200 Iteration: 26900 Loss: 2.0978846549987793\n",
      "Epoch: 46/200 Iteration: 26950 Loss: 2.376094341278076\n",
      "Epoch: 46/200 Iteration: 27000 Loss: 2.21040678024292\n",
      "Анна не\n",
      "\n",
      "думая про Нильсон! – Очень думаешь? – подумал Сергей Иванович, как показалось Левину. Кити и что это значило; она посмотрела ей в глаза, – и прекрасно, – отвечал Сергей Иванович и он повернулся к Вронскому, но в глубине службе, что она уже почувствовала его не могла понять, чего не форма Но в это последнее дело, как он въезжал ушла в серебряной Душенька, ее было одобрено чем-нибудь, то странное заблуждение, с корзинкой, Если они, слушая его, он и сказал: с каким он мог сказать ей, и она сама заставил их видеть, чтó значило… чем она приезжала сыну. Она отказала забыть (она тела\n",
      "Epoch: 46/200 Iteration: 27050 Loss: 2.1401607990264893\n",
      "Epoch: 47/200 Iteration: 27100 Loss: 1.9333957433700562\n",
      "Epoch: 47/200 Iteration: 27150 Loss: 1.903913974761963\n",
      "Epoch: 47/200 Iteration: 27200 Loss: 2.4349780082702637\n",
      "Epoch: 47/200 Iteration: 27250 Loss: 2.1998515129089355\n",
      "Epoch: 47/200 Iteration: 27300 Loss: 2.1468071937561035\n",
      "Epoch: 47/200 Iteration: 27350 Loss: 2.1860108375549316\n",
      "Epoch: 47/200 Iteration: 27400 Loss: 2.1527464389801025\n",
      "Epoch: 47/200 Iteration: 27450 Loss: 2.077111005783081\n",
      "Epoch: 47/200 Iteration: 27500 Loss: 2.1857287883758545\n",
      "Epoch: 47/200 Iteration: 27550 Loss: 1.9638382196426392\n",
      "Epoch: 47/200 Iteration: 27600 Loss: 2.036785125732422\n",
      "Epoch: 48/200 Iteration: 27650 Loss: 2.286484718322754\n",
      "Epoch: 48/200 Iteration: 27700 Loss: 1.9646929502487183\n",
      "Epoch: 48/200 Iteration: 27750 Loss: 2.1349117755889893\n",
      "Epoch: 48/200 Iteration: 27800 Loss: 2.1706531047821045\n",
      "Epoch: 48/200 Iteration: 27850 Loss: 2.3536574840545654\n",
      "Epoch: 48/200 Iteration: 27900 Loss: 2.392279624938965\n",
      "Epoch: 48/200 Iteration: 27950 Loss: 2.0099236965179443\n",
      "Epoch: 48/200 Iteration: 28000 Loss: 2.1743531227111816\n",
      "Анна не\n",
      "\n",
      "спросила ему в ту минуту, как бы обмануть глаза. Выражение ей слова, в чем же сотни его и жена и вместе с Сергеем Иванычем эта за то недоумением сказал ее и взгляды и глаза. Вронский слышал, и в особенности на лице Анны Павловны с братом сознанием, не замечая его, оборотившись себе очевидно, не в состоянии следить со ним и потому не был дома, после обеда Левин встретил спешившего ночного Долли стал думать теперь оскорбило его в аптеку, заставила приехавшую с ней девушку на руках радостью он помолчал. ему, но он не переставал любить выговорить в первую гостиную заставляло ее любви и\n",
      "Epoch: 48/200 Iteration: 28050 Loss: 1.8559749126434326\n",
      "Epoch: 48/200 Iteration: 28100 Loss: 2.1071925163269043\n",
      "Epoch: 48/200 Iteration: 28150 Loss: 2.2694222927093506\n",
      "Epoch: 48/200 Iteration: 28200 Loss: 2.0865859985351562\n",
      "Epoch: 49/200 Iteration: 28250 Loss: 2.1838865280151367\n",
      "Epoch: 49/200 Iteration: 28300 Loss: 2.1828341484069824\n",
      "Epoch: 49/200 Iteration: 28350 Loss: 2.4470036029815674\n",
      "Epoch: 49/200 Iteration: 28400 Loss: 2.1788430213928223\n",
      "Epoch: 49/200 Iteration: 28450 Loss: 2.0216126441955566\n",
      "Epoch: 49/200 Iteration: 28500 Loss: 2.2134690284729004\n",
      "Epoch: 49/200 Iteration: 28550 Loss: 2.151111364364624\n",
      "Epoch: 49/200 Iteration: 28600 Loss: 2.266741991043091\n",
      "Epoch: 49/200 Iteration: 28650 Loss: 2.1297848224639893\n",
      "Epoch: 49/200 Iteration: 28700 Loss: 2.2472169399261475\n",
      "Epoch: 49/200 Iteration: 28750 Loss: 2.090930461883545\n",
      "Epoch: 49/200 Iteration: 28800 Loss: 2.095482587814331\n"
     ]
    }
   ],
   "source": [
    "for e in range(50):\n",
    "    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "    state_h, state_c = model.zero_state(flags.batch_size)        \n",
    "    \n",
    "    # Transfer data to device (GPU)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device) \n",
    "    \n",
    "    for x, y in batches:\n",
    "#         print(type(x), x.shape, x)\n",
    "        iteration += 1\n",
    "\n",
    "        # Tell it we are in training mode\n",
    "        model.train()\n",
    "\n",
    "        # Reset all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Transfer data to GPU\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "        logits, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        # Perform back-propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping:   \n",
    "        _ = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), flags.gradients_norm)        \n",
    "\n",
    "        # Update the network's parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print the loss value:\n",
    "        if iteration % 50 == 0:\n",
    "            print('Epoch: {}/{}'.format(e, 200),\n",
    "                  'Iteration: {}'.format(iteration),\n",
    "                  'Loss: {}'.format(loss_value))\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            predict(device, model, [flags.initial_words], n_vocab,\n",
    "                    vocab_to_int, int_to_vocab, top_k=5)\n",
    "#             torch.save(model.state_dict(),\n",
    "#                        'checkpoint_pt/model-{}.pth'.format(iteration))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch_1_5v2",
   "language": "python",
   "name": "pytorch_1_5v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
