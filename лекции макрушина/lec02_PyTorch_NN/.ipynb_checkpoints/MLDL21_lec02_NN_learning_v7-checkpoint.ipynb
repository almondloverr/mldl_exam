{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 2: Обучение искуственных нейронных сетей в PyTorch\n",
    "\n",
    "__Автор: Сергей Вячеславович Макрушин__ e-mail: SVMakrushin@fa.ru \n",
    "\n",
    "Финансовый универсиет, 2021 г. \n",
    "\n",
    "При подготовке лекции использованы материалы:\n",
    "* ...\n",
    "\n",
    "* v0.5 старое название: TCN20_lNNp2_2_v5\n",
    "* v0.6 18.02.2021 \n",
    "* v0.7 22.02.2021 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделы: <a class=\"anchor\" id=\"разделы\"></a>\n",
    "* [Современные методы обучения нейронной сети и обратное распространение ошибки](#современные-методы)\n",
    "* [Обучение модели нейронной сети, алгоритм обратного распространения ошибки](#обратное-распространение)\n",
    "    * [Проблема обучения модели нейронной сети](#проблема-обучения)\n",
    "    * [Проблема поиска градиента](#проблема-поиска)\n",
    "* [Дифференцируемое программирование и реализация обратного распространения ошибки](#дифференцируемое)\n",
    "    * [Автоматическое дифференциирование в PyTorch](#автоматическое-PyTorch)\n",
    "\n",
    "-\n",
    "\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "﻿<style>\r\n",
       "\r\n",
       "\r\n",
       "b.n {\r\n",
       "    font-weight: normal;        \r\n",
       "}\r\n",
       "\r\n",
       "b.grbg {\r\n",
       "    background-color: #a0a0a0;      \r\n",
       "}\r\n",
       "\r\n",
       "b.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "b.b {    \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "b.g {\r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "// add your CSS styling here\r\n",
       "\r\n",
       "list-style: none;\r\n",
       "\r\n",
       "ul.s {\r\n",
       "//    list-style-type: none;\r\n",
       "    list-style: none;\r\n",
       "//    background-color: #ff0000;  \r\n",
       "//    color: #ffff00;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;\r\n",
       "}\r\n",
       "\r\n",
       "li.t {\r\n",
       "    list-style: none;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "*.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "li.t:before {\r\n",
       "    content: \"\\21D2\";    \r\n",
       "//    content: \"►\";\r\n",
       "//    padding-left: -1.2em;    \r\n",
       "    text-indent: -1.2em;    \r\n",
       "    display: block;\r\n",
       "    float: left;\r\n",
       "    \r\n",
       "    \r\n",
       "//    width: 1.2em;\r\n",
       "//    color: #ff0000;\r\n",
       "}\r\n",
       "\r\n",
       "i.m:before {\r\n",
       "    font-style: normal;    \r\n",
       "    content: \"\\21D2\";  \r\n",
       "}\r\n",
       "i.m {\r\n",
       "    font-style: normal; \r\n",
       "}    \r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "/* em {\r\n",
       "    font-style: normal; \r\n",
       "} */\r\n",
       "\r\n",
       "\r\n",
       "em.bl {\r\n",
       "    font-style: normal;     \r\n",
       "    font-weight: bold;        \r\n",
       "}\r\n",
       "\r\n",
       "/* em.grbg {\r\n",
       "    font-style: normal;         \r\n",
       "    background-color: #a0a0a0;      \r\n",
       "} */\r\n",
       "\r\n",
       "em.cr {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cb {    \r\n",
       "    font-style: normal;         \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cg {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "\r\n",
       "em.qs {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.qs::before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #ff0000;    \r\n",
       "    content: \"Q:\";  \r\n",
       "}\r\n",
       "\r\n",
       "em.an {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.an:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"A:\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.nt {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.nt:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Note:\";  \r\n",
       "}    \r\n",
       "    \r\n",
       "em.ex {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.ex:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #00ff00;    \r\n",
       "    content: \"Ex:\";  \r\n",
       "} \r\n",
       "    \r\n",
       "em.df {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.df:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Def:\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.pl {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.pl:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"+\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.mn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.mn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"-\";  \r\n",
       "}        \r\n",
       "\r\n",
       "em.plmn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.plmn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\00B1\";\\\\\"&plusmn;\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.hn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.hn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\21D2\";\\\\\"&rArr;\";  \r\n",
       "}     \r\n",
       "    \r\n",
       "\r\n",
       "#cssTableCenter td, th \r\n",
       "{\r\n",
       "    text-align: center; \r\n",
       "    vertical-align: middle;\r\n",
       "}\r\n",
       "\r\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем стиль для оформления презентации\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"file:./lec_v2.css\")\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Современные методы обучения нейронной сети и обратное распространение ошибки <a class=\"anchor\" id=\"современные-методы\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Применение тензоров:  прямое распространение сигналов и оценка ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Постановка задачи__ \n",
    "\n",
    "* У нас есть набор данных $D$, состоящий из пар $(\\pmb{x}, \\pmb{y})$, где $\\pmb{x}$ - признаки, а $\\pmb{y}$ - правильный ответ. \n",
    "* Модель сети $f_L$, имеющей $L$ слоев с весами $\\pmb{\\theta}$ (совокупность весов нейронов из всех слоев) на этих данных делает некоторые предсказания $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})$\n",
    "* Задана функция ошибки $E$, которую можно подсчитать на каждом примере: $E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$ (например, это может быть квадрат или модуль отклонения $\\hat{\\pmb{y}}$ от $\\pmb{y}$ в случае регрессии или перекрестная энтропия в случае классификации)\n",
    "* Тогда суммарная ошибка на наборе данных $D$ будет функцией от параметров модели: $E(\\pmb{\\theta})$ и определяется как $E(\\pmb{\\theta})=\\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки</b>    \n",
    "</center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямое распространение сигналов__\n",
    "\n",
    "* Модель нейронной сети это иерархия (она может быть простой и очень сложной) связанных (последовательно применяемых) функций слоев:\n",
    "    * т.е. модель сети $f_L$ может быть представленна как суперпозиция из $L$ слоев $h^i\\text{, }i \\in \\{1, \\ldots, L\\}$, каждый из которых параметризуется своими весами $w_i$:\n",
    "$$f_L(\\pmb{x}, \\pmb{\\theta})=f_L(\\pmb{x}, \\pmb{w}_1, \\ldots, \\pmb{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_11.png\" alt=\"многослойный перцептрон с двумя скрытыми слоями\" style=\"width: 600px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример модели сети: многослойный перцептрон с двумя скрытыми слоями</b>    \n",
    "</center> \n",
    "\n",
    "* Прямое распространение сигналов по модели (в частности: нейронной сети) реализуется с помощощью __прямого прохода (forward pass)__: входящая информация (вектор $\\pmb{x}$) распространяется через сеть $f_L$ с учетом весов связей $\\pmb{\\theta}$, расчитывается выходной вектор $\\hat{\\pmb{y}}=f_L(\\pmb{x}, \\pmb{\\theta})$ .\n",
    "    * Каждый слой нейронной сети - это последовательно применяемая функция слоя, которая рассчитывается при помощи операций с тензорами.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_12.png\" alt=\"пример прямого прохода\" style=\"width: 600px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример прямого прохода</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Последовательность операций с тензормаи используется для расчета результата:\n",
    "    * Прямой проход (forward pass): входящая информация (вектор $\\pmb{x}$) распространяется через сеть с учетом весов связей, расчитывается выходной вектор $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})= h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$\n",
    "    * Оценки ошибки $E(\\hat{\\pmb{y}}, \\pmb{y})$ на множестве правильных ответов: $\\pmb{y}$.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em class=\"qs\"></em> Почему для реализации модели ИНС используют тензоры?\n",
    "\n",
    "Рассмотрим примеры моделей (веса):\n",
    "* 1 персептрон (4 входа + константа(смещение, baias) ): \n",
    "```python\n",
    "inputs = torch.tensor([1.0, 2.0, 3.0, 2.5])\n",
    "weights = torch.tensor([0.2, 0.8, -0.5, 1.0, 2.0])```\n",
    "* 1 слой персептронов из 3 нейронов (ось 0), каждый с 4 входами и константой (ось 1):\n",
    "```python\n",
    "inputs = torch.tensor([1.0, 2.0, 3.0, 2.5])\n",
    "weights = torch.tensor([[0.2, 0.8, -0.5, 1.0, 2.0],\n",
    "                        [0.5, -0.91, 0.26, -0.5, 3.0],\n",
    "                        [-0.26, -0.27, 0.17, 0.87, 0.5]])\n",
    "```\n",
    "* 2 слоя персептронов из 3 нйронов и из 2 нейронов\n",
    "```python\n",
    "inputs = torch.tensor([1.0, 2.0, 3.0, 2.5])\n",
    "weights_layer1 = torch.tensor([[0.2, 0.8, -0.5, 1.0, 2.0],\n",
    "                               [0.5, -0.91, 0.26, -0.5, 3.0],\n",
    "                               [-0.26, -0.27, 0.17, 0.87, 0.5]])\n",
    "# выход слоя 1 - вход слоя 2\n",
    "weights2_layer2 = torch.tensor([[0.8, 0.5, 1.1, 2.0],\n",
    "                                [0.4, 0.26, -0.4, 3.0],\n",
    "                                [-0.2, 0.27, 0.17, 0.5]])\n",
    "```\n",
    "* набор входных вектров (batch):\n",
    "```python\n",
    "inputs = torch.tensor([[1.0, 2.0, 3.0, 2.5],\n",
    "                       [-1.1, 3.0, 2.1, 0.8],\n",
    "                       [-2.0, 1.3, 0.1, -1.8],\n",
    "                       [-1.3, 3.2, 1.1, 0.6]])\n",
    "weights = torch.tensor([[0.2, 0.8, -0.5, 1.0, 2.0],\n",
    "           [0.5, -0.91, 0.26, -0.5, 3.0],\n",
    "           [-0.26, -0.27, 0.17, 0.87, 0.5]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица X: \n",
      "tensor([[ 1., 40.],\n",
      "        [ 2., 30.],\n",
      "        [ 3., 20.],\n",
      "        [ 4., 10.]])\n",
      "X.size = torch.Size([4, 2])\n",
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n"
     ]
    }
   ],
   "source": [
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "\n",
    "# вариант исходных данны #1 (2й признак всегда равен 0):\n",
    "# X = torch.tensor([[1., 0.],\n",
    "#                   [2., 0.],\n",
    "#                   [3., 0.],\n",
    "#                   [4., 0.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "# вариант исходных данны #2 (2й признак используется и существенно больше 1го):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "print(f'Матрица X: \\n{X}')\n",
    "print(f'X.size = {X.size()}') \n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "\n",
    "# вариант #1:\n",
    "# w_ans = torch.tensor([2., 0.], dtype=torch.float32)\n",
    "\n",
    "# вариант #2:\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "print(f'w:\\n{w}')\n",
    "\n",
    "# прямое распространение (тут фактически описывается модель):\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Обучение модели нейронной сети, алгоритм обратного распространения ошибки <a class=\"anchor\" id=\"обратное-распространение\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема обучения модели нейронной сети <a class=\"anchor\" id=\"проблема-обучения\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Проблема обучения модели нейронной сети: общий взгляд__\n",
    "\n",
    "* <em class=\"nt\"></em> __основная проблема__ это не применение модели к входным данным $\\pmb{x}$ и оцнка ошибки на правильных ответах $\\pmb{y}$, а __обучение модели__ (опредление наилучших параметров модели $\\pmb{\\theta}$). \n",
    "     * В случае нейронной сети обучение сводится к поиску весов слоев сети $\\pmb{\\theta}=(\\pmb{w}_1, \\ldots, \\pmb{w}_L)$, которые в совокупности являются параметрами модели $\\pmb{\\theta}$.\n",
    "\n",
    "* Формально: цель обучения - найти оптимальное значение параметров $\\theta^{*}$, минимизирующих ошибку на обучающией выборке $D$: \n",
    "$$\\theta^{*} = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ E(\\pmb{\\theta}) = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "* Т.е. задача обучения сводится к задаче оптимизации.\n",
    "    * <em class=\"nt\"></em> На самом деле __все сложнее__: хороший результат на $D$ может плохо обобщаться (модель может давать низкое качество на другой выборке из той же генеральной совокупности) - __проблема переобучения__.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p1_v1.png\" alt=\"Приниципиальная логика обучения нейронной сети\" style=\"width: 800px;\"/><br/>\n",
    "    <b>Приниципиальная логика обучения нейронной сети</b>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямой проход и оценка ошибки__\n",
    "\n",
    "* __Прямой проход__ (forward pass): входящая информация (вектор $\\pmb{x}$) распространяется через сеть с учетом весов связей, расчитывается выходной вектор $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})= h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_12.png\" alt=\"пример прямого прохода\" style=\"width: 300px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример прямого прохода</b>    \n",
    "</center> \n",
    "\n",
    "* __Оценки ошибки__ $E(\\hat{\\pmb{y}}, \\pmb{y})$ на множестве правильных ответов: $\\pmb{y}$.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки в общей логике обучения нейронной сети</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача оптимизации__\n",
    "\n",
    "* Задача: корректировка весов сети (параметров модели $\\pmb{\\theta}$) на основе информации об ошибке на обучающих примерах $E(\\hat{\\pmb{y}}, \\pmb{y})$.\n",
    "    * Решение: использовать методы оптимизации, основанные на __методе градиентного спуска__.\n",
    "    \n",
    "\n",
    "* __Метод градиентныого спуска__ - метод нахождения локального экстремума (минимума или максимума) функции с помощью движения вдоль градиента. В нашем случае шаг метода градиентного спуска выглядит следующим образом:\n",
    "$$\\pmb{\\theta}_t = \\pmb{\\theta}_{t-1}-\\gamma\\nabla_\\theta E(\\pmb{\\theta}_{t-1}) = \\pmb{\\theta}_{t-1}-\\gamma \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} \\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "\n",
    "* <em class=\"nt\"></em> Выполнение на каждом шаге градиентого спуска суммирование по всем $(\\pmb{x}, \\pmb{y}) \\in D$ __обычно слшиком неэффективно__\n",
    "\n",
    "\n",
    "* Для выпуклых функций __задача локальной оптимизации__ - найти локальный минимум (максимум) автоматически превращается в __задачу глобальной оптимизации__ - найти точку, в которой достигается наименьшее (наибольшее) значение функции, то есть самую низкую (высокую) точку среди всех.\n",
    "* Оптимизировать веса одного перцептрона - выпуклая задача, но __для большой нейронной сети  целевая функция не является выпуклой__.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_15.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 500px;\"/><br/>\n",
    "    <b>Пример работы градиентного спуска для функции двух переменных</b>    \n",
    "</center>\n",
    "\n",
    "* У нейронных сетей функция ошибки может задавать __очень сложный ландшафт__ с огромным числом локальных максимумов и минимумов. Это свойство необходимо для обеспечения __выразительности нейронных сетей__, позволяющей им решать так много разных задач.\n",
    "\n",
    "\n",
    "* <em class=\"nt\"></em> для использования методов, основанных на методе градиентного спуска __необходимо знать градиент функции потерь по параметрам модели__: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$. Этот градиент определяет вектор (\"направление\") изменения параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "print(f'w:\\n{w}')\n",
    "\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "# градиент: \n",
    "# рассчитан аналитически по модели и функции потерь:\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2 * (w*x - y) * x\n",
    "def gradient(x, y, y_pred):   \n",
    "#     print(f'''y = {y},\n",
    "#     y_pred = {y_pred},\n",
    "#     (2* (y_pred - y)).unsqueeze(1) = {(2* (y_pred - y)).unsqueeze(1)},\n",
    "#     x = {x},\n",
    "#     ((2* (y_pred - y)).unsqueeze(1) * x) = {((2* (y_pred - y)).unsqueeze(1) * x)},\n",
    "#     ((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0) = {((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0)}''')\n",
    "    return ((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0013\n",
    "\n",
    "# predict = forward pass\n",
    "y_pred = forward(X)\n",
    "\n",
    "# loss\n",
    "l = loss(Y, y_pred)\n",
    "\n",
    "# calculate gradients\n",
    "dw = gradient(X, Y, y_pred)\n",
    "\n",
    "# update weights\n",
    "w -= learning_rate * dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: w = tensor([0.16900, 2.21000]), y_pred = tensor([0., 0., 0., 0.]), loss = 980.00000000\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 5: w = tensor([0.13813, 0.24422]), y_pred = tensor([81.70274, 61.57523, 41.44772, 21.32021]), loss = 646.58898926\n",
      "gradient = tensor([  77.23860, 1378.76135])\n",
      "epoch 10: w = tensor([0.33966, 1.82453]), y_pred = tensor([15.22918, 11.70162,  8.17406,  4.64650]), loss = 427.49359131\n",
      "gradient = tensor([  -89.12970, -1114.91895])\n",
      "epoch 15: w = tensor([0.34364, 0.53338]), y_pred = tensor([68.78387, 52.09386, 35.40386, 18.71385]), loss = 283.42648315\n",
      "gradient = tensor([ 47.01929, 904.69324])\n",
      "epoch 20: w = tensor([0.49880, 1.56850]), y_pred = tensor([25.13912, 19.37721, 13.61530,  7.85340]), loss = 188.61242676\n",
      "gradient = tensor([ -61.92349, -731.13959])\n",
      "epoch 25: w = tensor([0.52316, 0.72007]), y_pred = tensor([60.23328, 45.87371, 31.51414, 17.15457]), loss = 126.13953400\n",
      "gradient = tensor([ 27.57071, 593.68567])\n",
      "epoch 30: w = tensor([0.64554, 1.39771]), y_pred = tensor([31.56791, 24.41181, 17.25570, 10.09960]), loss = 84.91027832\n",
      "gradient = tensor([ -43.72150, -479.40973])\n",
      "epoch 35: w = tensor([0.68103, 0.83984]), y_pred = tensor([54.55604, 41.79293, 29.02982, 16.26671]), loss = 57.64197159\n",
      "gradient = tensor([ 15.14910, 389.64639])\n",
      "epoch 40: w = tensor([0.77979, 1.28313]), y_pred = tensor([35.72054, 27.71401, 19.70748, 11.70095]), loss = 39.55477524\n",
      "gradient = tensor([ -31.46259, -314.29950])\n",
      "epoch 45: w = tensor([0.82057, 0.91600]), y_pred = tensor([50.77066, 39.11558, 27.46050, 15.80541]), loss = 27.51074600\n",
      "gradient = tensor([  7.30249, 255.77905])\n",
      "epoch 50: w = tensor([0.90194, 1.20568]), y_pred = tensor([38.38666, 29.87982, 21.37299, 12.86615]), loss = 19.44941521\n",
      "gradient = tensor([ -23.13505, -206.00873])\n",
      "epoch 55: w = tensor([0.94440, 0.96380]), y_pred = tensor([48.23264, 37.35892, 26.48521, 15.61149]), loss = 14.01713943\n",
      "gradient = tensor([  2.42605, 167.94620])\n",
      "epoch 60: w = tensor([1.01265, 1.15282]), y_pred = tensor([40.08362, 31.30023, 22.51685, 13.73346]), loss = 10.32425213\n",
      "gradient = tensor([ -17.41577, -134.98840])\n",
      "epoch 65: w = tensor([1.05460, 0.99321]), y_pred = tensor([46.51847, 36.20624, 25.89402, 15.58179]), loss = 7.78575230\n",
      "gradient = tensor([ -0.52992, 110.31209])\n",
      "epoch 70: w = tensor([1.11272, 1.11631]), y_pred = tensor([41.15016, 32.23169, 23.31323, 14.39476]), loss = 6.01640034\n",
      "gradient = tensor([-13.43386, -88.41534])\n",
      "epoch 75: w = tensor([1.15289, 1.01076]), y_pred = tensor([45.34982, 35.44982, 25.54981, 15.64980]), loss = 4.76234531\n",
      "gradient = tensor([-2.25095, 72.49085])\n",
      "epoch 80: w = tensor([1.20299, 1.09071]), y_pred = tensor([41.80791, 32.84242, 23.87693, 14.91144]), loss = 3.85587311\n",
      "gradient = tensor([-10.61534, -57.87884])\n",
      "epoch 85: w = tensor([1.24068, 1.02071]), y_pred = tensor([44.54358, 34.95336, 25.36314, 15.77292]), loss = 3.18603969\n",
      "gradient = tensor([-3.18430, 47.66795])\n",
      "epoch 90: w = tensor([1.28429, 1.07244]), y_pred = tensor([42.20184, 33.24281, 24.28377, 15.32473]), loss = 2.67915201\n",
      "gradient = tensor([ -8.58116, -37.85973])\n",
      "epoch 95: w = tensor([1.31920, 1.02583]), y_pred = tensor([43.97911, 34.62745, 25.27578, 15.92412]), loss = 2.28608990\n",
      "gradient = tensor([-3.62110, 31.37226])\n",
      "epoch 100: w = tensor([1.35745, 1.05912]), y_pred = tensor([42.42657, 33.50523, 24.58390, 15.66256]), loss = 1.97393024\n",
      "gradient = tensor([ -7.08051, -24.73838])\n",
      "epoch 105: w = tensor([1.38948, 1.02793]), y_pred = tensor([43.57692, 34.41344, 25.24997, 16.08649]), loss = 1.72041523\n",
      "gradient = tensor([-3.75015, 20.67223])\n",
      "epoch 110: w = tensor([1.42322, 1.04919]), y_pred = tensor([42.54377, 33.67719, 24.81061, 15.94404]), loss = 1.51038074\n",
      "gradient = tensor([ -5.94694, -16.14055])\n",
      "epoch 115: w = tensor([1.45243, 1.02818]), y_pred = tensor([43.28445, 34.27287, 25.26129, 16.24971]), loss = 1.33336711\n",
      "gradient = tensor([-3.69355, 13.64342])\n",
      "epoch 120: w = tensor([1.48233, 1.04161]), y_pred = tensor([42.59350, 33.78979, 24.98609, 16.18238]), loss = 1.18204284\n",
      "gradient = tensor([ -5.06955, -10.51022])\n",
      "epoch 125: w = tensor([1.50884, 1.02733]), y_pred = tensor([43.06691, 34.18049, 25.29408, 16.40766]), loss = 1.05118954\n",
      "gradient = tensor([-3.52962,  9.02462])\n",
      "epoch 130: w = tensor([1.53541, 1.03567]), y_pred = tensor([42.60181, 33.86351, 25.12522, 16.38693]), loss = 0.93700927\n",
      "gradient = tensor([-4.37389, -6.82430])\n",
      "epoch 135: w = tensor([1.55940, 1.02587]), y_pred = tensor([42.90114, 34.11973, 25.33832, 16.55691]), loss = 0.83667958\n",
      "gradient = tensor([-3.30841,  5.98641])\n",
      "epoch 140: w = tensor([1.58308, 1.03092]), y_pred = tensor([42.58543, 33.91173, 25.23804, 16.56435]), loss = 0.74805033\n",
      "gradient = tensor([-3.80980, -4.41338])\n",
      "epoch 145: w = tensor([1.60474, 1.02409]), y_pred = tensor([42.77172, 34.07974, 25.38776, 16.69577]), loss = 0.66943973\n",
      "gradient = tensor([-3.06122,  3.98693])\n",
      "epoch 150: w = tensor([1.62588, 1.02703]), y_pred = tensor([42.55507, 33.94322, 25.33137, 16.71952]), loss = 0.59950614\n",
      "gradient = tensor([-3.34315, -2.83900])\n",
      "epoch 155: w = tensor([1.64540, 1.02219]), y_pred = tensor([42.66827, 34.05339, 25.43850, 16.82361]), loss = 0.53715116\n",
      "gradient = tensor([-2.80750,  2.66936])\n",
      "epoch 160: w = tensor([1.66429, 1.02378]), y_pred = tensor([42.51758, 33.96376, 25.40994, 16.85612]), loss = 0.48146009\n",
      "gradient = tensor([-2.95031, -1.81206])\n",
      "epoch 165: w = tensor([1.68186, 1.02029]), y_pred = tensor([42.58377, 34.03600, 25.48822, 16.94045]), loss = 0.43166173\n",
      "gradient = tensor([-2.55889,  1.79972])\n",
      "epoch 170: w = tensor([1.69877, 1.02103]), y_pred = tensor([42.47721, 33.97713, 25.47704, 16.97695]), loss = 0.38709140\n",
      "gradient = tensor([-2.61480, -1.14371])\n",
      "epoch 175: w = tensor([1.71457, 1.01845]), y_pred = tensor([42.51338, 34.02448, 25.53557, 17.04667]), loss = 0.34717295\n",
      "gradient = tensor([-2.32213,  1.22391])\n",
      "epoch 180: w = tensor([1.72971, 1.01867]), y_pred = tensor([42.43659, 33.98581, 25.53503, 17.08425]), loss = 0.31140509\n",
      "gradient = tensor([-2.32486, -0.70952])\n",
      "epoch 185: w = tensor([1.74391, 1.01672]), y_pred = tensor([42.45380, 34.01683, 25.57986, 17.14289]), loss = 0.27934361\n",
      "gradient = tensor([-2.10069,  0.84160])\n",
      "epoch 190: w = tensor([1.75748, 1.01662]), y_pred = tensor([42.39725, 33.99142, 25.58560, 17.17977]), loss = 0.25059801\n",
      "gradient = tensor([-2.07201, -0.42871])\n",
      "epoch 195: w = tensor([1.77024, 1.01511]), y_pred = tensor([42.40269, 34.01173, 25.62078, 17.22982]), loss = 0.22481927\n",
      "gradient = tensor([-1.89610,  0.58674])\n",
      "epoch 200: w = tensor([1.78240, 1.01482]), y_pred = tensor([42.36006, 33.99504, 25.63001, 17.26499]), loss = 0.20169993\n",
      "gradient = tensor([-1.84993, -0.24814])\n",
      "epoch 205: w = tensor([1.79385, 1.01363]), y_pred = tensor([42.35838, 34.00832, 25.65827, 17.30822]), loss = 0.18096074\n",
      "gradient = tensor([-1.70864,  0.41626])\n",
      "epoch 210: w = tensor([1.80476, 1.01324]), y_pred = tensor([42.32542, 33.99732, 25.66922, 17.34113]), loss = 0.16235729\n",
      "gradient = tensor([-1.65388, -0.13396])\n",
      "epoch 215: w = tensor([1.81504, 1.01227]), y_pred = tensor([42.31963, 34.00603, 25.69242, 17.37882]), loss = 0.14566770\n",
      "gradient = tensor([-1.53788,  0.30142])\n",
      "epoch 220: w = tensor([1.82482, 1.01185]), y_pred = tensor([42.29353, 33.99876, 25.70400, 17.40924]), loss = 0.13069558\n",
      "gradient = tensor([-1.48000, -0.06169])\n",
      "epoch 225: w = tensor([1.83405, 1.01104]), y_pred = tensor([42.28552, 34.00446, 25.72339, 17.44233]), loss = 0.11726224\n",
      "gradient = tensor([-1.38304,  0.22278])\n",
      "epoch 230: w = tensor([1.84282, 1.01060]), y_pred = tensor([42.26440, 33.99967, 25.73494, 17.47021]), loss = 0.10521053\n",
      "gradient = tensor([-1.32529, -0.01637])\n",
      "epoch 235: w = tensor([1.85111, 1.00993]), y_pred = tensor([42.25534, 34.00337, 25.75140, 17.49942]), loss = 0.09439759\n",
      "gradient = tensor([-1.24302,  0.16842])\n",
      "epoch 240: w = tensor([1.85897, 1.00950]), y_pred = tensor([42.23791, 34.00021, 25.76252, 17.52483]), loss = 0.08469677\n",
      "gradient = tensor([-1.18740,  0.01068])\n",
      "epoch 245: w = tensor([1.86641, 1.00892]), y_pred = tensor([42.22856, 34.00261, 25.77667, 17.55073]), loss = 0.07599217\n",
      "gradient = tensor([-1.11665,  0.13068])\n",
      "epoch 250: w = tensor([1.87346, 1.00851]), y_pred = tensor([42.21390, 34.00052, 25.78715, 17.57377]), loss = 0.06818256\n",
      "gradient = tensor([-1.06427,  0.02612])\n",
      "epoch 255: w = tensor([1.88014, 1.00801]), y_pred = tensor([42.20472, 34.00208, 25.79945, 17.59681]), loss = 0.06117591\n",
      "gradient = tensor([-1.00278,  0.10420])\n",
      "epoch 260: w = tensor([1.88647, 1.00763]), y_pred = tensor([42.19220, 34.00068, 25.80916, 17.61764]), loss = 0.05488885\n",
      "gradient = tensor([-0.95419,  0.03394])\n",
      "epoch 265: w = tensor([1.89246, 1.00719]), y_pred = tensor([42.18345, 34.00169, 25.81994, 17.63818]), loss = 0.04924808\n",
      "gradient = tensor([-0.90031,  0.08475])\n",
      "epoch 270: w = tensor([1.89813, 1.00684]), y_pred = tensor([42.17264, 34.00076, 25.82887, 17.65699]), loss = 0.04418735\n",
      "gradient = tensor([-0.85564,  0.03786])\n",
      "epoch 275: w = tensor([1.90351, 1.00646]), y_pred = tensor([42.16444, 34.00140, 25.83836, 17.67532]), loss = 0.03964623\n",
      "gradient = tensor([-0.80819,  0.07013])\n",
      "epoch 280: w = tensor([1.90860, 1.00614]), y_pred = tensor([42.15502, 34.00077, 25.84652, 17.69227]), loss = 0.03557184\n",
      "gradient = tensor([-0.76740,  0.03853])\n",
      "epoch 285: w = tensor([1.91342, 1.00580]), y_pred = tensor([42.14744, 34.00118, 25.85492, 17.70866]), loss = 0.03191639\n",
      "gradient = tensor([-0.72539,  0.05912])\n",
      "epoch 290: w = tensor([1.91799, 1.00550]), y_pred = tensor([42.13918, 34.00076, 25.86233, 17.72391]), loss = 0.02863660\n",
      "gradient = tensor([-0.68833,  0.03774])\n",
      "epoch 295: w = tensor([1.92232, 1.00520]), y_pred = tensor([42.13222, 34.00101, 25.86980, 17.73858]), loss = 0.02569385\n",
      "gradient = tensor([-0.65102,  0.05041])\n",
      "epoch 300: w = tensor([1.92642, 1.00494]), y_pred = tensor([42.12493, 34.00072, 25.87651, 17.75230]), loss = 0.02305340\n",
      "gradient = tensor([-0.61746,  0.03589])\n",
      "epoch 305: w = tensor([1.93030, 1.00467]), y_pred = tensor([42.11858, 34.00087, 25.88315, 17.76544]), loss = 0.02068412\n",
      "gradient = tensor([-0.58424,  0.04342])\n",
      "epoch 310: w = tensor([1.93398, 1.00443]), y_pred = tensor([42.11213, 34.00067, 25.88922, 17.77776]), loss = 0.01855873\n",
      "gradient = tensor([-0.55391,  0.03366])\n",
      "epoch 315: w = tensor([1.93747, 1.00419]), y_pred = tensor([42.10637, 34.00076, 25.89515, 17.78953]), loss = 0.01665138\n",
      "gradient = tensor([-0.52427,  0.03789])\n",
      "epoch 320: w = tensor([1.94077, 1.00397]), y_pred = tensor([42.10064, 34.00063, 25.90062, 17.80061]), loss = 0.01494033\n",
      "gradient = tensor([-0.49691,  0.03132])\n",
      "epoch 325: w = tensor([1.94389, 1.00376]), y_pred = tensor([42.09541, 34.00066, 25.90591, 17.81116]), loss = 0.01340491\n",
      "gradient = tensor([-0.47046,  0.03308])\n",
      "epoch 330: w = tensor([1.94685, 1.00356]), y_pred = tensor([42.09031, 34.00057, 25.91084, 17.82110]), loss = 0.01202754\n",
      "gradient = tensor([-0.44581,  0.02867])\n",
      "epoch 335: w = tensor([1.94966, 1.00337]), y_pred = tensor([42.08558, 34.00058, 25.91557, 17.83055]), loss = 0.01079139\n",
      "gradient = tensor([-0.42217,  0.02872])\n",
      "epoch 340: w = tensor([1.95231, 1.00320]), y_pred = tensor([42.08104, 34.00053, 25.92001, 17.83949]), loss = 0.00968240\n",
      "gradient = tensor([-0.39996,  0.02622])\n",
      "epoch 345: w = tensor([1.95483, 1.00303]), y_pred = tensor([42.07679, 34.00052, 25.92424, 17.84797]), loss = 0.00868748\n",
      "gradient = tensor([-0.37878,  0.02594])\n",
      "epoch 350: w = tensor([1.95722, 1.00287]), y_pred = tensor([42.07271, 34.00047, 25.92823, 17.85599]), loss = 0.00779471\n",
      "gradient = tensor([-0.35886,  0.02347])\n",
      "epoch 355: w = tensor([1.95947, 1.00272]), y_pred = tensor([42.06890, 34.00047, 25.93203, 17.86359]), loss = 0.00699359\n",
      "gradient = tensor([-0.33986,  0.02316])\n",
      "epoch 360: w = tensor([1.96161, 1.00257]), y_pred = tensor([42.06524, 34.00042, 25.93560, 17.87078]), loss = 0.00627505\n",
      "gradient = tensor([-0.32198,  0.02117])\n",
      "epoch 365: w = tensor([1.96364, 1.00244]), y_pred = tensor([42.06182, 34.00042, 25.93901, 17.87761]), loss = 0.00563018\n",
      "gradient = tensor([-0.30494,  0.02073])\n",
      "epoch 370: w = tensor([1.96556, 1.00231]), y_pred = tensor([42.05854, 34.00038, 25.94222, 17.88406]), loss = 0.00505164\n",
      "gradient = tensor([-0.28889,  0.01907])\n",
      "epoch 375: w = tensor([1.96737, 1.00219]), y_pred = tensor([42.05547, 34.00037, 25.94528, 17.89019]), loss = 0.00453248\n",
      "gradient = tensor([-0.27360,  0.01865])\n",
      "epoch 380: w = tensor([1.96910, 1.00207]), y_pred = tensor([42.05252, 34.00034, 25.94816, 17.89598]), loss = 0.00406663\n",
      "gradient = tensor([-0.25919,  0.01716])\n",
      "epoch 385: w = tensor([1.97073, 1.00196]), y_pred = tensor([42.04977, 34.00034, 25.95090, 17.90147]), loss = 0.00364873\n",
      "gradient = tensor([-0.24547,  0.01686])\n",
      "epoch 390: w = tensor([1.97227, 1.00186]), y_pred = tensor([42.04712, 34.00031, 25.95349, 17.90667]), loss = 0.00327375\n",
      "gradient = tensor([-0.23257,  0.01525])\n",
      "epoch 395: w = tensor([1.97374, 1.00176]), y_pred = tensor([42.04465, 34.00030, 25.95595, 17.91160]), loss = 0.00293732\n",
      "gradient = tensor([-0.22025,  0.01501])\n",
      "epoch 400: w = tensor([1.97512, 1.00167]), y_pred = tensor([42.04229, 34.00028, 25.95827, 17.91626]), loss = 0.00263548\n",
      "gradient = tensor([-0.20865,  0.01399])\n",
      "epoch 405: w = tensor([1.97643, 1.00158]), y_pred = tensor([42.04005, 34.00026, 25.96047, 17.92068]), loss = 0.00236459\n",
      "gradient = tensor([-0.19764,  0.01316])\n",
      "epoch 410: w = tensor([1.97768, 1.00150]), y_pred = tensor([42.03794, 34.00025, 25.96256, 17.92487]), loss = 0.00212172\n",
      "gradient = tensor([-0.18721,  0.01259])\n",
      "epoch 415: w = tensor([1.97886, 1.00142]), y_pred = tensor([42.03594, 34.00024, 25.96453, 17.92883]), loss = 0.00190360\n",
      "gradient = tensor([-0.17732,  0.01196])\n",
      "epoch 420: w = tensor([1.97997, 1.00134]), y_pred = tensor([42.03405, 34.00023, 25.96641, 17.93259]), loss = 0.00170798\n",
      "gradient = tensor([-0.16796,  0.01138])\n",
      "epoch 425: w = tensor([1.98103, 1.00127]), y_pred = tensor([42.03225, 34.00022, 25.96818, 17.93615]), loss = 0.00153252\n",
      "gradient = tensor([-0.15909,  0.01087])\n",
      "epoch 430: w = tensor([1.98203, 1.00120]), y_pred = tensor([42.03054, 34.00020, 25.96986, 17.93951]), loss = 0.00137501\n",
      "gradient = tensor([-0.15071,  0.01001])\n",
      "epoch 435: w = tensor([1.98298, 1.00114]), y_pred = tensor([42.02893, 34.00019, 25.97145, 17.94271]), loss = 0.00123367\n",
      "gradient = tensor([-0.14276,  0.00950])\n",
      "epoch 440: w = tensor([1.98388, 1.00108]), y_pred = tensor([42.02741, 34.00019, 25.97296, 17.94573]), loss = 0.00110684\n",
      "gradient = tensor([-0.13520,  0.00934])\n",
      "epoch 445: w = tensor([1.98473, 1.00102]), y_pred = tensor([42.02594, 34.00016, 25.97437, 17.94859]), loss = 0.00099315\n",
      "gradient = tensor([-0.12813,  0.00793])\n",
      "epoch 450: w = tensor([1.98553, 1.00097]), y_pred = tensor([42.02460, 34.00017, 25.97574, 17.95131]), loss = 0.00089105\n",
      "gradient = tensor([-0.12130,  0.00848])\n",
      "epoch 455: w = tensor([1.98630, 1.00092]), y_pred = tensor([42.02329, 34.00015, 25.97701, 17.95388]), loss = 0.00079952\n",
      "gradient = tensor([-0.11493,  0.00759])\n",
      "epoch 460: w = tensor([1.98702, 1.00087]), y_pred = tensor([42.02205, 34.00014, 25.97823, 17.95631]), loss = 0.00071731\n",
      "gradient = tensor([-0.10887,  0.00698])\n",
      "epoch 465: w = tensor([1.98771, 1.00082]), y_pred = tensor([42.02092, 34.00015, 25.97939, 17.95862]), loss = 0.00064363\n",
      "gradient = tensor([-0.10306,  0.00759])\n",
      "epoch 470: w = tensor([1.98835, 1.00078]), y_pred = tensor([42.01978, 34.00011, 25.98046, 17.96080]), loss = 0.00057748\n",
      "gradient = tensor([-0.09772,  0.00578])\n",
      "epoch 475: w = tensor([1.98897, 1.00074]), y_pred = tensor([42.01876, 34.00014, 25.98150, 17.96288]), loss = 0.00051811\n",
      "gradient = tensor([-0.09247,  0.00677])\n",
      "epoch 480: w = tensor([1.98955, 1.00070]), y_pred = tensor([42.01775, 34.00011, 25.98247, 17.96483]), loss = 0.00046487\n",
      "gradient = tensor([-0.08766,  0.00547])\n",
      "epoch 485: w = tensor([1.99010, 1.00066]), y_pred = tensor([42.01683, 34.00012, 25.98340, 17.96669]), loss = 0.00041709\n",
      "gradient = tensor([-0.08298,  0.00595])\n",
      "epoch 490: w = tensor([1.99063, 1.00063]), y_pred = tensor([42.01592, 34.00009, 25.98427, 17.96844]), loss = 0.00037425\n",
      "gradient = tensor([-0.07866,  0.00471])\n",
      "epoch 495: w = tensor([1.99112, 1.00060]), y_pred = tensor([42.01510, 34.00010, 25.98511, 17.97011]), loss = 0.00033581\n",
      "gradient = tensor([-0.07447,  0.00512])\n",
      "epoch 500: w = tensor([1.99159, 1.00056]), y_pred = tensor([42.01431, 34.00010, 25.98589, 17.97169]), loss = 0.00030129\n",
      "gradient = tensor([-0.07053,  0.00497])\n",
      "epoch 505: w = tensor([1.99203, 1.00053]), y_pred = tensor([42.01354, 34.00008, 25.98663, 17.97318]), loss = 0.00027033\n",
      "gradient = tensor([-0.06684,  0.00425])\n",
      "epoch 510: w = tensor([1.99245, 1.00051]), y_pred = tensor([42.01283, 34.00009, 25.98734, 17.97460]), loss = 0.00024254\n",
      "gradient = tensor([-0.06329,  0.00439])\n",
      "epoch 515: w = tensor([1.99285, 1.00048]), y_pred = tensor([42.01215, 34.00007, 25.98800, 17.97593]), loss = 0.00021764\n",
      "gradient = tensor([-0.05998,  0.00373])\n",
      "epoch 520: w = tensor([1.99323, 1.00045]), y_pred = tensor([42.01151, 34.00008, 25.98864, 17.97721]), loss = 0.00019526\n",
      "gradient = tensor([-0.05680,  0.00376])\n",
      "epoch 525: w = tensor([1.99359, 1.00043]), y_pred = tensor([42.01090, 34.00007, 25.98924, 17.97841]), loss = 0.00017518\n",
      "gradient = tensor([-0.05380,  0.00354])\n",
      "epoch 530: w = tensor([1.99392, 1.00041]), y_pred = tensor([42.01033, 34.00007, 25.98981, 17.97955]), loss = 0.00015719\n",
      "gradient = tensor([-0.05095,  0.00349])\n",
      "epoch 535: w = tensor([1.99425, 1.00039]), y_pred = tensor([42.00978, 34.00006, 25.99035, 17.98063]), loss = 0.00014103\n",
      "gradient = tensor([-0.04827,  0.00313])\n",
      "epoch 540: w = tensor([1.99455, 1.00037]), y_pred = tensor([42.00927, 34.00006, 25.99086, 17.98165]), loss = 0.00012652\n",
      "gradient = tensor([-0.04570,  0.00323])\n",
      "epoch 545: w = tensor([1.99484, 1.00035]), y_pred = tensor([42.00878, 34.00006, 25.99134, 17.98262]), loss = 0.00011353\n",
      "gradient = tensor([-0.04331,  0.00290])\n",
      "epoch 550: w = tensor([1.99511, 1.00033]), y_pred = tensor([42.00831, 34.00005, 25.99179, 17.98354]), loss = 0.00010187\n",
      "gradient = tensor([-0.04103,  0.00267])\n",
      "epoch 555: w = tensor([1.99537, 1.00031]), y_pred = tensor([42.00787, 34.00005, 25.99223, 17.98441]), loss = 0.00009140\n",
      "gradient = tensor([-0.03886,  0.00257])\n",
      "epoch 560: w = tensor([1.99561, 1.00029]), y_pred = tensor([42.00747, 34.00006, 25.99264, 17.98523]), loss = 0.00008202\n",
      "gradient = tensor([-0.03678,  0.00282])\n",
      "epoch 565: w = tensor([1.99584, 1.00028]), y_pred = tensor([42.00705, 34.00004, 25.99302, 17.98600]), loss = 0.00007359\n",
      "gradient = tensor([-0.03490,  0.00187])\n",
      "epoch 570: w = tensor([1.99606, 1.00026]), y_pred = tensor([42.00670, 34.00005, 25.99340, 17.98675]), loss = 0.00006602\n",
      "gradient = tensor([-0.03301,  0.00237])\n",
      "epoch 575: w = tensor([1.99627, 1.00025]), y_pred = tensor([42.00633, 34.00003, 25.99374, 17.98744]), loss = 0.00005925\n",
      "gradient = tensor([-0.03131,  0.00177])\n",
      "epoch 580: w = tensor([1.99647, 1.00024]), y_pred = tensor([42.00601, 34.00005, 25.99408, 17.98811]), loss = 0.00005316\n",
      "gradient = tensor([-0.02962,  0.00223])\n",
      "epoch 585: w = tensor([1.99665, 1.00022]), y_pred = tensor([42.00569, 34.00003, 25.99439, 17.98874]), loss = 0.00004769\n",
      "gradient = tensor([-0.02807,  0.00181])\n",
      "epoch 590: w = tensor([1.99683, 1.00021]), y_pred = tensor([42.00538, 34.00003, 25.99468, 17.98933]), loss = 0.00004279\n",
      "gradient = tensor([-0.02660,  0.00154])\n",
      "epoch 595: w = tensor([1.99700, 1.00020]), y_pred = tensor([42.00511, 34.00004, 25.99497, 17.98989]), loss = 0.00003839\n",
      "gradient = tensor([-0.02516,  0.00196])\n",
      "epoch 600: w = tensor([1.99716, 1.00019]), y_pred = tensor([42.00483, 34.00003, 25.99523, 17.99043]), loss = 0.00003444\n",
      "gradient = tensor([-0.02386,  0.00145])\n",
      "epoch 605: w = tensor([1.99731, 1.00018]), y_pred = tensor([42.00458, 34.00003, 25.99548, 17.99093]), loss = 0.00003091\n",
      "gradient = tensor([-0.02259,  0.00156])\n",
      "epoch 610: w = tensor([1.99745, 1.00017]), y_pred = tensor([42.00435, 34.00003, 25.99572, 17.99141]), loss = 0.00002773\n",
      "gradient = tensor([-0.02138,  0.00179])\n",
      "epoch 615: w = tensor([1.99758, 1.00016]), y_pred = tensor([42.00410, 34.00002, 25.99594, 17.99186]), loss = 0.00002489\n",
      "gradient = tensor([-0.02030,  0.00092])\n",
      "epoch 620: w = tensor([1.99771, 1.00015]), y_pred = tensor([42.00390, 34.00003, 25.99616, 17.99229]), loss = 0.00002233\n",
      "gradient = tensor([-0.01918,  0.00168])\n",
      "epoch 625: w = tensor([1.99783, 1.00015]), y_pred = tensor([42.00368, 34.00002, 25.99636, 17.99270]), loss = 0.00002004\n",
      "gradient = tensor([-0.01821,  0.00098])\n",
      "epoch 630: w = tensor([1.99795, 1.00014]), y_pred = tensor([42.00349, 34.00002, 25.99655, 17.99308]), loss = 0.00001798\n",
      "gradient = tensor([-0.01723,  0.00118])\n",
      "epoch 635: w = tensor([1.99805, 1.00013]), y_pred = tensor([42.00330, 34.00002, 25.99673, 17.99345]), loss = 0.00001613\n",
      "gradient = tensor([-0.01633,  0.00100])\n",
      "epoch 640: w = tensor([1.99816, 1.00012]), y_pred = tensor([42.00313, 34.00002, 25.99691, 17.99379]), loss = 0.00001448\n",
      "gradient = tensor([-0.01547,  0.00095])\n",
      "epoch 645: w = tensor([1.99825, 1.00012]), y_pred = tensor([42.00298, 34.00003, 25.99708, 17.99413]), loss = 0.00001298\n",
      "gradient = tensor([-0.01461,  0.00145])\n",
      "epoch 650: w = tensor([1.99835, 1.00011]), y_pred = tensor([42.00280, 34.00001, 25.99722, 17.99443]), loss = 0.00001165\n",
      "gradient = tensor([-0.01390,  0.00051])\n",
      "epoch 655: w = tensor([1.99843, 1.00010]), y_pred = tensor([42.00267, 34.00002, 25.99738, 17.99473]), loss = 0.00001045\n",
      "gradient = tensor([-0.01312,  0.00114])\n",
      "epoch 660: w = tensor([1.99852, 1.00010]), y_pred = tensor([42.00251, 34.00001, 25.99751, 17.99500]), loss = 0.00000938\n",
      "gradient = tensor([-0.01247,  0.00051])\n",
      "epoch 665: w = tensor([1.99859, 1.00009]), y_pred = tensor([42.00239, 34.00002, 25.99764, 17.99527]), loss = 0.00000841\n",
      "gradient = tensor([-0.01178,  0.00090])\n",
      "epoch 670: w = tensor([1.99867, 1.00009]), y_pred = tensor([42.00225, 34.00001, 25.99776, 17.99552]), loss = 0.00000755\n",
      "gradient = tensor([-0.01119,  0.00039])\n",
      "epoch 675: w = tensor([1.99874, 1.00008]), y_pred = tensor([42.00216, 34.00003, 25.99789, 17.99576]), loss = 0.00000678\n",
      "gradient = tensor([-0.01054,  0.00137])\n",
      "epoch 680: w = tensor([1.99881, 1.00008]), y_pred = tensor([42.00201, 34.00000, 25.99799, 17.99597]), loss = 0.00000608\n",
      "gradient = tensor([-1.00603e-02,  8.58307e-05])\n",
      "epoch 685: w = tensor([1.99887, 1.00008]), y_pred = tensor([42.00194, 34.00002, 25.99811, 17.99619]), loss = 0.00000546\n",
      "gradient = tensor([-0.00946,  0.00114])\n",
      "epoch 690: w = tensor([1.99893, 1.00007]), y_pred = tensor([42.00181, 34.00000, 25.99820, 17.99639]), loss = 0.00000489\n",
      "gradient = tensor([-9.02748e-03,  5.72205e-05])\n",
      "epoch 695: w = tensor([1.99898, 1.00007]), y_pred = tensor([42.00173, 34.00002, 25.99830, 17.99658]), loss = 0.00000439\n",
      "gradient = tensor([-0.00851,  0.00069])\n",
      "epoch 700: w = tensor([1.99904, 1.00006]), y_pred = tensor([42.00164, 34.00002, 25.99839, 17.99677]), loss = 0.00000394\n",
      "gradient = tensor([-0.00805,  0.00076])\n",
      "epoch 705: w = tensor([1.99909, 1.00006]), y_pred = tensor([42.00154, 34.00001, 25.99847, 17.99693]), loss = 0.00000353\n",
      "gradient = tensor([-0.00765,  0.00038])\n",
      "epoch 710: w = tensor([1.99914, 1.00006]), y_pred = tensor([42.00146, 34.00000, 25.99855, 17.99709]), loss = 0.00000317\n",
      "gradient = tensor([-0.00726,  0.00015])\n",
      "epoch 715: w = tensor([1.99918, 1.00005]), y_pred = tensor([42.00139, 34.00002, 25.99863, 17.99725]), loss = 0.00000284\n",
      "gradient = tensor([-0.00684,  0.00063])\n",
      "epoch 720: w = tensor([1.99923, 1.00005]), y_pred = tensor([42.00131, 34.00000, 25.99870, 17.99739]), loss = 0.00000255\n",
      "gradient = tensor([-6.51836e-03,  9.53674e-05])\n",
      "epoch 725: w = tensor([1.99927, 1.00005]), y_pred = tensor([42.00126, 34.00002, 25.99878, 17.99753]), loss = 0.00000229\n",
      "gradient = tensor([-0.00612,  0.00083])\n",
      "epoch 730: w = tensor([1.99931, 1.00005]), y_pred = tensor([42.00117, 34.00000, 25.99883, 17.99766]), loss = 0.00000206\n",
      "gradient = tensor([-0.00586,  0.00000])\n",
      "epoch 735: w = tensor([1.99934, 1.00004]), y_pred = tensor([42.00113, 34.00002, 25.99890, 17.99779]), loss = 0.00000184\n",
      "gradient = tensor([-0.00549,  0.00076])\n",
      "epoch 740: w = tensor([1.99938, 1.00004]), y_pred = tensor([42.00105, 34.00000, 25.99895, 17.99790]), loss = 0.00000166\n",
      "gradient = tensor([-5.25570e-03, -3.81470e-05])\n",
      "epoch 745: w = tensor([1.99941, 1.00004]), y_pred = tensor([42.00101, 34.00001, 25.99901, 17.99801]), loss = 0.00000149\n",
      "gradient = tensor([-0.00494,  0.00058])\n",
      "epoch 750: w = tensor([1.99944, 1.00004]), y_pred = tensor([42.00096, 34.00001, 25.99907, 17.99812]), loss = 0.00000133\n",
      "gradient = tensor([-0.00467,  0.00057])\n",
      "epoch 755: w = tensor([1.99947, 1.00004]), y_pred = tensor([42.00090, 34.00000, 25.99911, 17.99822]), loss = 0.00000119\n",
      "gradient = tensor([-0.00445,  0.00010])\n",
      "epoch 760: w = tensor([1.99950, 1.00003]), y_pred = tensor([42.00086, 34.00001, 25.99916, 17.99831]), loss = 0.00000107\n",
      "gradient = tensor([-0.00419,  0.00046])\n",
      "epoch 765: w = tensor([1.99952, 1.00003]), y_pred = tensor([42.00081, 34.00000, 25.99920, 17.99840]), loss = 0.00000096\n",
      "gradient = tensor([-0.00398,  0.00027])\n",
      "epoch 770: w = tensor([1.99955, 1.00003]), y_pred = tensor([42.00076, 34.00000, 25.99924, 17.99848]), loss = 0.00000086\n",
      "gradient = tensor([-0.00379,  0.00010])\n",
      "epoch 775: w = tensor([1.99957, 1.00003]), y_pred = tensor([42.00073, 34.00001, 25.99929, 17.99857]), loss = 0.00000077\n",
      "gradient = tensor([-0.00357,  0.00038])\n",
      "epoch 780: w = tensor([1.99960, 1.00003]), y_pred = tensor([42.00068, 34.00000, 25.99932, 17.99864]), loss = 0.00000069\n",
      "gradient = tensor([-3.39699e-03,  6.67572e-05])\n",
      "epoch 785: w = tensor([1.99962, 1.00003]), y_pred = tensor([42.00065, 34.00001, 25.99936, 17.99871]), loss = 0.00000062\n",
      "gradient = tensor([-0.00320,  0.00031])\n",
      "epoch 790: w = tensor([1.99964, 1.00002]), y_pred = tensor([42.00062, 34.00001, 25.99940, 17.99878]), loss = 0.00000056\n",
      "gradient = tensor([-0.00303,  0.00041])\n",
      "epoch 795: w = tensor([1.99966, 1.00002]), y_pred = tensor([42.00059, 34.00001, 25.99943, 17.99885]), loss = 0.00000050\n",
      "gradient = tensor([-0.00287,  0.00035])\n",
      "epoch 800: w = tensor([1.99967, 1.00002]), y_pred = tensor([42.00055, 34.00000, 25.99945, 17.99891]), loss = 0.00000045\n",
      "gradient = tensor([-2.73609e-03,  3.81470e-05])\n",
      "epoch 805: w = tensor([1.99969, 1.00002]), y_pred = tensor([42.00053, 34.00001, 25.99949, 17.99896]), loss = 0.00000040\n",
      "gradient = tensor([-0.00257,  0.00031])\n",
      "epoch 810: w = tensor([1.99971, 1.00002]), y_pred = tensor([42.00049, 34.00000, 25.99951, 17.99902]), loss = 0.00000036\n",
      "gradient = tensor([-0.00246,  0.00000])\n",
      "epoch 815: w = tensor([1.99972, 1.00002]), y_pred = tensor([42.00047, 34.00000, 25.99954, 17.99907]), loss = 0.00000033\n",
      "gradient = tensor([-0.00232,  0.00015])\n",
      "epoch 820: w = tensor([1.99974, 1.00002]), y_pred = tensor([42.00045, 34.00000, 25.99956, 17.99912]), loss = 0.00000029\n",
      "gradient = tensor([-0.00219,  0.00019])\n",
      "epoch 825: w = tensor([1.99975, 1.00002]), y_pred = tensor([42.00042, 34.00000, 25.99958, 17.99916]), loss = 0.00000026\n",
      "gradient = tensor([-0.00208,  0.00011])\n",
      "epoch 830: w = tensor([1.99977, 1.00002]), y_pred = tensor([42.00040, 34.00000, 25.99960, 17.99921]), loss = 0.00000024\n",
      "gradient = tensor([-1.97983e-03,  9.53674e-06])\n",
      "epoch 835: w = tensor([1.99978, 1.00001]), y_pred = tensor([42.00038, 34.00000, 25.99963, 17.99925]), loss = 0.00000021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient = tensor([-0.00186,  0.00025])\n",
      "epoch 840: w = tensor([1.99979, 1.00001]), y_pred = tensor([42.00035, 34.00000, 25.99965, 17.99929]), loss = 0.00000019\n",
      "gradient = tensor([-0.00177,  0.00000])\n",
      "epoch 845: w = tensor([1.99980, 1.00001]), y_pred = tensor([42.00034, 34.00000, 25.99967, 17.99933]), loss = 0.00000017\n",
      "gradient = tensor([-0.00167,  0.00015])\n",
      "epoch 850: w = tensor([1.99981, 1.00001]), y_pred = tensor([42.00032, 34.00000, 25.99968, 17.99936]), loss = 0.00000015\n",
      "gradient = tensor([-0.00158,  0.00020])\n",
      "epoch 855: w = tensor([1.99982, 1.00001]), y_pred = tensor([42.00030, 34.00000, 25.99970, 17.99940]), loss = 0.00000014\n",
      "gradient = tensor([-0.00151,  0.00000])\n",
      "epoch 860: w = tensor([1.99983, 1.00001]), y_pred = tensor([42.00028, 33.99999, 25.99971, 17.99943]), loss = 0.00000012\n",
      "gradient = tensor([-0.00145, -0.00031])\n",
      "epoch 865: w = tensor([1.99984, 1.00001]), y_pred = tensor([42.00027, 34.00000, 25.99973, 17.99946]), loss = 0.00000011\n",
      "gradient = tensor([-0.00134,  0.00018])\n",
      "epoch 870: w = tensor([1.99985, 1.00001]), y_pred = tensor([42.00026, 34.00000, 25.99975, 17.99949]), loss = 0.00000010\n",
      "gradient = tensor([-0.00127,  0.00023])\n",
      "epoch 875: w = tensor([1.99986, 1.00001]), y_pred = tensor([42.00023, 33.99999, 25.99975, 17.99951]), loss = 0.00000009\n",
      "gradient = tensor([-0.00124, -0.00038])\n",
      "epoch 880: w = tensor([1.99986, 1.00001]), y_pred = tensor([42.00024, 34.00001, 25.99978, 17.99954]), loss = 0.00000008\n",
      "gradient = tensor([-0.00111,  0.00054])\n",
      "epoch 885: w = tensor([1.99987, 1.00001]), y_pred = tensor([42.00021, 33.99999, 25.99978, 17.99956]), loss = 0.00000007\n",
      "gradient = tensor([-0.00111, -0.00032])\n",
      "epoch 890: w = tensor([1.99988, 1.00001]), y_pred = tensor([42.00024, 34.00002, 25.99981, 17.99960]), loss = 0.00000006\n",
      "gradient = tensor([-0.00095,  0.00114])\n",
      "epoch 895: w = tensor([1.99988, 1.00001]), y_pred = tensor([42.00018, 33.99998, 25.99980, 17.99961]), loss = 0.00000006\n",
      "gradient = tensor([-0.00102, -0.00066])\n",
      "epoch 900: w = tensor([1.99989, 1.00001]), y_pred = tensor([42.00020, 34.00001, 25.99982, 17.99963]), loss = 0.00000005\n",
      "gradient = tensor([-0.00089,  0.00051])\n",
      "epoch 905: w = tensor([1.99990, 1.00001]), y_pred = tensor([42.00016, 33.99999, 25.99982, 17.99965]), loss = 0.00000005\n",
      "gradient = tensor([-0.00092, -0.00057])\n",
      "epoch 910: w = tensor([1.99990, 1.00001]), y_pred = tensor([42.00018, 34.00001, 25.99984, 17.99967]), loss = 0.00000004\n",
      "gradient = tensor([-0.00081,  0.00035])\n",
      "epoch 915: w = tensor([1.99991, 1.00001]), y_pred = tensor([42.00016, 34.00000, 25.99985, 17.99969]), loss = 0.00000004\n",
      "gradient = tensor([-0.00077,  0.00025])\n",
      "epoch 920: w = tensor([1.99991, 1.00001]), y_pred = tensor([42.00015, 34.00000, 25.99985, 17.99970]), loss = 0.00000003\n",
      "gradient = tensor([-7.50542e-04, -2.86102e-05])\n",
      "epoch 925: w = tensor([1.99992, 1.00001]), y_pred = tensor([42.00014, 34.00000, 25.99986, 17.99972]), loss = 0.00000003\n",
      "gradient = tensor([-0.00070,  0.00015])\n",
      "epoch 930: w = tensor([1.99992, 1.00001]), y_pred = tensor([42.00013, 33.99999, 25.99986, 17.99973]), loss = 0.00000003\n",
      "gradient = tensor([-0.00069, -0.00031])\n",
      "epoch 935: w = tensor([1.99992, 1.00000]), y_pred = tensor([42.00013, 34.00000, 25.99988, 17.99975]), loss = 0.00000002\n",
      "gradient = tensor([-0.00062,  0.00025])\n",
      "epoch 940: w = tensor([1.99993, 1.00001]), y_pred = tensor([42.00011, 33.99999, 25.99988, 17.99976]), loss = 0.00000002\n",
      "gradient = tensor([-0.00062, -0.00035])\n",
      "epoch 945: w = tensor([1.99993, 1.00000]), y_pred = tensor([42.00012, 34.00000, 25.99989, 17.99977]), loss = 0.00000002\n",
      "gradient = tensor([-0.00055,  0.00019])\n",
      "epoch 950: w = tensor([1.99994, 1.00000]), y_pred = tensor([42.00010, 34.00000, 25.99989, 17.99978]), loss = 0.00000002\n",
      "gradient = tensor([-0.00055, -0.00019])\n",
      "epoch 955: w = tensor([1.99994, 1.00000]), y_pred = tensor([42.00011, 34.00000, 25.99990, 17.99980]), loss = 0.00000002\n",
      "gradient = tensor([-0.00050,  0.00019])\n",
      "epoch 960: w = tensor([1.99994, 1.00000]), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99981]), loss = 0.00000001\n",
      "gradient = tensor([-4.83513e-04, -2.86102e-05])\n",
      "epoch 965: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00010, 34.00000, 25.99991, 17.99982]), loss = 0.00000001\n",
      "gradient = tensor([-4.52995e-04,  9.53674e-05])\n",
      "epoch 970: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00008, 33.99999, 25.99991, 17.99982]), loss = 0.00000001\n",
      "gradient = tensor([-0.00046, -0.00038])\n",
      "epoch 975: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00010, 34.00001, 25.99993, 17.99984]), loss = 0.00000001\n",
      "gradient = tensor([-0.00037,  0.00063])\n",
      "epoch 980: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00007, 34.00000, 25.99992, 17.99984]), loss = 0.00000001\n",
      "gradient = tensor([-0.00040, -0.00019])\n",
      "epoch 985: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00008, 34.00001, 25.99993, 17.99986]), loss = 0.00000001\n",
      "gradient = tensor([-0.00034,  0.00038])\n",
      "epoch 990: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00007, 34.00000, 25.99993, 17.99986]), loss = 0.00000001\n",
      "gradient = tensor([-3.47137e-04, -9.53674e-06])\n",
      "epoch 995: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00006, 34.00000, 25.99993, 17.99987]), loss = 0.00000001\n",
      "gradient = tensor([-0.00034, -0.00019])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988]), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# вариант #1:\n",
    "# learning_rate = 0.05\n",
    "# n_iters = 20 + 1\n",
    "\n",
    "# вариант #2:\n",
    "learning_rate = 0.0013\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "# основной цикл:\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема поиска градиента <a class=\"anchor\" id=\"проблема-поиска\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <em class=\"qs\"></em> Проблема: как найти градиент для нейронной сети: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$?\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p3_v1.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Проблема поиска градиента в общей логике обучения нейронной сети</b>    \n",
    "</center> \n",
    "\n",
    "* Для решения этой задачи и используется __алгоритм обратного распространения ошибки__ (backpropagation). Суть алгоритма:\n",
    "    * рассчитывается ошибка между выходным вектором сети $\\hat{\\pmb{y}}$ и правильным ответом обучающего примера $\\pmb{y}$\n",
    "    * ошибка распростаняется от результата к источнику (в обратную сторону) для корректировки весов\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_13.png\" alt=\"Пример обратного распространения ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em>Пример обратного распространения ошибки</b>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Рассчет градиента суперпозиции двух функций нескольких переменных__\n",
    "\n",
    "* Сначала рассмотрим подзадачу: как рассчитать градиент для $f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L)$? \n",
    "\n",
    "* Для этого нам нужно будет __рассчитывать градиент суперпозиции (сложной функции)__ состоящей из последовательного применения функций слоев $h^i$.\n",
    "\n",
    "* Вспомним, как рассчитать производную (градиент) суперпозиции нескольких функций.\n",
    "    * Пусть $z=f(y)$, $y=g(x)$\n",
    "    * Тогда производная суперпозиции функций (правило дифференцирования сложной функции (chain rule)): $\\frac{\\mathrm{d} z}{\\mathrm{d} x}=\\frac{\\mathrm{d} z}{\\mathrm{d} y}\\frac{\\mathrm{d} y}{\\mathrm{d} x}$\n",
    "    * Если $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{y} \\in \\mathbb{R}^m$, а $\\mathbf{z} \\in \\mathbb{R}$, то: $\\frac{\\partial z }{\\partial x_i} = \\sum_j \\frac{\\partial z}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i}$\n",
    "\n",
    "<center> \n",
    "    \n",
    "__Примеры рассчета градиента суперпозиции двух функций нескольких переменных:__\n",
    "\n",
    "<img src=\"./img/ann_18.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "Т.е. нам нужны градиенты по всем возможным путям (рассмотренным в обработном порядке) завимиостей переменных.\n",
    "\n",
    "Запись этой же задачи в векторной нотации: \n",
    "* $\\frac{\\mathrm{d} z}{\\mathrm{d} \\mathbf{x}} = \\nabla_x (z)= \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial x_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial x_n} \\end{pmatrix}=\\left ( \\frac{\\mathrm{d} \\mathbf{y}}{\\mathrm{d} \\mathbf{x}} \\right )^T \\cdot \\nabla_y (z) = J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\nabla_y (z)= J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial y_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial y_m} \\end{pmatrix}$    \n",
    "* Где $J$ это Якобиан: $$J(\\mathbf{y}(\\mathbf{x})) = \\begin{pmatrix}\n",
    "    \\dfrac{\\partial y_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_1}{\\partial x_n}\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial y_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_m}{\\partial x_n} \\end{pmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача поиска градиента: $\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})$__\n",
    "\n",
    "* Перейдем от $f_L$ к последовательному рассчету функций слоев $h^i$:\n",
    "$$\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L ), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L), \\mathbf{y})$$\n",
    "\n",
    "* Обозначим через $\\mathbf{a}^l$ результат рассчета функции активации на слое $l$: $\\mathbf{a}^l=h^l(\\mathbf{x}_l,\\mathbf{w}_l)$. Тогда: $\\mathbf{x}_{l+1}=\\mathbf{a}_l$ (вход следущего слоя является результатом рассчета функции активации предыдущего слоя)\n",
    "\n",
    "* Тогда можно записать: $\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_\\theta E(\\mathbf{a}^L, \\mathbf{y})$. Функция потерь $E(\\mathbf{a}^L, \\mathbf{y})$ зависит от $\\mathbf{a}^L$, $\\mathbf{a}^L$ от $\\mathbf{a}^{L-1}$, ..., $\\mathbf{a}^{l+1}$ от $\\mathbf{a}^{l}$\n",
    "\n",
    "* Исходя из этого представления можно градиенты весов $l$-го слоя можно записать как: \n",
    "$$\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\color{blue}{ \\dfrac{\\partial E}{\\partial \\mathbf{a}_L} \\cdot \\dfrac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\cdot \\cdots \\cdot \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}} \\cdot \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$$\n",
    "\n",
    "* Произведение всех сомножетелей кроме последнего является градиентом функции потерь по результатам рассчета функции активации слоя $l$:\n",
    "$$\\color{blue} {\\dfrac{\\partial E}{\\partial \\mathbf{a}_L} \\cdot \\dfrac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\cdot \\cdots \\cdot \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}} = \\color{blue} {\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$$\n",
    "\n",
    "* Тогда:\n",
    "$$\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\left ( \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$$ для рассчета $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ нам нужен только якобиан функции активации $l$-го слоя по параметрам слоя $\\mathbf{w}_{l}$. \n",
    "\n",
    "* Градиент функции потерь по результатам рассчета функции активации слоя  $l$ может быть рассчитан рекурсивно по результатам слоя $l$, собственно тут и происходит __обратное распространение__:\n",
    "$\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}=\\left ( \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}} \\right )^T \\cdot \\dfrac{\\partial E}{\\partial \\mathbf{a}_{l+1}}=\\left ( \\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{x}_{l+1}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l+1}}}$ для рассчета $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{x}_{l+1}}}$ нам нужен только якобиан функции активации $l+1$-го слоя по входным значениям слоя $\\mathbf{x}_{l+1}$\n",
    "\n",
    "* Т.е. чтобы проводить обратное распространение ошибки, нам на каждом слое (например $l$-м) нужно рассчитывать два якобиана:\n",
    "    * якобиан функции активации $l$-го слоя по параметрам слоя $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ - он позволит рассчитать градиент $\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}$ и сделать очередной шаг градиентного спуска для параметров этого слоя: $\\mathbf{w}_l^{t+1} = \\mathbf{w}_l^{t}-\\gamma\\nabla_{w_l} E(\\mathbf{w}^{t})=\\mathbf{w}_l^{t}-\\gamma \\dfrac{\\partial E(\\mathbf{w}^{t})}{\\partial \\mathbf{w}_l}$\n",
    "    * якобиан функции активации $l$-го слоя по входным значениям слоя: $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}}$ - он позволит распространить ошибку на низлежащие слои.\n",
    "\n",
    "Такми образом при обучении нам нужна только __очень локальная информация, содержащаяся в самом слое__. Т.е.:\n",
    "* __нет нобходимости знать как устроены сосоедние слои__: между слоями __очень простой интерфейс__ \n",
    "* т.е. __можно создать модульную архитектуру для слоев нейронной сети__: каждый модуль рассчитывает значение функции активации на основе выходов на прямом проходе и распространяет ошибку пришедшую на выходы на обратном проходе; все модули станадртным образом стыкуются друг с другом\n",
    "* при модульной архитектуре граф нейронной сети может быть очень сложным, но его __рассчет выполняется по одной простой и универсальной схеме__\n",
    "* внутри __модули могут быть сложно устроены, это никак не меняет логику остальных модулей и всего процесса обучения__, главное чтобы модуль корректно выполнял прямой и обратный проход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "##  Дифференцируемое программирование и реализация обратного распространения ошибки <a class=\"anchor\" id=\"дифференцируемое\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Почему Tensor *Flow*?__\n",
    "\n",
    "<em class=\"qs\"></em> Как реализовать __алгоритм обратного распространения ошибки__ удобно для использования в задачах моделирования ИНС?\n",
    "\n",
    "Основная абстракция TensorFlow, PyTorch и других аналогичных библиотеках - __граф потока вычислений__.\n",
    "\n",
    "* Рассматриваемые библиотеки обычно:\n",
    "    1. задают __граф потока вычислений__ (формирует объект отложенных вычислений)\n",
    "    2. запускают __процедуру выполненния отложенных вычислений__ и получает __результаты__ вычислений (в т.ч. ошибку модели).\n",
    "* Возможность в явном виде работать с графом потока вычислений дает большое приемущество для __автоматического решения задачи обратного распространения ошибки__, являющейся составляющей адачей обучения модели ИНС.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ker_6.png\" alt=\"Принцип устройства графа потока вычислений в TensorFlow\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Принцип устройства графа потока вычислений в TensorFlow</b>    \n",
    "</center>\n",
    "\n",
    "* Нейронная сеть это иерархия (она может быть простой и очень сложной) связанных (последовательно применяемых) функций слоев ИНС. Модель сети $f_L$ может быть представленна как суперпозиция из $L$ слоев $h^i\\text{, }i \\in \\{1, \\ldots, L\\}$, каждый из которых параметризуется своими весами $w_i$:\n",
    "$$f_L(\\pmb{x}, \\pmb{\\theta})=f_L(\\pmb{x}, \\pmb{w}_1, \\ldots, \\pmb{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$$\n",
    "\n",
    "* Вычисление функций слоев и взаимосвязи между слоями формируют граф потока вычислений в библиотеке моделирования ИНС.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_16.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Примеры иерархий в нейронных сетях</b>    \n",
    "</center>\n",
    "\n",
    "\n",
    "* По сути, ИНС это композиция модулей, представляющих собой слои нейронной сети:\n",
    "    * если сеть прямого распространения (feedforward), то все просто\n",
    "    * если сеть является направленным ациклическим графом, то существует правильный порядок применения функций\n",
    "    * в случае, если есть циклы, образующие рекуррентные связи, то существуют специальные подходы (будут рассмотрены позднее)\n",
    "\n",
    "\n",
    "* На обратном проходе (при обратном распространении ошибки) нам необходимо __дифференциировать сложную функцию__ многослойной ИНС\n",
    "$$\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L ), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L), \\mathbf{y})$$\n",
    "* алгоритм обратного распространения ошибки позволяет свести эту задачу к дифференциированию составляющих функций, но для этого необходимо __храниить информацию о виде и взаимосвязях функций задействованных в расчете модели ИНС__, именно эта информация и хранится в графе потока вычислений. Это позволяет организовать __автоматическое дифференциирование__ сложной функци многослойной ИНС."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Дифференциируемое программирование__\n",
    "\n",
    "<em class=\"df\"></em> __Дифференциируемое программирование__ (differentiable programming) - парадигма программирования при которой программа (функция рассчета значения) может быть продифференциирована в любой точке, обычно с помощью __автоматического диффиренциирования__. \n",
    "\n",
    "Это свойство позволяет использовать к программе __методы оптимизации основанные на рассчете градиента__, обычно - __методы градиентного спуска__.\n",
    "\n",
    "Дифференциируемое программирование используется в:\n",
    "* глубоком обучении\n",
    "* глубоком обучении комбинированном с физическими моделями в робототехнике\n",
    "* специализированных методах трассировки лучей\n",
    "* обработке изображений\n",
    "\n",
    "Большинство фреймоврков для дифференциируемого программирования использует граф потока вычислений определяющий выполнение программы и ее структуры данных.\n",
    "\n",
    "Основные классы фреймворков для дифференциируемого программирования:\n",
    "* __статические__ - они компилируют граф потока вычислений. Типичные представители: TensorFlow, Theano и др. Плюсы и минусы\n",
    "    * <em class=\"pl\"></em> могут использовать оптимизацию при компиляции\n",
    "    * <em class=\"pl\"></em> легче масштабирются на большие системы\n",
    "    * <em class=\"mn\"></em> статичность ограничевает интерактивность\n",
    "    * <em class=\"mn\"></em> многие программы не могут реализовываться легко (в частности: циклы, рекурсия)\n",
    "* __динамические__ - динамически исполняют граф потока вычислений. Используют перегрузку операторов для записи. Типичные представители: PyTorch, AutoGradrFlow. Плюсы и минусы:\n",
    "    * <em class=\"pl\"></em> более простая и понятная запись программы\n",
    "    * <em class=\"mn\"></em> накладные расходы интерпретатора\n",
    "    * <em class=\"mn\"></em> невозможно использовать оптимизацию компилятора\n",
    "    * <em class=\"mn\"></em> хуже масштабируемость\n",
    "* статическая на основе разбора промежуточного представления синтаксического разбора исходной программы. Пример фрэймоврк Zygote (язык программирования Julia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямой проход__:\n",
    "* Модули из графа обходятся один за одним начиная с узла входных данных и далее по мере готовности всех необходимых входных данных для очередного модуля, который еще не был обойден\n",
    "* Рассчет функций активации для каждого модуля по входным данным: $a_l=h_l(x_l, w_l)$\n",
    "* Промежуточные значения кэшируются, чтобы не рассчитывать их повторно (в сложном графе сети и при обратном проходе)\n",
    "* Выходы одних модулей становятся входами других модулей: $x_{l+1}=a_l$\n",
    "* Последним модулем рассчитывается сумма потерь для входных данных\n",
    "<center> \n",
    "    \n",
    "__Прямой и обратный проход процедуры обучения многослойной ИНС:__\n",
    "\n",
    "<img src=\"./img/ann_19.png\" alt=\"Прямой и обратный проход \" style=\"width: 300px;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "__Обратный проход__:\n",
    "* Сначала должен быть произведен прямой проход. На входе обратного прохода известна сумма потерь.\n",
    "* Строится обратный порядок обхода графа зависимостей модулей.\n",
    "* Модули из графа обходятся один за одним начиная с узла рассчета функции потерь и далее по мере готовности всех необходимых входных данных для очередного модуля, который еще не был обойден \n",
    "* Для каждого модуля рассчитыватся якобиан функции активации по параметрам слоя $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ и якобиан функции активации по входным значениям слоя: $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}}$ \n",
    "* По пришедшму в модуль градиенту ошибки (полученному из модулей использовавших результаты данного модуля на прямом проходе) $\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$ рассчитывается:\n",
    "    * Градиент для шага градиентного спуска по параметрам модуля $w_l$: $\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\left ( \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$\n",
    "    * Градиент ошбки, который передается в модули, поставившие данные в этот модуль во время прямого прохода: $\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l-1}}}=\\left ( \\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 1__\n",
    "\n",
    "<img src=\"./img/bp_2.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 2__\n",
    "\n",
    "<img src=\"./img/bp_3.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 3__\n",
    "\n",
    "<img src=\"./img/bp_4.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 1__\n",
    "\n",
    "<img src=\"./img/bp_5.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 2__\n",
    "\n",
    "<img src=\"./img/bp_6.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Производные популярных функций активации__\n",
    "\n",
    "<img src=\"./img/ann_17.png\" alt=\"Пример\" style=\"width: 500px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 3__\n",
    "\n",
    "<img src=\"./img/bp_7.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":\n",
    "##  Автоматическое дифференциирование в PyTorch <a class=\"anchor\" id=\"автоматическое-PyTorch\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.56797, -1.50591,  0.61205], requires_grad=True)\n",
      "tensor([1.43203, 0.49409, 2.61205], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x00000221B5715088>\n"
     ]
    }
   ],
   "source": [
    "# The autograd package provides automatic differentiation \n",
    "# for all operations on Tensors\n",
    "\n",
    "# requires_grad = True -> tracks all operations on the tensor. \n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.15213,  0.73239, 20.46849], grad_fn=<MulBackward0>)\n",
      "tensor(9.11767, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.86406, 0.98819, 5.22411])\n"
     ]
    }
   ],
   "source": [
    "# Let's compute the gradients with backpropagation\n",
    "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "# The gradient for this tensor will be accumulated into .grad attribute.\n",
    "# It is the partial derivate of the function w.r.t. the tensor\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) # dz/dx\n",
    "\n",
    "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "# It computes partial derivates while applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__Примеры рассчета градиента суперпозиции двух функций нескольких переменных:__\n",
    "\n",
    "<img src=\"./img/ann_18.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "Т.е. нам нужны градиенты по всем возможным путям (рассмотренным в обработном порядке) завимиостей переменных.\n",
    "\n",
    "Запись этой же задачи в векторной нотации: \n",
    "* $\\frac{\\mathrm{d} z}{\\mathrm{d} \\mathbf{x}} = \\nabla_x (z)= \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial x_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial x_n} \\end{pmatrix}=\\left ( \\frac{\\mathrm{d} \\mathbf{y}}{\\mathrm{d} \\mathbf{x}} \\right )^T \\cdot \\nabla_y (z) = J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\nabla_y (z)= J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial y_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial y_m} \\end{pmatrix}$    \n",
    "* Где $J$ это Якобиан: $$J(\\mathbf{y}(\\mathbf{x})) = \\begin{pmatrix}\n",
    "    \\dfrac{\\partial y_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_1}{\\partial x_n}\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial y_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_m}{\\partial x_n} \\end{pmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 271.97305, 1583.08276, 7957.87158], grad_fn=<MulBackward0>)\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Model with non-scalar output:\n",
    "# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \n",
    "# specify a gradient argument that is a tensor of matching shape.\n",
    "# needed for vector-Jacobian product\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "for _ in range(10):\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.04800e+02, 2.04800e+03, 2.04800e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Stop a tensor from tracking history:\n",
    "For example during our training loop when we want to update our weights\n",
    "then this update operation should not be part of the gradient computation\n",
    "* `x.requires_grad_(False)`\n",
    "* `x.detach()`\n",
    "* wrap in `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad = False\n",
      "b.grad_fn = None\n",
      "a.requires_grad = True\n",
      "b.grad_fn = <SumBackward0 object at 0x00000221B5715AC8>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "\n",
    "a = torch.randn(2, 2)\n",
    "print(f'a.requires_grad = {a.requires_grad}')\n",
    "\n",
    "b = ((a * 3) / (a - 1))\n",
    "print(f'b.grad_fn = {b.grad_fn}')\n",
    "      \n",
    "a.requires_grad_(True)\n",
    "print(f'a.requires_grad = {a.requires_grad}')\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(f'b.grad_fn = {b.grad_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = a.detach()\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([0.10000, 0.10000, 0.10000, 0.10000], requires_grad=True)\n",
      "tensor(4.80000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!!\n",
    "# Use .zero_() to empty the gradients before a new optimization step!\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    # 'forward pass'\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "\n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)\n",
    "print(model_output)\n",
    "\n",
    "# Optimizer has zero_grad() method\n",
    "# optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "# During training:\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Автоматическое выполнение обратного прохода с помощью `l.backward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "epoch 0: w = tensor([0.16900, 2.21000], requires_grad=True), y_pred = tensor([0., 0., 0., 0.], grad_fn=<MvBackward>), loss = 980.00000000\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 5: w = tensor([0.13813, 0.24422], requires_grad=True), y_pred = tensor([81.70274, 61.57523, 41.44772, 21.32021], grad_fn=<MvBackward>), loss = 646.58898926\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 10: w = tensor([0.33966, 1.82453], requires_grad=True), y_pred = tensor([15.22920, 11.70164,  8.17407,  4.64651], grad_fn=<MvBackward>), loss = 427.49307251\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 15: w = tensor([0.34364, 0.53338], requires_grad=True), y_pred = tensor([68.78386, 52.09385, 35.40385, 18.71384], grad_fn=<MvBackward>), loss = 283.42614746\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 20: w = tensor([0.49880, 1.56850], requires_grad=True), y_pred = tensor([25.13912, 19.37721, 13.61531,  7.85340], grad_fn=<MvBackward>), loss = 188.61228943\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 25: w = tensor([0.52316, 0.72007], requires_grad=True), y_pred = tensor([60.23326, 45.87370, 31.51413, 17.15457], grad_fn=<MvBackward>), loss = 126.13926697\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 30: w = tensor([0.64554, 1.39771], requires_grad=True), y_pred = tensor([31.56792, 24.41182, 17.25571, 10.09960], grad_fn=<MvBackward>), loss = 84.91012573\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 35: w = tensor([0.68103, 0.83984], requires_grad=True), y_pred = tensor([54.55604, 41.79293, 29.02982, 16.26671], grad_fn=<MvBackward>), loss = 57.64197159\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 40: w = tensor([0.77979, 1.28313], requires_grad=True), y_pred = tensor([35.72054, 27.71401, 19.70748, 11.70095], grad_fn=<MvBackward>), loss = 39.55477905\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 45: w = tensor([0.82057, 0.91600], requires_grad=True), y_pred = tensor([50.77066, 39.11558, 27.46050, 15.80541], grad_fn=<MvBackward>), loss = 27.51074600\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 50: w = tensor([0.90194, 1.20568], requires_grad=True), y_pred = tensor([38.38666, 29.87982, 21.37299, 12.86615], grad_fn=<MvBackward>), loss = 19.44943237\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 55: w = tensor([0.94440, 0.96380], requires_grad=True), y_pred = tensor([48.23264, 37.35892, 26.48521, 15.61149], grad_fn=<MvBackward>), loss = 14.01713943\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 60: w = tensor([1.01265, 1.15282], requires_grad=True), y_pred = tensor([40.08362, 31.30023, 22.51685, 13.73346], grad_fn=<MvBackward>), loss = 10.32425213\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 65: w = tensor([1.05460, 0.99321], requires_grad=True), y_pred = tensor([46.51847, 36.20624, 25.89402, 15.58179], grad_fn=<MvBackward>), loss = 7.78575325\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 70: w = tensor([1.11272, 1.11631], requires_grad=True), y_pred = tensor([41.15015, 32.23169, 23.31322, 14.39476], grad_fn=<MvBackward>), loss = 6.01642036\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 75: w = tensor([1.15289, 1.01076], requires_grad=True), y_pred = tensor([45.34982, 35.44982, 25.54981, 15.64980], grad_fn=<MvBackward>), loss = 4.76234579\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 80: w = tensor([1.20299, 1.09071], requires_grad=True), y_pred = tensor([41.80791, 32.84242, 23.87693, 14.91144], grad_fn=<MvBackward>), loss = 3.85587597\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 85: w = tensor([1.24068, 1.02071], requires_grad=True), y_pred = tensor([44.54358, 34.95336, 25.36314, 15.77292], grad_fn=<MvBackward>), loss = 3.18603969\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 90: w = tensor([1.28429, 1.07244], requires_grad=True), y_pred = tensor([42.20184, 33.24281, 24.28377, 15.32473], grad_fn=<MvBackward>), loss = 2.67915201\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 95: w = tensor([1.31920, 1.02583], requires_grad=True), y_pred = tensor([43.97913, 34.62745, 25.27579, 15.92412], grad_fn=<MvBackward>), loss = 2.28610182\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 100: w = tensor([1.35745, 1.05912], requires_grad=True), y_pred = tensor([42.42656, 33.50523, 24.58389, 15.66256], grad_fn=<MvBackward>), loss = 1.97393394\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 105: w = tensor([1.38948, 1.02793], requires_grad=True), y_pred = tensor([43.57692, 34.41344, 25.24997, 16.08649], grad_fn=<MvBackward>), loss = 1.72041774\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 110: w = tensor([1.42322, 1.04919], requires_grad=True), y_pred = tensor([42.54377, 33.67719, 24.81061, 15.94403], grad_fn=<MvBackward>), loss = 1.51038170\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 115: w = tensor([1.45243, 1.02818], requires_grad=True), y_pred = tensor([43.28445, 34.27287, 25.26129, 16.24971], grad_fn=<MvBackward>), loss = 1.33336782\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 120: w = tensor([1.48233, 1.04161], requires_grad=True), y_pred = tensor([42.59350, 33.78979, 24.98609, 16.18238], grad_fn=<MvBackward>), loss = 1.18204451\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 125: w = tensor([1.50884, 1.02733], requires_grad=True), y_pred = tensor([43.06691, 34.18049, 25.29407, 16.40766], grad_fn=<MvBackward>), loss = 1.05119181\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 130: w = tensor([1.53541, 1.03567], requires_grad=True), y_pred = tensor([42.60181, 33.86351, 25.12522, 16.38693], grad_fn=<MvBackward>), loss = 0.93700927\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 135: w = tensor([1.55940, 1.02587], requires_grad=True), y_pred = tensor([42.90114, 34.11973, 25.33832, 16.55691], grad_fn=<MvBackward>), loss = 0.83668095\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 140: w = tensor([1.58308, 1.03092], requires_grad=True), y_pred = tensor([42.58541, 33.91172, 25.23803, 16.56434], grad_fn=<MvBackward>), loss = 0.74805164\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 145: w = tensor([1.60474, 1.02409], requires_grad=True), y_pred = tensor([42.77173, 34.07975, 25.38776, 16.69578], grad_fn=<MvBackward>), loss = 0.66944206\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 150: w = tensor([1.62588, 1.02703], requires_grad=True), y_pred = tensor([42.55506, 33.94321, 25.33137, 16.71952], grad_fn=<MvBackward>), loss = 0.59950674\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 155: w = tensor([1.64540, 1.02219], requires_grad=True), y_pred = tensor([42.66828, 34.05339, 25.43850, 16.82362], grad_fn=<MvBackward>), loss = 0.53715217\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 160: w = tensor([1.66429, 1.02378], requires_grad=True), y_pred = tensor([42.51758, 33.96376, 25.40994, 16.85612], grad_fn=<MvBackward>), loss = 0.48146015\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 165: w = tensor([1.68186, 1.02029], requires_grad=True), y_pred = tensor([42.58377, 34.03600, 25.48822, 16.94045], grad_fn=<MvBackward>), loss = 0.43166173\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 170: w = tensor([1.69877, 1.02103], requires_grad=True), y_pred = tensor([42.47721, 33.97713, 25.47704, 16.97695], grad_fn=<MvBackward>), loss = 0.38709140\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 175: w = tensor([1.71457, 1.01845], requires_grad=True), y_pred = tensor([42.51338, 34.02448, 25.53557, 17.04667], grad_fn=<MvBackward>), loss = 0.34717295\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 180: w = tensor([1.72971, 1.01867], requires_grad=True), y_pred = tensor([42.43659, 33.98581, 25.53503, 17.08424], grad_fn=<MvBackward>), loss = 0.31140596\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 185: w = tensor([1.74391, 1.01672], requires_grad=True), y_pred = tensor([42.45380, 34.01683, 25.57986, 17.14289], grad_fn=<MvBackward>), loss = 0.27934441\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 190: w = tensor([1.75748, 1.01662], requires_grad=True), y_pred = tensor([42.39725, 33.99142, 25.58560, 17.17977], grad_fn=<MvBackward>), loss = 0.25059801\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 195: w = tensor([1.77024, 1.01511], requires_grad=True), y_pred = tensor([42.40269, 34.01173, 25.62078, 17.22982], grad_fn=<MvBackward>), loss = 0.22481927\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 200: w = tensor([1.78240, 1.01482], requires_grad=True), y_pred = tensor([42.36006, 33.99504, 25.63001, 17.26499], grad_fn=<MvBackward>), loss = 0.20169993\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 205: w = tensor([1.79385, 1.01363], requires_grad=True), y_pred = tensor([42.35838, 34.00832, 25.65827, 17.30822], grad_fn=<MvBackward>), loss = 0.18096074\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 210: w = tensor([1.80476, 1.01324], requires_grad=True), y_pred = tensor([42.32542, 33.99732, 25.66922, 17.34113], grad_fn=<MvBackward>), loss = 0.16235729\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 215: w = tensor([1.81504, 1.01227], requires_grad=True), y_pred = tensor([42.31963, 34.00603, 25.69242, 17.37882], grad_fn=<MvBackward>), loss = 0.14566770\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 220: w = tensor([1.82482, 1.01185], requires_grad=True), y_pred = tensor([42.29353, 33.99876, 25.70400, 17.40924], grad_fn=<MvBackward>), loss = 0.13069558\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 225: w = tensor([1.83405, 1.01104], requires_grad=True), y_pred = tensor([42.28552, 34.00446, 25.72339, 17.44233], grad_fn=<MvBackward>), loss = 0.11726224\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 230: w = tensor([1.84282, 1.01060], requires_grad=True), y_pred = tensor([42.26440, 33.99967, 25.73494, 17.47021], grad_fn=<MvBackward>), loss = 0.10521053\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 235: w = tensor([1.85111, 1.00993], requires_grad=True), y_pred = tensor([42.25534, 34.00337, 25.75140, 17.49942], grad_fn=<MvBackward>), loss = 0.09439759\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 240: w = tensor([1.85897, 1.00950], requires_grad=True), y_pred = tensor([42.23791, 34.00021, 25.76252, 17.52483], grad_fn=<MvBackward>), loss = 0.08469677\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 245: w = tensor([1.86641, 1.00892], requires_grad=True), y_pred = tensor([42.22856, 34.00261, 25.77667, 17.55073], grad_fn=<MvBackward>), loss = 0.07599217\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 250: w = tensor([1.87346, 1.00851], requires_grad=True), y_pred = tensor([42.21390, 34.00052, 25.78715, 17.57377], grad_fn=<MvBackward>), loss = 0.06818256\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 255: w = tensor([1.88014, 1.00801], requires_grad=True), y_pred = tensor([42.20472, 34.00208, 25.79945, 17.59681], grad_fn=<MvBackward>), loss = 0.06117591\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 260: w = tensor([1.88647, 1.00763], requires_grad=True), y_pred = tensor([42.19220, 34.00068, 25.80916, 17.61764], grad_fn=<MvBackward>), loss = 0.05488885\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 265: w = tensor([1.89246, 1.00719], requires_grad=True), y_pred = tensor([42.18345, 34.00169, 25.81994, 17.63818], grad_fn=<MvBackward>), loss = 0.04924808\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 270: w = tensor([1.89813, 1.00684], requires_grad=True), y_pred = tensor([42.17264, 34.00076, 25.82887, 17.65699], grad_fn=<MvBackward>), loss = 0.04418735\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 275: w = tensor([1.90351, 1.00646], requires_grad=True), y_pred = tensor([42.16444, 34.00140, 25.83836, 17.67532], grad_fn=<MvBackward>), loss = 0.03964623\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 280: w = tensor([1.90860, 1.00614], requires_grad=True), y_pred = tensor([42.15502, 34.00077, 25.84652, 17.69227], grad_fn=<MvBackward>), loss = 0.03557184\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 285: w = tensor([1.91342, 1.00580], requires_grad=True), y_pred = tensor([42.14744, 34.00118, 25.85492, 17.70866], grad_fn=<MvBackward>), loss = 0.03191639\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 290: w = tensor([1.91799, 1.00550], requires_grad=True), y_pred = tensor([42.13918, 34.00076, 25.86233, 17.72391], grad_fn=<MvBackward>), loss = 0.02863660\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 295: w = tensor([1.92232, 1.00520], requires_grad=True), y_pred = tensor([42.13222, 34.00101, 25.86980, 17.73858], grad_fn=<MvBackward>), loss = 0.02569385\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 300: w = tensor([1.92642, 1.00494], requires_grad=True), y_pred = tensor([42.12493, 34.00072, 25.87651, 17.75230], grad_fn=<MvBackward>), loss = 0.02305340\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 305: w = tensor([1.93030, 1.00467], requires_grad=True), y_pred = tensor([42.11858, 34.00087, 25.88315, 17.76544], grad_fn=<MvBackward>), loss = 0.02068412\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 310: w = tensor([1.93398, 1.00443], requires_grad=True), y_pred = tensor([42.11213, 34.00067, 25.88922, 17.77776], grad_fn=<MvBackward>), loss = 0.01855873\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 315: w = tensor([1.93747, 1.00419], requires_grad=True), y_pred = tensor([42.10637, 34.00076, 25.89515, 17.78953], grad_fn=<MvBackward>), loss = 0.01665138\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 320: w = tensor([1.94077, 1.00397], requires_grad=True), y_pred = tensor([42.10064, 34.00063, 25.90062, 17.80061], grad_fn=<MvBackward>), loss = 0.01494033\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 325: w = tensor([1.94389, 1.00376], requires_grad=True), y_pred = tensor([42.09541, 34.00066, 25.90591, 17.81116], grad_fn=<MvBackward>), loss = 0.01340491\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 330: w = tensor([1.94685, 1.00356], requires_grad=True), y_pred = tensor([42.09031, 34.00057, 25.91084, 17.82110], grad_fn=<MvBackward>), loss = 0.01202754\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 335: w = tensor([1.94966, 1.00337], requires_grad=True), y_pred = tensor([42.08558, 34.00058, 25.91557, 17.83055], grad_fn=<MvBackward>), loss = 0.01079139\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 340: w = tensor([1.95231, 1.00320], requires_grad=True), y_pred = tensor([42.08104, 34.00053, 25.92001, 17.83949], grad_fn=<MvBackward>), loss = 0.00968240\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 345: w = tensor([1.95483, 1.00303], requires_grad=True), y_pred = tensor([42.07679, 34.00052, 25.92424, 17.84797], grad_fn=<MvBackward>), loss = 0.00868748\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 350: w = tensor([1.95722, 1.00287], requires_grad=True), y_pred = tensor([42.07271, 34.00047, 25.92823, 17.85599], grad_fn=<MvBackward>), loss = 0.00779471\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 355: w = tensor([1.95947, 1.00272], requires_grad=True), y_pred = tensor([42.06890, 34.00047, 25.93203, 17.86359], grad_fn=<MvBackward>), loss = 0.00699359\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 360: w = tensor([1.96161, 1.00257], requires_grad=True), y_pred = tensor([42.06524, 34.00042, 25.93560, 17.87078], grad_fn=<MvBackward>), loss = 0.00627505\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 365: w = tensor([1.96364, 1.00244], requires_grad=True), y_pred = tensor([42.06182, 34.00042, 25.93901, 17.87761], grad_fn=<MvBackward>), loss = 0.00563018\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 370: w = tensor([1.96556, 1.00231], requires_grad=True), y_pred = tensor([42.05854, 34.00038, 25.94222, 17.88406], grad_fn=<MvBackward>), loss = 0.00505164\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 375: w = tensor([1.96737, 1.00219], requires_grad=True), y_pred = tensor([42.05547, 34.00037, 25.94528, 17.89019], grad_fn=<MvBackward>), loss = 0.00453248\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 380: w = tensor([1.96910, 1.00207], requires_grad=True), y_pred = tensor([42.05252, 34.00034, 25.94816, 17.89598], grad_fn=<MvBackward>), loss = 0.00406663\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 385: w = tensor([1.97073, 1.00196], requires_grad=True), y_pred = tensor([42.04977, 34.00034, 25.95090, 17.90147], grad_fn=<MvBackward>), loss = 0.00364873\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 390: w = tensor([1.97227, 1.00186], requires_grad=True), y_pred = tensor([42.04712, 34.00031, 25.95349, 17.90667], grad_fn=<MvBackward>), loss = 0.00327375\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 395: w = tensor([1.97374, 1.00176], requires_grad=True), y_pred = tensor([42.04465, 34.00030, 25.95595, 17.91160], grad_fn=<MvBackward>), loss = 0.00293732\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 400: w = tensor([1.97512, 1.00167], requires_grad=True), y_pred = tensor([42.04229, 34.00028, 25.95827, 17.91626], grad_fn=<MvBackward>), loss = 0.00263548\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 405: w = tensor([1.97643, 1.00158], requires_grad=True), y_pred = tensor([42.04005, 34.00026, 25.96047, 17.92068], grad_fn=<MvBackward>), loss = 0.00236459\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 410: w = tensor([1.97768, 1.00150], requires_grad=True), y_pred = tensor([42.03794, 34.00025, 25.96256, 17.92487], grad_fn=<MvBackward>), loss = 0.00212172\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 415: w = tensor([1.97886, 1.00142], requires_grad=True), y_pred = tensor([42.03594, 34.00024, 25.96453, 17.92883], grad_fn=<MvBackward>), loss = 0.00190360\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 420: w = tensor([1.97997, 1.00134], requires_grad=True), y_pred = tensor([42.03405, 34.00023, 25.96641, 17.93259], grad_fn=<MvBackward>), loss = 0.00170798\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 425: w = tensor([1.98103, 1.00127], requires_grad=True), y_pred = tensor([42.03225, 34.00022, 25.96818, 17.93615], grad_fn=<MvBackward>), loss = 0.00153252\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 430: w = tensor([1.98203, 1.00120], requires_grad=True), y_pred = tensor([42.03054, 34.00020, 25.96986, 17.93951], grad_fn=<MvBackward>), loss = 0.00137501\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 435: w = tensor([1.98298, 1.00114], requires_grad=True), y_pred = tensor([42.02893, 34.00019, 25.97145, 17.94271], grad_fn=<MvBackward>), loss = 0.00123367\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 440: w = tensor([1.98388, 1.00108], requires_grad=True), y_pred = tensor([42.02741, 34.00019, 25.97296, 17.94573], grad_fn=<MvBackward>), loss = 0.00110684\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 445: w = tensor([1.98473, 1.00102], requires_grad=True), y_pred = tensor([42.02594, 34.00016, 25.97437, 17.94859], grad_fn=<MvBackward>), loss = 0.00099315\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 450: w = tensor([1.98553, 1.00097], requires_grad=True), y_pred = tensor([42.02460, 34.00017, 25.97574, 17.95131], grad_fn=<MvBackward>), loss = 0.00089105\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 455: w = tensor([1.98630, 1.00092], requires_grad=True), y_pred = tensor([42.02329, 34.00015, 25.97701, 17.95388], grad_fn=<MvBackward>), loss = 0.00079952\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 460: w = tensor([1.98702, 1.00087], requires_grad=True), y_pred = tensor([42.02205, 34.00014, 25.97823, 17.95631], grad_fn=<MvBackward>), loss = 0.00071731\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 465: w = tensor([1.98771, 1.00082], requires_grad=True), y_pred = tensor([42.02092, 34.00015, 25.97939, 17.95862], grad_fn=<MvBackward>), loss = 0.00064363\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 470: w = tensor([1.98835, 1.00078], requires_grad=True), y_pred = tensor([42.01978, 34.00011, 25.98046, 17.96080], grad_fn=<MvBackward>), loss = 0.00057748\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 475: w = tensor([1.98897, 1.00074], requires_grad=True), y_pred = tensor([42.01876, 34.00014, 25.98150, 17.96288], grad_fn=<MvBackward>), loss = 0.00051811\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 480: w = tensor([1.98955, 1.00070], requires_grad=True), y_pred = tensor([42.01775, 34.00011, 25.98247, 17.96483], grad_fn=<MvBackward>), loss = 0.00046487\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 485: w = tensor([1.99010, 1.00066], requires_grad=True), y_pred = tensor([42.01683, 34.00012, 25.98340, 17.96669], grad_fn=<MvBackward>), loss = 0.00041709\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 490: w = tensor([1.99063, 1.00063], requires_grad=True), y_pred = tensor([42.01592, 34.00009, 25.98427, 17.96844], grad_fn=<MvBackward>), loss = 0.00037425\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 495: w = tensor([1.99112, 1.00060], requires_grad=True), y_pred = tensor([42.01510, 34.00010, 25.98511, 17.97011], grad_fn=<MvBackward>), loss = 0.00033581\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 500: w = tensor([1.99159, 1.00056], requires_grad=True), y_pred = tensor([42.01431, 34.00010, 25.98589, 17.97169], grad_fn=<MvBackward>), loss = 0.00030129\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 505: w = tensor([1.99203, 1.00053], requires_grad=True), y_pred = tensor([42.01354, 34.00008, 25.98663, 17.97318], grad_fn=<MvBackward>), loss = 0.00027033\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 510: w = tensor([1.99245, 1.00051], requires_grad=True), y_pred = tensor([42.01283, 34.00009, 25.98734, 17.97460], grad_fn=<MvBackward>), loss = 0.00024254\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 515: w = tensor([1.99285, 1.00048], requires_grad=True), y_pred = tensor([42.01215, 34.00007, 25.98800, 17.97593], grad_fn=<MvBackward>), loss = 0.00021764\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 520: w = tensor([1.99323, 1.00045], requires_grad=True), y_pred = tensor([42.01151, 34.00008, 25.98864, 17.97721], grad_fn=<MvBackward>), loss = 0.00019526\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 525: w = tensor([1.99359, 1.00043], requires_grad=True), y_pred = tensor([42.01090, 34.00007, 25.98924, 17.97841], grad_fn=<MvBackward>), loss = 0.00017518\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 530: w = tensor([1.99392, 1.00041], requires_grad=True), y_pred = tensor([42.01033, 34.00007, 25.98981, 17.97955], grad_fn=<MvBackward>), loss = 0.00015719\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 535: w = tensor([1.99425, 1.00039], requires_grad=True), y_pred = tensor([42.00978, 34.00006, 25.99035, 17.98063], grad_fn=<MvBackward>), loss = 0.00014103\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 540: w = tensor([1.99455, 1.00037], requires_grad=True), y_pred = tensor([42.00927, 34.00006, 25.99086, 17.98165], grad_fn=<MvBackward>), loss = 0.00012652\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 545: w = tensor([1.99484, 1.00035], requires_grad=True), y_pred = tensor([42.00878, 34.00006, 25.99134, 17.98262], grad_fn=<MvBackward>), loss = 0.00011353\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 550: w = tensor([1.99511, 1.00033], requires_grad=True), y_pred = tensor([42.00831, 34.00005, 25.99179, 17.98354], grad_fn=<MvBackward>), loss = 0.00010187\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 555: w = tensor([1.99537, 1.00031], requires_grad=True), y_pred = tensor([42.00787, 34.00005, 25.99223, 17.98441], grad_fn=<MvBackward>), loss = 0.00009140\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 560: w = tensor([1.99561, 1.00029], requires_grad=True), y_pred = tensor([42.00747, 34.00006, 25.99264, 17.98523], grad_fn=<MvBackward>), loss = 0.00008202\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 565: w = tensor([1.99584, 1.00028], requires_grad=True), y_pred = tensor([42.00705, 34.00004, 25.99302, 17.98600], grad_fn=<MvBackward>), loss = 0.00007359\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 570: w = tensor([1.99606, 1.00026], requires_grad=True), y_pred = tensor([42.00670, 34.00005, 25.99340, 17.98675], grad_fn=<MvBackward>), loss = 0.00006602\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 575: w = tensor([1.99627, 1.00025], requires_grad=True), y_pred = tensor([42.00633, 34.00003, 25.99374, 17.98744], grad_fn=<MvBackward>), loss = 0.00005925\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 580: w = tensor([1.99647, 1.00024], requires_grad=True), y_pred = tensor([42.00601, 34.00005, 25.99408, 17.98811], grad_fn=<MvBackward>), loss = 0.00005316\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 585: w = tensor([1.99665, 1.00022], requires_grad=True), y_pred = tensor([42.00569, 34.00003, 25.99439, 17.98874], grad_fn=<MvBackward>), loss = 0.00004769\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 590: w = tensor([1.99683, 1.00021], requires_grad=True), y_pred = tensor([42.00538, 34.00003, 25.99468, 17.98933], grad_fn=<MvBackward>), loss = 0.00004279\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 595: w = tensor([1.99700, 1.00020], requires_grad=True), y_pred = tensor([42.00511, 34.00004, 25.99497, 17.98989], grad_fn=<MvBackward>), loss = 0.00003839\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 600: w = tensor([1.99716, 1.00019], requires_grad=True), y_pred = tensor([42.00483, 34.00003, 25.99523, 17.99043], grad_fn=<MvBackward>), loss = 0.00003444\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 605: w = tensor([1.99731, 1.00018], requires_grad=True), y_pred = tensor([42.00458, 34.00003, 25.99548, 17.99093], grad_fn=<MvBackward>), loss = 0.00003091\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 610: w = tensor([1.99745, 1.00017], requires_grad=True), y_pred = tensor([42.00435, 34.00003, 25.99572, 17.99141], grad_fn=<MvBackward>), loss = 0.00002773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 615: w = tensor([1.99758, 1.00016], requires_grad=True), y_pred = tensor([42.00410, 34.00002, 25.99594, 17.99186], grad_fn=<MvBackward>), loss = 0.00002489\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 620: w = tensor([1.99771, 1.00015], requires_grad=True), y_pred = tensor([42.00390, 34.00003, 25.99616, 17.99229], grad_fn=<MvBackward>), loss = 0.00002233\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 625: w = tensor([1.99783, 1.00015], requires_grad=True), y_pred = tensor([42.00368, 34.00002, 25.99636, 17.99270], grad_fn=<MvBackward>), loss = 0.00002004\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 630: w = tensor([1.99795, 1.00014], requires_grad=True), y_pred = tensor([42.00349, 34.00002, 25.99655, 17.99308], grad_fn=<MvBackward>), loss = 0.00001798\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 635: w = tensor([1.99805, 1.00013], requires_grad=True), y_pred = tensor([42.00330, 34.00002, 25.99673, 17.99345], grad_fn=<MvBackward>), loss = 0.00001613\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 640: w = tensor([1.99816, 1.00012], requires_grad=True), y_pred = tensor([42.00313, 34.00002, 25.99691, 17.99379], grad_fn=<MvBackward>), loss = 0.00001448\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 645: w = tensor([1.99825, 1.00012], requires_grad=True), y_pred = tensor([42.00298, 34.00003, 25.99708, 17.99413], grad_fn=<MvBackward>), loss = 0.00001298\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 650: w = tensor([1.99835, 1.00011], requires_grad=True), y_pred = tensor([42.00280, 34.00001, 25.99722, 17.99443], grad_fn=<MvBackward>), loss = 0.00001165\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 655: w = tensor([1.99843, 1.00010], requires_grad=True), y_pred = tensor([42.00267, 34.00002, 25.99738, 17.99473], grad_fn=<MvBackward>), loss = 0.00001045\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 660: w = tensor([1.99852, 1.00010], requires_grad=True), y_pred = tensor([42.00251, 34.00001, 25.99751, 17.99500], grad_fn=<MvBackward>), loss = 0.00000938\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 665: w = tensor([1.99859, 1.00009], requires_grad=True), y_pred = tensor([42.00239, 34.00002, 25.99764, 17.99527], grad_fn=<MvBackward>), loss = 0.00000841\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 670: w = tensor([1.99867, 1.00009], requires_grad=True), y_pred = tensor([42.00225, 34.00001, 25.99776, 17.99552], grad_fn=<MvBackward>), loss = 0.00000755\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 675: w = tensor([1.99874, 1.00008], requires_grad=True), y_pred = tensor([42.00216, 34.00003, 25.99789, 17.99576], grad_fn=<MvBackward>), loss = 0.00000678\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 680: w = tensor([1.99881, 1.00008], requires_grad=True), y_pred = tensor([42.00201, 34.00000, 25.99799, 17.99597], grad_fn=<MvBackward>), loss = 0.00000608\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 685: w = tensor([1.99887, 1.00008], requires_grad=True), y_pred = tensor([42.00194, 34.00002, 25.99811, 17.99619], grad_fn=<MvBackward>), loss = 0.00000546\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 690: w = tensor([1.99893, 1.00007], requires_grad=True), y_pred = tensor([42.00181, 34.00000, 25.99820, 17.99639], grad_fn=<MvBackward>), loss = 0.00000489\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 695: w = tensor([1.99898, 1.00007], requires_grad=True), y_pred = tensor([42.00173, 34.00002, 25.99830, 17.99658], grad_fn=<MvBackward>), loss = 0.00000439\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 700: w = tensor([1.99904, 1.00006], requires_grad=True), y_pred = tensor([42.00164, 34.00002, 25.99839, 17.99677], grad_fn=<MvBackward>), loss = 0.00000394\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 705: w = tensor([1.99909, 1.00006], requires_grad=True), y_pred = tensor([42.00154, 34.00001, 25.99847, 17.99693], grad_fn=<MvBackward>), loss = 0.00000353\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 710: w = tensor([1.99914, 1.00006], requires_grad=True), y_pred = tensor([42.00146, 34.00000, 25.99855, 17.99709], grad_fn=<MvBackward>), loss = 0.00000317\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 715: w = tensor([1.99918, 1.00005], requires_grad=True), y_pred = tensor([42.00139, 34.00002, 25.99863, 17.99725], grad_fn=<MvBackward>), loss = 0.00000284\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 720: w = tensor([1.99923, 1.00005], requires_grad=True), y_pred = tensor([42.00131, 34.00000, 25.99870, 17.99739], grad_fn=<MvBackward>), loss = 0.00000255\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 725: w = tensor([1.99927, 1.00005], requires_grad=True), y_pred = tensor([42.00126, 34.00002, 25.99878, 17.99753], grad_fn=<MvBackward>), loss = 0.00000229\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 730: w = tensor([1.99931, 1.00005], requires_grad=True), y_pred = tensor([42.00117, 34.00000, 25.99883, 17.99766], grad_fn=<MvBackward>), loss = 0.00000206\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 735: w = tensor([1.99934, 1.00004], requires_grad=True), y_pred = tensor([42.00113, 34.00002, 25.99890, 17.99779], grad_fn=<MvBackward>), loss = 0.00000184\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 740: w = tensor([1.99938, 1.00004], requires_grad=True), y_pred = tensor([42.00105, 34.00000, 25.99895, 17.99790], grad_fn=<MvBackward>), loss = 0.00000166\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 745: w = tensor([1.99941, 1.00004], requires_grad=True), y_pred = tensor([42.00101, 34.00001, 25.99901, 17.99801], grad_fn=<MvBackward>), loss = 0.00000149\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 750: w = tensor([1.99944, 1.00004], requires_grad=True), y_pred = tensor([42.00096, 34.00001, 25.99907, 17.99812], grad_fn=<MvBackward>), loss = 0.00000133\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 755: w = tensor([1.99947, 1.00004], requires_grad=True), y_pred = tensor([42.00090, 34.00000, 25.99911, 17.99822], grad_fn=<MvBackward>), loss = 0.00000119\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 760: w = tensor([1.99950, 1.00003], requires_grad=True), y_pred = tensor([42.00086, 34.00001, 25.99916, 17.99831], grad_fn=<MvBackward>), loss = 0.00000107\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 765: w = tensor([1.99952, 1.00003], requires_grad=True), y_pred = tensor([42.00081, 34.00000, 25.99920, 17.99840], grad_fn=<MvBackward>), loss = 0.00000096\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 770: w = tensor([1.99955, 1.00003], requires_grad=True), y_pred = tensor([42.00076, 34.00000, 25.99924, 17.99848], grad_fn=<MvBackward>), loss = 0.00000086\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 775: w = tensor([1.99957, 1.00003], requires_grad=True), y_pred = tensor([42.00073, 34.00001, 25.99929, 17.99857], grad_fn=<MvBackward>), loss = 0.00000077\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 780: w = tensor([1.99960, 1.00003], requires_grad=True), y_pred = tensor([42.00068, 34.00000, 25.99932, 17.99864], grad_fn=<MvBackward>), loss = 0.00000069\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 785: w = tensor([1.99962, 1.00003], requires_grad=True), y_pred = tensor([42.00065, 34.00001, 25.99936, 17.99871], grad_fn=<MvBackward>), loss = 0.00000062\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 790: w = tensor([1.99964, 1.00002], requires_grad=True), y_pred = tensor([42.00062, 34.00001, 25.99940, 17.99878], grad_fn=<MvBackward>), loss = 0.00000056\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 795: w = tensor([1.99966, 1.00002], requires_grad=True), y_pred = tensor([42.00059, 34.00001, 25.99943, 17.99885], grad_fn=<MvBackward>), loss = 0.00000050\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 800: w = tensor([1.99967, 1.00002], requires_grad=True), y_pred = tensor([42.00055, 34.00000, 25.99945, 17.99891], grad_fn=<MvBackward>), loss = 0.00000045\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 805: w = tensor([1.99969, 1.00002], requires_grad=True), y_pred = tensor([42.00053, 34.00001, 25.99949, 17.99896], grad_fn=<MvBackward>), loss = 0.00000040\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 810: w = tensor([1.99971, 1.00002], requires_grad=True), y_pred = tensor([42.00049, 34.00000, 25.99951, 17.99902], grad_fn=<MvBackward>), loss = 0.00000036\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 815: w = tensor([1.99972, 1.00002], requires_grad=True), y_pred = tensor([42.00047, 34.00000, 25.99954, 17.99907], grad_fn=<MvBackward>), loss = 0.00000033\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 820: w = tensor([1.99974, 1.00002], requires_grad=True), y_pred = tensor([42.00045, 34.00000, 25.99956, 17.99912], grad_fn=<MvBackward>), loss = 0.00000029\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 825: w = tensor([1.99975, 1.00002], requires_grad=True), y_pred = tensor([42.00042, 34.00000, 25.99958, 17.99916], grad_fn=<MvBackward>), loss = 0.00000026\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 830: w = tensor([1.99977, 1.00002], requires_grad=True), y_pred = tensor([42.00040, 34.00000, 25.99960, 17.99921], grad_fn=<MvBackward>), loss = 0.00000024\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 835: w = tensor([1.99978, 1.00001], requires_grad=True), y_pred = tensor([42.00038, 34.00000, 25.99963, 17.99925], grad_fn=<MvBackward>), loss = 0.00000021\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 840: w = tensor([1.99979, 1.00001], requires_grad=True), y_pred = tensor([42.00035, 34.00000, 25.99965, 17.99929], grad_fn=<MvBackward>), loss = 0.00000019\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 845: w = tensor([1.99980, 1.00001], requires_grad=True), y_pred = tensor([42.00034, 34.00000, 25.99967, 17.99933], grad_fn=<MvBackward>), loss = 0.00000017\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 850: w = tensor([1.99981, 1.00001], requires_grad=True), y_pred = tensor([42.00032, 34.00000, 25.99968, 17.99936], grad_fn=<MvBackward>), loss = 0.00000015\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 855: w = tensor([1.99982, 1.00001], requires_grad=True), y_pred = tensor([42.00030, 34.00000, 25.99970, 17.99940], grad_fn=<MvBackward>), loss = 0.00000014\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 860: w = tensor([1.99983, 1.00001], requires_grad=True), y_pred = tensor([42.00028, 33.99999, 25.99971, 17.99943], grad_fn=<MvBackward>), loss = 0.00000012\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 865: w = tensor([1.99984, 1.00001], requires_grad=True), y_pred = tensor([42.00027, 34.00000, 25.99973, 17.99946], grad_fn=<MvBackward>), loss = 0.00000011\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 870: w = tensor([1.99985, 1.00001], requires_grad=True), y_pred = tensor([42.00026, 34.00000, 25.99975, 17.99949], grad_fn=<MvBackward>), loss = 0.00000010\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 875: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00023, 33.99999, 25.99975, 17.99951], grad_fn=<MvBackward>), loss = 0.00000009\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 880: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00001, 25.99978, 17.99954], grad_fn=<MvBackward>), loss = 0.00000008\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 885: w = tensor([1.99987, 1.00001], requires_grad=True), y_pred = tensor([42.00021, 33.99999, 25.99978, 17.99956], grad_fn=<MvBackward>), loss = 0.00000007\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 890: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00002, 25.99981, 17.99960], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 895: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 33.99998, 25.99980, 17.99961], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 900: w = tensor([1.99989, 1.00001], requires_grad=True), y_pred = tensor([42.00020, 34.00001, 25.99982, 17.99963], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 905: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 33.99999, 25.99982, 17.99965], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 910: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 34.00001, 25.99984, 17.99967], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 915: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 34.00000, 25.99985, 17.99969], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 920: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00015, 34.00000, 25.99985, 17.99970], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 925: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00014, 34.00000, 25.99986, 17.99972], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 930: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00013, 33.99999, 25.99986, 17.99973], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 935: w = tensor([1.99992, 1.00000], requires_grad=True), y_pred = tensor([42.00013, 34.00000, 25.99988, 17.99975], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 940: w = tensor([1.99993, 1.00001], requires_grad=True), y_pred = tensor([42.00011, 33.99999, 25.99988, 17.99976], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 945: w = tensor([1.99993, 1.00000], requires_grad=True), y_pred = tensor([42.00012, 34.00000, 25.99989, 17.99977], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 950: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99989, 17.99978], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 955: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00011, 34.00000, 25.99990, 17.99980], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 960: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99981], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 965: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 970: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 33.99999, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 975: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00001, 25.99993, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 980: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99992, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 985: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 34.00001, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 990: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 995: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00006, 34.00000, 25.99993, 17.99987], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n"
     ]
    }
   ],
   "source": [
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "#!!! requires_grad=True\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.0013\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "# основной цикл:\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    #!!! backward pass        \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Использование встроенного оптимизатора `optimizer = torch.optim.SGD([w], lr=learning_rate)` и функции потерь `loss = nn.MSELoss()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "epoch 0: w = tensor([0.16900, 2.21000], requires_grad=True), y_pred = tensor([0., 0., 0., 0.], grad_fn=<MvBackward>), loss = 980.00000000\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 5: w = tensor([0.13813, 0.24422], requires_grad=True), y_pred = tensor([81.70274, 61.57523, 41.44772, 21.32021], grad_fn=<MvBackward>), loss = 646.58898926\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 10: w = tensor([0.33966, 1.82453], requires_grad=True), y_pred = tensor([15.22920, 11.70164,  8.17407,  4.64651], grad_fn=<MvBackward>), loss = 427.49307251\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 15: w = tensor([0.34364, 0.53338], requires_grad=True), y_pred = tensor([68.78386, 52.09385, 35.40385, 18.71384], grad_fn=<MvBackward>), loss = 283.42614746\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 20: w = tensor([0.49880, 1.56850], requires_grad=True), y_pred = tensor([25.13912, 19.37721, 13.61531,  7.85340], grad_fn=<MvBackward>), loss = 188.61228943\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 25: w = tensor([0.52316, 0.72007], requires_grad=True), y_pred = tensor([60.23326, 45.87370, 31.51413, 17.15457], grad_fn=<MvBackward>), loss = 126.13926697\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 30: w = tensor([0.64554, 1.39771], requires_grad=True), y_pred = tensor([31.56792, 24.41182, 17.25571, 10.09960], grad_fn=<MvBackward>), loss = 84.91012573\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 35: w = tensor([0.68103, 0.83984], requires_grad=True), y_pred = tensor([54.55604, 41.79293, 29.02982, 16.26671], grad_fn=<MvBackward>), loss = 57.64197159\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 40: w = tensor([0.77979, 1.28313], requires_grad=True), y_pred = tensor([35.72054, 27.71401, 19.70748, 11.70095], grad_fn=<MvBackward>), loss = 39.55477905\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 45: w = tensor([0.82057, 0.91600], requires_grad=True), y_pred = tensor([50.77066, 39.11558, 27.46050, 15.80541], grad_fn=<MvBackward>), loss = 27.51074600\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 50: w = tensor([0.90194, 1.20568], requires_grad=True), y_pred = tensor([38.38666, 29.87982, 21.37299, 12.86615], grad_fn=<MvBackward>), loss = 19.44943237\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 55: w = tensor([0.94440, 0.96380], requires_grad=True), y_pred = tensor([48.23264, 37.35892, 26.48521, 15.61149], grad_fn=<MvBackward>), loss = 14.01713943\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 60: w = tensor([1.01265, 1.15282], requires_grad=True), y_pred = tensor([40.08362, 31.30023, 22.51685, 13.73346], grad_fn=<MvBackward>), loss = 10.32425213\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 65: w = tensor([1.05460, 0.99321], requires_grad=True), y_pred = tensor([46.51847, 36.20624, 25.89402, 15.58179], grad_fn=<MvBackward>), loss = 7.78575325\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 70: w = tensor([1.11272, 1.11631], requires_grad=True), y_pred = tensor([41.15015, 32.23169, 23.31322, 14.39476], grad_fn=<MvBackward>), loss = 6.01642036\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 75: w = tensor([1.15289, 1.01076], requires_grad=True), y_pred = tensor([45.34982, 35.44982, 25.54981, 15.64980], grad_fn=<MvBackward>), loss = 4.76234579\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 80: w = tensor([1.20299, 1.09071], requires_grad=True), y_pred = tensor([41.80791, 32.84242, 23.87693, 14.91144], grad_fn=<MvBackward>), loss = 3.85587597\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 85: w = tensor([1.24068, 1.02071], requires_grad=True), y_pred = tensor([44.54358, 34.95336, 25.36314, 15.77292], grad_fn=<MvBackward>), loss = 3.18603969\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 90: w = tensor([1.28429, 1.07244], requires_grad=True), y_pred = tensor([42.20184, 33.24281, 24.28377, 15.32473], grad_fn=<MvBackward>), loss = 2.67915201\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 95: w = tensor([1.31920, 1.02583], requires_grad=True), y_pred = tensor([43.97913, 34.62745, 25.27579, 15.92412], grad_fn=<MvBackward>), loss = 2.28610182\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 100: w = tensor([1.35745, 1.05912], requires_grad=True), y_pred = tensor([42.42656, 33.50523, 24.58389, 15.66256], grad_fn=<MvBackward>), loss = 1.97393394\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 105: w = tensor([1.38948, 1.02793], requires_grad=True), y_pred = tensor([43.57692, 34.41344, 25.24997, 16.08649], grad_fn=<MvBackward>), loss = 1.72041774\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 110: w = tensor([1.42322, 1.04919], requires_grad=True), y_pred = tensor([42.54377, 33.67719, 24.81061, 15.94403], grad_fn=<MvBackward>), loss = 1.51038170\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 115: w = tensor([1.45243, 1.02818], requires_grad=True), y_pred = tensor([43.28445, 34.27287, 25.26129, 16.24971], grad_fn=<MvBackward>), loss = 1.33336782\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 120: w = tensor([1.48233, 1.04161], requires_grad=True), y_pred = tensor([42.59350, 33.78979, 24.98609, 16.18238], grad_fn=<MvBackward>), loss = 1.18204451\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 125: w = tensor([1.50884, 1.02733], requires_grad=True), y_pred = tensor([43.06691, 34.18049, 25.29407, 16.40766], grad_fn=<MvBackward>), loss = 1.05119181\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 130: w = tensor([1.53541, 1.03567], requires_grad=True), y_pred = tensor([42.60181, 33.86351, 25.12522, 16.38693], grad_fn=<MvBackward>), loss = 0.93700927\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 135: w = tensor([1.55940, 1.02587], requires_grad=True), y_pred = tensor([42.90114, 34.11973, 25.33832, 16.55691], grad_fn=<MvBackward>), loss = 0.83668095\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 140: w = tensor([1.58308, 1.03092], requires_grad=True), y_pred = tensor([42.58541, 33.91172, 25.23803, 16.56434], grad_fn=<MvBackward>), loss = 0.74805164\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 145: w = tensor([1.60474, 1.02409], requires_grad=True), y_pred = tensor([42.77173, 34.07975, 25.38776, 16.69578], grad_fn=<MvBackward>), loss = 0.66944206\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 150: w = tensor([1.62588, 1.02703], requires_grad=True), y_pred = tensor([42.55506, 33.94321, 25.33137, 16.71952], grad_fn=<MvBackward>), loss = 0.59950674\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 155: w = tensor([1.64540, 1.02219], requires_grad=True), y_pred = tensor([42.66828, 34.05339, 25.43850, 16.82362], grad_fn=<MvBackward>), loss = 0.53715217\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 160: w = tensor([1.66429, 1.02378], requires_grad=True), y_pred = tensor([42.51758, 33.96376, 25.40994, 16.85612], grad_fn=<MvBackward>), loss = 0.48146015\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 165: w = tensor([1.68186, 1.02029], requires_grad=True), y_pred = tensor([42.58377, 34.03600, 25.48822, 16.94045], grad_fn=<MvBackward>), loss = 0.43166173\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 170: w = tensor([1.69877, 1.02103], requires_grad=True), y_pred = tensor([42.47721, 33.97713, 25.47704, 16.97695], grad_fn=<MvBackward>), loss = 0.38709140\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 175: w = tensor([1.71457, 1.01845], requires_grad=True), y_pred = tensor([42.51338, 34.02448, 25.53557, 17.04667], grad_fn=<MvBackward>), loss = 0.34717295\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 180: w = tensor([1.72971, 1.01867], requires_grad=True), y_pred = tensor([42.43659, 33.98581, 25.53503, 17.08424], grad_fn=<MvBackward>), loss = 0.31140596\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 185: w = tensor([1.74391, 1.01672], requires_grad=True), y_pred = tensor([42.45380, 34.01683, 25.57986, 17.14289], grad_fn=<MvBackward>), loss = 0.27934441\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 190: w = tensor([1.75748, 1.01662], requires_grad=True), y_pred = tensor([42.39725, 33.99142, 25.58560, 17.17977], grad_fn=<MvBackward>), loss = 0.25059801\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 195: w = tensor([1.77024, 1.01511], requires_grad=True), y_pred = tensor([42.40269, 34.01173, 25.62078, 17.22982], grad_fn=<MvBackward>), loss = 0.22481927\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 200: w = tensor([1.78240, 1.01482], requires_grad=True), y_pred = tensor([42.36006, 33.99504, 25.63001, 17.26499], grad_fn=<MvBackward>), loss = 0.20169993\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 205: w = tensor([1.79385, 1.01363], requires_grad=True), y_pred = tensor([42.35838, 34.00832, 25.65827, 17.30822], grad_fn=<MvBackward>), loss = 0.18096074\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 210: w = tensor([1.80476, 1.01324], requires_grad=True), y_pred = tensor([42.32542, 33.99732, 25.66922, 17.34113], grad_fn=<MvBackward>), loss = 0.16235729\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 215: w = tensor([1.81504, 1.01227], requires_grad=True), y_pred = tensor([42.31963, 34.00603, 25.69242, 17.37882], grad_fn=<MvBackward>), loss = 0.14566770\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 220: w = tensor([1.82482, 1.01185], requires_grad=True), y_pred = tensor([42.29353, 33.99876, 25.70400, 17.40924], grad_fn=<MvBackward>), loss = 0.13069558\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 225: w = tensor([1.83405, 1.01104], requires_grad=True), y_pred = tensor([42.28552, 34.00446, 25.72339, 17.44233], grad_fn=<MvBackward>), loss = 0.11726224\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 230: w = tensor([1.84282, 1.01060], requires_grad=True), y_pred = tensor([42.26440, 33.99967, 25.73494, 17.47021], grad_fn=<MvBackward>), loss = 0.10521053\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 235: w = tensor([1.85111, 1.00993], requires_grad=True), y_pred = tensor([42.25534, 34.00337, 25.75140, 17.49942], grad_fn=<MvBackward>), loss = 0.09439759\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 240: w = tensor([1.85897, 1.00950], requires_grad=True), y_pred = tensor([42.23791, 34.00021, 25.76252, 17.52483], grad_fn=<MvBackward>), loss = 0.08469677\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 245: w = tensor([1.86641, 1.00892], requires_grad=True), y_pred = tensor([42.22856, 34.00261, 25.77667, 17.55073], grad_fn=<MvBackward>), loss = 0.07599217\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 250: w = tensor([1.87346, 1.00851], requires_grad=True), y_pred = tensor([42.21390, 34.00052, 25.78715, 17.57377], grad_fn=<MvBackward>), loss = 0.06818256\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 255: w = tensor([1.88014, 1.00801], requires_grad=True), y_pred = tensor([42.20472, 34.00208, 25.79945, 17.59681], grad_fn=<MvBackward>), loss = 0.06117591\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 260: w = tensor([1.88647, 1.00763], requires_grad=True), y_pred = tensor([42.19220, 34.00068, 25.80916, 17.61764], grad_fn=<MvBackward>), loss = 0.05488885\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 265: w = tensor([1.89246, 1.00719], requires_grad=True), y_pred = tensor([42.18345, 34.00169, 25.81994, 17.63818], grad_fn=<MvBackward>), loss = 0.04924808\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 270: w = tensor([1.89813, 1.00684], requires_grad=True), y_pred = tensor([42.17264, 34.00076, 25.82887, 17.65699], grad_fn=<MvBackward>), loss = 0.04418735\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 275: w = tensor([1.90351, 1.00646], requires_grad=True), y_pred = tensor([42.16444, 34.00140, 25.83836, 17.67532], grad_fn=<MvBackward>), loss = 0.03964623\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 280: w = tensor([1.90860, 1.00614], requires_grad=True), y_pred = tensor([42.15502, 34.00077, 25.84652, 17.69227], grad_fn=<MvBackward>), loss = 0.03557184\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 285: w = tensor([1.91342, 1.00580], requires_grad=True), y_pred = tensor([42.14744, 34.00118, 25.85492, 17.70866], grad_fn=<MvBackward>), loss = 0.03191639\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 290: w = tensor([1.91799, 1.00550], requires_grad=True), y_pred = tensor([42.13918, 34.00076, 25.86233, 17.72391], grad_fn=<MvBackward>), loss = 0.02863660\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 295: w = tensor([1.92232, 1.00520], requires_grad=True), y_pred = tensor([42.13222, 34.00101, 25.86980, 17.73858], grad_fn=<MvBackward>), loss = 0.02569385\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 300: w = tensor([1.92642, 1.00494], requires_grad=True), y_pred = tensor([42.12493, 34.00072, 25.87651, 17.75230], grad_fn=<MvBackward>), loss = 0.02305340\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 305: w = tensor([1.93030, 1.00467], requires_grad=True), y_pred = tensor([42.11858, 34.00087, 25.88315, 17.76544], grad_fn=<MvBackward>), loss = 0.02068412\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 310: w = tensor([1.93398, 1.00443], requires_grad=True), y_pred = tensor([42.11213, 34.00067, 25.88922, 17.77776], grad_fn=<MvBackward>), loss = 0.01855873\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 315: w = tensor([1.93747, 1.00419], requires_grad=True), y_pred = tensor([42.10637, 34.00076, 25.89515, 17.78953], grad_fn=<MvBackward>), loss = 0.01665138\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 320: w = tensor([1.94077, 1.00397], requires_grad=True), y_pred = tensor([42.10064, 34.00063, 25.90062, 17.80061], grad_fn=<MvBackward>), loss = 0.01494033\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 325: w = tensor([1.94389, 1.00376], requires_grad=True), y_pred = tensor([42.09541, 34.00066, 25.90591, 17.81116], grad_fn=<MvBackward>), loss = 0.01340491\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 330: w = tensor([1.94685, 1.00356], requires_grad=True), y_pred = tensor([42.09031, 34.00057, 25.91084, 17.82110], grad_fn=<MvBackward>), loss = 0.01202754\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 335: w = tensor([1.94966, 1.00337], requires_grad=True), y_pred = tensor([42.08558, 34.00058, 25.91557, 17.83055], grad_fn=<MvBackward>), loss = 0.01079139\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 340: w = tensor([1.95231, 1.00320], requires_grad=True), y_pred = tensor([42.08104, 34.00053, 25.92001, 17.83949], grad_fn=<MvBackward>), loss = 0.00968240\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 345: w = tensor([1.95483, 1.00303], requires_grad=True), y_pred = tensor([42.07679, 34.00052, 25.92424, 17.84797], grad_fn=<MvBackward>), loss = 0.00868748\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 350: w = tensor([1.95722, 1.00287], requires_grad=True), y_pred = tensor([42.07271, 34.00047, 25.92823, 17.85599], grad_fn=<MvBackward>), loss = 0.00779471\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 355: w = tensor([1.95947, 1.00272], requires_grad=True), y_pred = tensor([42.06890, 34.00047, 25.93203, 17.86359], grad_fn=<MvBackward>), loss = 0.00699359\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 360: w = tensor([1.96161, 1.00257], requires_grad=True), y_pred = tensor([42.06524, 34.00042, 25.93560, 17.87078], grad_fn=<MvBackward>), loss = 0.00627505\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 365: w = tensor([1.96364, 1.00244], requires_grad=True), y_pred = tensor([42.06182, 34.00042, 25.93901, 17.87761], grad_fn=<MvBackward>), loss = 0.00563018\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 370: w = tensor([1.96556, 1.00231], requires_grad=True), y_pred = tensor([42.05854, 34.00038, 25.94222, 17.88406], grad_fn=<MvBackward>), loss = 0.00505164\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 375: w = tensor([1.96737, 1.00219], requires_grad=True), y_pred = tensor([42.05547, 34.00037, 25.94528, 17.89019], grad_fn=<MvBackward>), loss = 0.00453248\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 380: w = tensor([1.96910, 1.00207], requires_grad=True), y_pred = tensor([42.05252, 34.00034, 25.94816, 17.89598], grad_fn=<MvBackward>), loss = 0.00406663\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 385: w = tensor([1.97073, 1.00196], requires_grad=True), y_pred = tensor([42.04977, 34.00034, 25.95090, 17.90147], grad_fn=<MvBackward>), loss = 0.00364873\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 390: w = tensor([1.97227, 1.00186], requires_grad=True), y_pred = tensor([42.04712, 34.00031, 25.95349, 17.90667], grad_fn=<MvBackward>), loss = 0.00327375\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 395: w = tensor([1.97374, 1.00176], requires_grad=True), y_pred = tensor([42.04465, 34.00030, 25.95595, 17.91160], grad_fn=<MvBackward>), loss = 0.00293732\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 400: w = tensor([1.97512, 1.00167], requires_grad=True), y_pred = tensor([42.04229, 34.00028, 25.95827, 17.91626], grad_fn=<MvBackward>), loss = 0.00263548\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 405: w = tensor([1.97643, 1.00158], requires_grad=True), y_pred = tensor([42.04005, 34.00026, 25.96047, 17.92068], grad_fn=<MvBackward>), loss = 0.00236459\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 410: w = tensor([1.97768, 1.00150], requires_grad=True), y_pred = tensor([42.03794, 34.00025, 25.96256, 17.92487], grad_fn=<MvBackward>), loss = 0.00212172\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 415: w = tensor([1.97886, 1.00142], requires_grad=True), y_pred = tensor([42.03594, 34.00024, 25.96453, 17.92883], grad_fn=<MvBackward>), loss = 0.00190360\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 420: w = tensor([1.97997, 1.00134], requires_grad=True), y_pred = tensor([42.03405, 34.00023, 25.96641, 17.93259], grad_fn=<MvBackward>), loss = 0.00170798\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 425: w = tensor([1.98103, 1.00127], requires_grad=True), y_pred = tensor([42.03225, 34.00022, 25.96818, 17.93615], grad_fn=<MvBackward>), loss = 0.00153252\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 430: w = tensor([1.98203, 1.00120], requires_grad=True), y_pred = tensor([42.03054, 34.00020, 25.96986, 17.93951], grad_fn=<MvBackward>), loss = 0.00137501\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 435: w = tensor([1.98298, 1.00114], requires_grad=True), y_pred = tensor([42.02893, 34.00019, 25.97145, 17.94271], grad_fn=<MvBackward>), loss = 0.00123367\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 440: w = tensor([1.98388, 1.00108], requires_grad=True), y_pred = tensor([42.02741, 34.00019, 25.97296, 17.94573], grad_fn=<MvBackward>), loss = 0.00110684\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 445: w = tensor([1.98473, 1.00102], requires_grad=True), y_pred = tensor([42.02594, 34.00016, 25.97437, 17.94859], grad_fn=<MvBackward>), loss = 0.00099315\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 450: w = tensor([1.98553, 1.00097], requires_grad=True), y_pred = tensor([42.02460, 34.00017, 25.97574, 17.95131], grad_fn=<MvBackward>), loss = 0.00089105\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 455: w = tensor([1.98630, 1.00092], requires_grad=True), y_pred = tensor([42.02329, 34.00015, 25.97701, 17.95388], grad_fn=<MvBackward>), loss = 0.00079952\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 460: w = tensor([1.98702, 1.00087], requires_grad=True), y_pred = tensor([42.02205, 34.00014, 25.97823, 17.95631], grad_fn=<MvBackward>), loss = 0.00071731\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 465: w = tensor([1.98771, 1.00082], requires_grad=True), y_pred = tensor([42.02092, 34.00015, 25.97939, 17.95862], grad_fn=<MvBackward>), loss = 0.00064363\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 470: w = tensor([1.98835, 1.00078], requires_grad=True), y_pred = tensor([42.01978, 34.00011, 25.98046, 17.96080], grad_fn=<MvBackward>), loss = 0.00057748\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 475: w = tensor([1.98897, 1.00074], requires_grad=True), y_pred = tensor([42.01876, 34.00014, 25.98150, 17.96288], grad_fn=<MvBackward>), loss = 0.00051811\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 480: w = tensor([1.98955, 1.00070], requires_grad=True), y_pred = tensor([42.01775, 34.00011, 25.98247, 17.96483], grad_fn=<MvBackward>), loss = 0.00046487\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 485: w = tensor([1.99010, 1.00066], requires_grad=True), y_pred = tensor([42.01683, 34.00012, 25.98340, 17.96669], grad_fn=<MvBackward>), loss = 0.00041709\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 490: w = tensor([1.99063, 1.00063], requires_grad=True), y_pred = tensor([42.01592, 34.00009, 25.98427, 17.96844], grad_fn=<MvBackward>), loss = 0.00037425\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 495: w = tensor([1.99112, 1.00060], requires_grad=True), y_pred = tensor([42.01510, 34.00010, 25.98511, 17.97011], grad_fn=<MvBackward>), loss = 0.00033581\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 500: w = tensor([1.99159, 1.00056], requires_grad=True), y_pred = tensor([42.01431, 34.00010, 25.98589, 17.97169], grad_fn=<MvBackward>), loss = 0.00030129\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 505: w = tensor([1.99203, 1.00053], requires_grad=True), y_pred = tensor([42.01354, 34.00008, 25.98663, 17.97318], grad_fn=<MvBackward>), loss = 0.00027033\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 510: w = tensor([1.99245, 1.00051], requires_grad=True), y_pred = tensor([42.01283, 34.00009, 25.98734, 17.97460], grad_fn=<MvBackward>), loss = 0.00024254\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 515: w = tensor([1.99285, 1.00048], requires_grad=True), y_pred = tensor([42.01215, 34.00007, 25.98800, 17.97593], grad_fn=<MvBackward>), loss = 0.00021764\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 520: w = tensor([1.99323, 1.00045], requires_grad=True), y_pred = tensor([42.01151, 34.00008, 25.98864, 17.97721], grad_fn=<MvBackward>), loss = 0.00019526\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 525: w = tensor([1.99359, 1.00043], requires_grad=True), y_pred = tensor([42.01090, 34.00007, 25.98924, 17.97841], grad_fn=<MvBackward>), loss = 0.00017518\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 530: w = tensor([1.99392, 1.00041], requires_grad=True), y_pred = tensor([42.01033, 34.00007, 25.98981, 17.97955], grad_fn=<MvBackward>), loss = 0.00015719\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 535: w = tensor([1.99425, 1.00039], requires_grad=True), y_pred = tensor([42.00978, 34.00006, 25.99035, 17.98063], grad_fn=<MvBackward>), loss = 0.00014103\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 540: w = tensor([1.99455, 1.00037], requires_grad=True), y_pred = tensor([42.00927, 34.00006, 25.99086, 17.98165], grad_fn=<MvBackward>), loss = 0.00012652\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 545: w = tensor([1.99484, 1.00035], requires_grad=True), y_pred = tensor([42.00878, 34.00006, 25.99134, 17.98262], grad_fn=<MvBackward>), loss = 0.00011353\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 550: w = tensor([1.99511, 1.00033], requires_grad=True), y_pred = tensor([42.00831, 34.00005, 25.99179, 17.98354], grad_fn=<MvBackward>), loss = 0.00010187\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 555: w = tensor([1.99537, 1.00031], requires_grad=True), y_pred = tensor([42.00787, 34.00005, 25.99223, 17.98441], grad_fn=<MvBackward>), loss = 0.00009140\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 560: w = tensor([1.99561, 1.00029], requires_grad=True), y_pred = tensor([42.00747, 34.00006, 25.99264, 17.98523], grad_fn=<MvBackward>), loss = 0.00008202\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 565: w = tensor([1.99584, 1.00028], requires_grad=True), y_pred = tensor([42.00705, 34.00004, 25.99302, 17.98600], grad_fn=<MvBackward>), loss = 0.00007359\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 570: w = tensor([1.99606, 1.00026], requires_grad=True), y_pred = tensor([42.00670, 34.00005, 25.99340, 17.98675], grad_fn=<MvBackward>), loss = 0.00006602\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 575: w = tensor([1.99627, 1.00025], requires_grad=True), y_pred = tensor([42.00633, 34.00003, 25.99374, 17.98744], grad_fn=<MvBackward>), loss = 0.00005925\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 580: w = tensor([1.99647, 1.00024], requires_grad=True), y_pred = tensor([42.00601, 34.00005, 25.99408, 17.98811], grad_fn=<MvBackward>), loss = 0.00005316\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 585: w = tensor([1.99665, 1.00022], requires_grad=True), y_pred = tensor([42.00569, 34.00003, 25.99439, 17.98874], grad_fn=<MvBackward>), loss = 0.00004769\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 590: w = tensor([1.99683, 1.00021], requires_grad=True), y_pred = tensor([42.00538, 34.00003, 25.99468, 17.98933], grad_fn=<MvBackward>), loss = 0.00004279\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 595: w = tensor([1.99700, 1.00020], requires_grad=True), y_pred = tensor([42.00511, 34.00004, 25.99497, 17.98989], grad_fn=<MvBackward>), loss = 0.00003839\n",
      "gradient = tensor([ -130., -1700.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 600: w = tensor([1.99716, 1.00019], requires_grad=True), y_pred = tensor([42.00483, 34.00003, 25.99523, 17.99043], grad_fn=<MvBackward>), loss = 0.00003444\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 605: w = tensor([1.99731, 1.00018], requires_grad=True), y_pred = tensor([42.00458, 34.00003, 25.99548, 17.99093], grad_fn=<MvBackward>), loss = 0.00003091\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 610: w = tensor([1.99745, 1.00017], requires_grad=True), y_pred = tensor([42.00435, 34.00003, 25.99572, 17.99141], grad_fn=<MvBackward>), loss = 0.00002773\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 615: w = tensor([1.99758, 1.00016], requires_grad=True), y_pred = tensor([42.00410, 34.00002, 25.99594, 17.99186], grad_fn=<MvBackward>), loss = 0.00002489\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 620: w = tensor([1.99771, 1.00015], requires_grad=True), y_pred = tensor([42.00390, 34.00003, 25.99616, 17.99229], grad_fn=<MvBackward>), loss = 0.00002233\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 625: w = tensor([1.99783, 1.00015], requires_grad=True), y_pred = tensor([42.00368, 34.00002, 25.99636, 17.99270], grad_fn=<MvBackward>), loss = 0.00002004\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 630: w = tensor([1.99795, 1.00014], requires_grad=True), y_pred = tensor([42.00349, 34.00002, 25.99655, 17.99308], grad_fn=<MvBackward>), loss = 0.00001798\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 635: w = tensor([1.99805, 1.00013], requires_grad=True), y_pred = tensor([42.00330, 34.00002, 25.99673, 17.99345], grad_fn=<MvBackward>), loss = 0.00001613\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 640: w = tensor([1.99816, 1.00012], requires_grad=True), y_pred = tensor([42.00313, 34.00002, 25.99691, 17.99379], grad_fn=<MvBackward>), loss = 0.00001448\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 645: w = tensor([1.99825, 1.00012], requires_grad=True), y_pred = tensor([42.00298, 34.00003, 25.99708, 17.99413], grad_fn=<MvBackward>), loss = 0.00001298\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 650: w = tensor([1.99835, 1.00011], requires_grad=True), y_pred = tensor([42.00280, 34.00001, 25.99722, 17.99443], grad_fn=<MvBackward>), loss = 0.00001165\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 655: w = tensor([1.99843, 1.00010], requires_grad=True), y_pred = tensor([42.00267, 34.00002, 25.99738, 17.99473], grad_fn=<MvBackward>), loss = 0.00001045\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 660: w = tensor([1.99852, 1.00010], requires_grad=True), y_pred = tensor([42.00251, 34.00001, 25.99751, 17.99500], grad_fn=<MvBackward>), loss = 0.00000938\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 665: w = tensor([1.99859, 1.00009], requires_grad=True), y_pred = tensor([42.00239, 34.00002, 25.99764, 17.99527], grad_fn=<MvBackward>), loss = 0.00000841\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 670: w = tensor([1.99867, 1.00009], requires_grad=True), y_pred = tensor([42.00225, 34.00001, 25.99776, 17.99552], grad_fn=<MvBackward>), loss = 0.00000755\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 675: w = tensor([1.99874, 1.00008], requires_grad=True), y_pred = tensor([42.00216, 34.00003, 25.99789, 17.99576], grad_fn=<MvBackward>), loss = 0.00000678\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 680: w = tensor([1.99881, 1.00008], requires_grad=True), y_pred = tensor([42.00201, 34.00000, 25.99799, 17.99597], grad_fn=<MvBackward>), loss = 0.00000608\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 685: w = tensor([1.99887, 1.00008], requires_grad=True), y_pred = tensor([42.00194, 34.00002, 25.99811, 17.99619], grad_fn=<MvBackward>), loss = 0.00000546\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 690: w = tensor([1.99893, 1.00007], requires_grad=True), y_pred = tensor([42.00181, 34.00000, 25.99820, 17.99639], grad_fn=<MvBackward>), loss = 0.00000489\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 695: w = tensor([1.99898, 1.00007], requires_grad=True), y_pred = tensor([42.00173, 34.00002, 25.99830, 17.99658], grad_fn=<MvBackward>), loss = 0.00000439\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 700: w = tensor([1.99904, 1.00006], requires_grad=True), y_pred = tensor([42.00164, 34.00002, 25.99839, 17.99677], grad_fn=<MvBackward>), loss = 0.00000394\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 705: w = tensor([1.99909, 1.00006], requires_grad=True), y_pred = tensor([42.00154, 34.00001, 25.99847, 17.99693], grad_fn=<MvBackward>), loss = 0.00000353\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 710: w = tensor([1.99914, 1.00006], requires_grad=True), y_pred = tensor([42.00146, 34.00000, 25.99855, 17.99709], grad_fn=<MvBackward>), loss = 0.00000317\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 715: w = tensor([1.99918, 1.00005], requires_grad=True), y_pred = tensor([42.00139, 34.00002, 25.99863, 17.99725], grad_fn=<MvBackward>), loss = 0.00000284\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 720: w = tensor([1.99923, 1.00005], requires_grad=True), y_pred = tensor([42.00131, 34.00000, 25.99870, 17.99739], grad_fn=<MvBackward>), loss = 0.00000255\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 725: w = tensor([1.99927, 1.00005], requires_grad=True), y_pred = tensor([42.00126, 34.00002, 25.99878, 17.99753], grad_fn=<MvBackward>), loss = 0.00000229\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 730: w = tensor([1.99931, 1.00005], requires_grad=True), y_pred = tensor([42.00117, 34.00000, 25.99883, 17.99766], grad_fn=<MvBackward>), loss = 0.00000206\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 735: w = tensor([1.99934, 1.00004], requires_grad=True), y_pred = tensor([42.00113, 34.00002, 25.99890, 17.99779], grad_fn=<MvBackward>), loss = 0.00000184\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 740: w = tensor([1.99938, 1.00004], requires_grad=True), y_pred = tensor([42.00105, 34.00000, 25.99895, 17.99790], grad_fn=<MvBackward>), loss = 0.00000166\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 745: w = tensor([1.99941, 1.00004], requires_grad=True), y_pred = tensor([42.00101, 34.00001, 25.99901, 17.99801], grad_fn=<MvBackward>), loss = 0.00000149\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 750: w = tensor([1.99944, 1.00004], requires_grad=True), y_pred = tensor([42.00096, 34.00001, 25.99907, 17.99812], grad_fn=<MvBackward>), loss = 0.00000133\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 755: w = tensor([1.99947, 1.00004], requires_grad=True), y_pred = tensor([42.00090, 34.00000, 25.99911, 17.99822], grad_fn=<MvBackward>), loss = 0.00000119\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 760: w = tensor([1.99950, 1.00003], requires_grad=True), y_pred = tensor([42.00086, 34.00001, 25.99916, 17.99831], grad_fn=<MvBackward>), loss = 0.00000107\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 765: w = tensor([1.99952, 1.00003], requires_grad=True), y_pred = tensor([42.00081, 34.00000, 25.99920, 17.99840], grad_fn=<MvBackward>), loss = 0.00000096\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 770: w = tensor([1.99955, 1.00003], requires_grad=True), y_pred = tensor([42.00076, 34.00000, 25.99924, 17.99848], grad_fn=<MvBackward>), loss = 0.00000086\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 775: w = tensor([1.99957, 1.00003], requires_grad=True), y_pred = tensor([42.00073, 34.00001, 25.99929, 17.99857], grad_fn=<MvBackward>), loss = 0.00000077\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 780: w = tensor([1.99960, 1.00003], requires_grad=True), y_pred = tensor([42.00068, 34.00000, 25.99932, 17.99864], grad_fn=<MvBackward>), loss = 0.00000069\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 785: w = tensor([1.99962, 1.00003], requires_grad=True), y_pred = tensor([42.00065, 34.00001, 25.99936, 17.99871], grad_fn=<MvBackward>), loss = 0.00000062\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 790: w = tensor([1.99964, 1.00002], requires_grad=True), y_pred = tensor([42.00062, 34.00001, 25.99940, 17.99878], grad_fn=<MvBackward>), loss = 0.00000056\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 795: w = tensor([1.99966, 1.00002], requires_grad=True), y_pred = tensor([42.00059, 34.00001, 25.99943, 17.99885], grad_fn=<MvBackward>), loss = 0.00000050\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 800: w = tensor([1.99967, 1.00002], requires_grad=True), y_pred = tensor([42.00055, 34.00000, 25.99945, 17.99891], grad_fn=<MvBackward>), loss = 0.00000045\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 805: w = tensor([1.99969, 1.00002], requires_grad=True), y_pred = tensor([42.00053, 34.00001, 25.99949, 17.99896], grad_fn=<MvBackward>), loss = 0.00000040\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 810: w = tensor([1.99971, 1.00002], requires_grad=True), y_pred = tensor([42.00049, 34.00000, 25.99951, 17.99902], grad_fn=<MvBackward>), loss = 0.00000036\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 815: w = tensor([1.99972, 1.00002], requires_grad=True), y_pred = tensor([42.00047, 34.00000, 25.99954, 17.99907], grad_fn=<MvBackward>), loss = 0.00000033\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 820: w = tensor([1.99974, 1.00002], requires_grad=True), y_pred = tensor([42.00045, 34.00000, 25.99956, 17.99912], grad_fn=<MvBackward>), loss = 0.00000029\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 825: w = tensor([1.99975, 1.00002], requires_grad=True), y_pred = tensor([42.00042, 34.00000, 25.99958, 17.99916], grad_fn=<MvBackward>), loss = 0.00000026\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 830: w = tensor([1.99977, 1.00002], requires_grad=True), y_pred = tensor([42.00040, 34.00000, 25.99960, 17.99921], grad_fn=<MvBackward>), loss = 0.00000024\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 835: w = tensor([1.99978, 1.00001], requires_grad=True), y_pred = tensor([42.00038, 34.00000, 25.99963, 17.99925], grad_fn=<MvBackward>), loss = 0.00000021\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 840: w = tensor([1.99979, 1.00001], requires_grad=True), y_pred = tensor([42.00035, 34.00000, 25.99965, 17.99929], grad_fn=<MvBackward>), loss = 0.00000019\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 845: w = tensor([1.99980, 1.00001], requires_grad=True), y_pred = tensor([42.00034, 34.00000, 25.99967, 17.99933], grad_fn=<MvBackward>), loss = 0.00000017\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 850: w = tensor([1.99981, 1.00001], requires_grad=True), y_pred = tensor([42.00032, 34.00000, 25.99968, 17.99936], grad_fn=<MvBackward>), loss = 0.00000015\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 855: w = tensor([1.99982, 1.00001], requires_grad=True), y_pred = tensor([42.00030, 34.00000, 25.99970, 17.99940], grad_fn=<MvBackward>), loss = 0.00000014\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 860: w = tensor([1.99983, 1.00001], requires_grad=True), y_pred = tensor([42.00028, 33.99999, 25.99971, 17.99943], grad_fn=<MvBackward>), loss = 0.00000012\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 865: w = tensor([1.99984, 1.00001], requires_grad=True), y_pred = tensor([42.00027, 34.00000, 25.99973, 17.99946], grad_fn=<MvBackward>), loss = 0.00000011\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 870: w = tensor([1.99985, 1.00001], requires_grad=True), y_pred = tensor([42.00026, 34.00000, 25.99975, 17.99949], grad_fn=<MvBackward>), loss = 0.00000010\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 875: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00023, 33.99999, 25.99975, 17.99951], grad_fn=<MvBackward>), loss = 0.00000009\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 880: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00001, 25.99978, 17.99954], grad_fn=<MvBackward>), loss = 0.00000008\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 885: w = tensor([1.99987, 1.00001], requires_grad=True), y_pred = tensor([42.00021, 33.99999, 25.99978, 17.99956], grad_fn=<MvBackward>), loss = 0.00000007\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 890: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00002, 25.99981, 17.99960], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 895: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 33.99998, 25.99980, 17.99961], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 900: w = tensor([1.99989, 1.00001], requires_grad=True), y_pred = tensor([42.00020, 34.00001, 25.99982, 17.99963], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 905: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 33.99999, 25.99982, 17.99965], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 910: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 34.00001, 25.99984, 17.99967], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 915: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 34.00000, 25.99985, 17.99969], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 920: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00015, 34.00000, 25.99985, 17.99970], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 925: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00014, 34.00000, 25.99986, 17.99972], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 930: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00013, 33.99999, 25.99986, 17.99973], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 935: w = tensor([1.99992, 1.00000], requires_grad=True), y_pred = tensor([42.00013, 34.00000, 25.99988, 17.99975], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 940: w = tensor([1.99993, 1.00001], requires_grad=True), y_pred = tensor([42.00011, 33.99999, 25.99988, 17.99976], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 945: w = tensor([1.99993, 1.00000], requires_grad=True), y_pred = tensor([42.00012, 34.00000, 25.99989, 17.99977], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 950: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99989, 17.99978], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 955: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00011, 34.00000, 25.99990, 17.99980], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 960: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99981], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 965: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 970: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 33.99999, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 975: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00001, 25.99993, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 980: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99992, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 985: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 34.00001, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 990: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 995: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00006, 34.00000, 25.99993, 17.99987], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "#--------------------\n",
    "# 0) Training samples\n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "#--------------------\n",
    "# 1) Design Model: Weights to optimize and forward function\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "#--------------------\n",
    "# 2) Define loss and optimizer\n",
    "\n",
    "# callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "# def loss(y, y_pred):\n",
    "#     return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "learning_rate = 0.0013\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "\n",
    "#--------------------\n",
    "# 3) Training loop\n",
    "# основной цикл:\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "         \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "Использование модели:\n",
    "\n",
    "`torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "Applies a linear transformation to the incoming data: $y = xA^T + b$\n",
    "\n",
    "Parameters:\n",
    "* `in_features` – size of each input sample\n",
    "* `out_features` – size of each output sample\n",
    "* `bias` – If set to False, the layer will not learn an additive bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "Prediction before training: f(5) = tensor([[ 6.43014],\n",
      "        [ 4.13109],\n",
      "        [ 1.83204],\n",
      "        [-0.46701]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# X_test = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) # torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(n_samples, n_features)\n",
    "\n",
    "# n_samples, n_features = X_test.shape\n",
    "# input_size = n_features\n",
    "# output_size = n_features\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(n_features, 1)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([4, 2])\n",
      "Y.shape = torch.Size([4])\n",
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "Prediction before training: f(tensor([[ 1., 40.],\n",
      "        [ 2., 30.],\n",
      "        [ 3., 20.],\n",
      "        [ 4., 10.]])) = tensor([[-0.36370],\n",
      "        [-0.80204],\n",
      "        [-1.24038],\n",
      "        [-1.67872]], grad_fn=<MmBackward>)\n",
      "epoch 0: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 5: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 10: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 15: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 20: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 25: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 30: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 35: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 40: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 45: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 50: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 55: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 60: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 65: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 70: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 75: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 80: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 85: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 90: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 95: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 100: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 105: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 110: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 115: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 120: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 125: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 130: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 135: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 140: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 145: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 150: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 155: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 160: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 165: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 170: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 175: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 180: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 185: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 190: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 195: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 200: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 205: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 210: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 215: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 220: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 225: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 230: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 235: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 240: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 245: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 250: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 255: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 260: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 265: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 270: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 275: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 280: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 285: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 290: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 295: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 300: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 305: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 310: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 315: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 320: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 325: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 330: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 335: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 340: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 345: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 350: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 355: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 360: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 365: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 370: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 375: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 380: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 385: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 390: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 395: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 400: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 405: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 410: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpha\\.conda\\envs\\pyTorch_1_5v2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:432: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 415: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 420: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 425: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 430: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 435: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 440: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 445: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 450: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 455: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 460: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 465: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 470: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 475: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 480: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 485: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 490: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 495: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 500: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 505: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 510: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 515: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 520: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 525: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 530: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 535: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 540: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 545: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 550: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 555: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 560: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 565: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 570: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 575: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 580: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 585: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 590: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 595: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 600: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 605: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 610: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 615: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 620: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 625: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 630: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 635: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 640: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 645: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 650: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 655: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 660: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 665: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 670: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 675: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 680: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 685: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 690: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 695: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 700: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 705: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 710: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 715: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 720: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 725: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 730: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 735: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 740: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 745: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 750: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 755: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 760: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 765: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 770: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 775: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 780: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 785: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 790: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 795: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 800: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 805: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 810: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 815: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 820: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 825: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 830: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 835: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 840: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 845: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 850: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 855: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 860: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 865: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 870: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 875: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 880: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 885: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 890: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 895: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 900: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 905: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 910: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 915: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 920: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 925: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 930: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 935: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 940: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 945: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 950: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 955: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 960: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 965: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 970: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 975: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 980: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 985: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 990: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 995: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "#--------------------\n",
    "# 0) Training samples\n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "print(f'X.shape = {X.shape}')\n",
    "X_samples, X_features = X.shape\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "print(f'Y.shape = {Y.shape}')\n",
    "Y_features = 1\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "#--------------------\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(X_features, Y_features)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f({X}) = {model(X)}')\n",
    "\n",
    "\n",
    "# # model (модель, в нашем случае: линейная регрессия)\n",
    "# # прямое распространение:\n",
    "# def forward(X):\n",
    "#     return X @ w # Size([4])\n",
    "\n",
    "#--------------------\n",
    "# 2) Define loss and optimizer\n",
    "\n",
    "# callable function\n",
    "criterion  = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.0013\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "#--------------------\n",
    "# 3) Training loop\n",
    "# основной цикл:\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, Y)\n",
    "\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Спасибо за внимание!\n",
    "\n",
    "---\n",
    "### Технический раздел:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * И Введение в искусственные нейронные сети\n",
    "     * Базовые понятия и история\n",
    " * И Машинное обучение и концепция глубокого обучения\n",
    " * И Почему глубокое обучение начало приносить плоды и активно использоваться только после 2010 г?\n",
    "     * Производительность оборудования\n",
    "     * Доступность наборов данных и тестов\n",
    "     * Алгоритмические достижения в области глубокого обучения\n",
    "         * Улчшенные подходы к регуляризации\n",
    "         * Улучшенные схемы инициализации весов\n",
    "         * (повтор) Усовершенствованные методы градиентного супска\n",
    "         \n",
    "\n",
    "* Обратное распространение ошибки\n",
    " * Оптимизация\n",
    "     * Стохастический градиентный спуск\n",
    "     * Усовершенствованные методы градиентного супска\n",
    "* Введение в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> next <em class=\"qs\"></em> qs line \n",
    "<br/> next <em class=\"an\"></em> an line \n",
    "<br/> next <em class=\"nt\"></em> an line \n",
    "<br/> next <em class=\"df\"></em> df line \n",
    "<br/> next <em class=\"ex\"></em> ex line \n",
    "<br/> next <em class=\"pl\"></em> pl line \n",
    "<br/> next <em class=\"mn\"></em> mn line \n",
    "<br/> next <em class=\"plmn\"></em> plmn line \n",
    "<br/> next <em class=\"hn\"></em> hn line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Работа с графом потока вычислений нужна  для того, чтобы решить __задачу обучения многослойной ИНС__. А эта задача требует после получения резуьтатов и оценки ошибки __выполнения обратного прохода__ дающего градиент ошибки для весов (параметров) модели и последующей процедуры оптимизации весов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<img src=\"./img/ker_7.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "<img src=\"./img/ker_8.png\" alt=\"\" style=\"width: 500px;\"/>    \n",
    "<img src=\"./img/ker_9.png\" alt=\"\" style=\"width: 500px;\"/>        \n",
    "<img src=\"./img/ker_10.png\" alt=\"\" style=\"width: 500px;\"/>        \n",
    "<img src=\"./img/ker_11.png\" alt=\"\" style=\"width: 500px;\"/>            \n",
    "<img src=\"./img/ker_12.png\" alt=\"\" style=\"width: 500px;\"/>                \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch_1_5v2",
   "language": "python",
   "name": "pytorch_1_5v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
