{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция NN.2: Введение в PyTorch\n",
    "\n",
    "\n",
    "\n",
    "## Макрушин Сергей Вячеславович, Финансовый университет, 2020 г.\n",
    "\n",
    "v 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "﻿<style>\r\n",
       "\r\n",
       "\r\n",
       "b.n {\r\n",
       "    font-weight: normal;        \r\n",
       "}\r\n",
       "\r\n",
       "b.grbg {\r\n",
       "    background-color: #a0a0a0;      \r\n",
       "}\r\n",
       "\r\n",
       "b.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "b.b {    \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "b.g {\r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "// add your CSS styling here\r\n",
       "\r\n",
       "list-style: none;\r\n",
       "\r\n",
       "ul.s {\r\n",
       "//    list-style-type: none;\r\n",
       "    list-style: none;\r\n",
       "//    background-color: #ff0000;  \r\n",
       "//    color: #ffff00;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;\r\n",
       "}\r\n",
       "\r\n",
       "li.t {\r\n",
       "    list-style: none;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "*.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "li.t:before {\r\n",
       "    content: \"\\21D2\";    \r\n",
       "//    content: \"►\";\r\n",
       "//    padding-left: -1.2em;    \r\n",
       "    text-indent: -1.2em;    \r\n",
       "    display: block;\r\n",
       "    float: left;\r\n",
       "    \r\n",
       "    \r\n",
       "//    width: 1.2em;\r\n",
       "//    color: #ff0000;\r\n",
       "}\r\n",
       "\r\n",
       "i.m:before {\r\n",
       "    font-style: normal;    \r\n",
       "    content: \"\\21D2\";  \r\n",
       "}\r\n",
       "i.m {\r\n",
       "    font-style: normal; \r\n",
       "}    \r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "/* em {\r\n",
       "    font-style: normal; \r\n",
       "} */\r\n",
       "\r\n",
       "\r\n",
       "em.bl {\r\n",
       "    font-style: normal;     \r\n",
       "    font-weight: bold;        \r\n",
       "}\r\n",
       "\r\n",
       "/* em.grbg {\r\n",
       "    font-style: normal;         \r\n",
       "    background-color: #a0a0a0;      \r\n",
       "} */\r\n",
       "\r\n",
       "em.cr {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cb {    \r\n",
       "    font-style: normal;         \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cg {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "\r\n",
       "em.qs {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.qs::before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #ff0000;    \r\n",
       "    content: \"Q:\";  \r\n",
       "}\r\n",
       "\r\n",
       "em.an {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.an:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"A:\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.nt {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.nt:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Note:\";  \r\n",
       "}    \r\n",
       "    \r\n",
       "em.ex {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.ex:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #00ff00;    \r\n",
       "    content: \"Ex:\";  \r\n",
       "} \r\n",
       "    \r\n",
       "em.df {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.df:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Def:\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.pl {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.pl:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"+\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.mn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.mn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"-\";  \r\n",
       "}        \r\n",
       "\r\n",
       "em.plmn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.plmn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\00B1\";\\\\\"&plusmn;\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.hn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.hn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\21D2\";\\\\\"&rArr;\";  \r\n",
       "}     \r\n",
       "    \r\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем стиль для оформления презентации\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"file:./lec_v1.css\")\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Современные инструменты для создание моделей на основе ИНС\n",
    "\n",
    "* TensorFlow / Keras \n",
    "* PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Знакомство с PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Установка PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Стартуем консоль Анаконды:\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/lnnp2_anaconda_prompt1.png\" alt=\"Запуск консоли Анаконды\" style=\"width: 200px;\"/><br/>\n",
    "    <b>Запуск консоли Анаконды</b>    \n",
    "</center> \n",
    "\n",
    "Далее в консоли:\n",
    "2. Определяем текущую версию Python. Пример:\n",
    "\n",
    "```console\n",
    "(base) C:\\Users\\alpha>python --version\n",
    "Python 3.7.6\n",
    "```\n",
    "\n",
    "3. Экспортируем текущую версию окружения (например, в файл `environment.yml` ). Пример:\n",
    "\n",
    "```console\n",
    "(base) C:\\Users\\alpha>conda env export > environment.yml\n",
    "```\n",
    "4. Создаем новое __виртуальное окружение__ для текущей версии Python (можете выбрать удобноее Вам имя виртуального окружения).\n",
    "    * <em class=\"qs\"></em> Что такое __virtualenv__ (виртуальное окружение Python) и зачем оно нужно?\n",
    "    * <em class=\"an\"></em> Базовые ответы есть тут: \n",
    "        * https://pythontips.com/2013/07/30/what-is-virtualenv/\n",
    "        * https://stackoverflow.com/questions/41972261/what-is-a-virtualenv-and-why-should-i-use-one\n",
    "        * Важно знать, что у Anaconda есть собственный инсрументарий для работы c __virtualenv__, и если вы пользуетесь анакондой, то предпочитительно пользоваться им:\n",
    "            * Шпаргалка с кратким набором команд (см. раздел \"Using environments\"): https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "            * Документация: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\n",
    "    * Пример (`pyTorch_1_5v2` - имя нового окружения, `-f=environment.yml` - импортируем окружение из файла, созданного на шаге 3):\n",
    "\n",
    "```console\n",
    "(base) C:\\Users\\alpha>conda-env create -n pyTorch_1_5v2 python=3.7 -f=environment.yml\n",
    "Collecting package metadata (repodata.json): done\n",
    "Solving environment: done\n",
    "\n",
    "\n",
    "==> WARNING: A newer version of conda exists. <==\n",
    "  current version: 4.8.2\n",
    "  latest version: 4.8.3\n",
    "\n",
    "Please update conda by running\n",
    "\n",
    "    $ conda update -n base -c defaults conda\n",
    "\n",
    "\n",
    "\n",
    "Downloading and Extracting Packages\n",
    "anaconda-navigator-1 | 4.4 MB    | ############################################################################ | 100%\n",
    "Preparing transaction: done\n",
    "Verifying transaction: done\n",
    "Executing transaction: done\n",
    "#\n",
    "# To activate this environment, use\n",
    "#\n",
    "#     $ conda activate pyTorch_1_5v2\n",
    "#\n",
    "# To deactivate an active environment, use\n",
    "#\n",
    "#     $ conda deactivate\n",
    "```\n",
    "            \n",
    "4. С помощью `conda env list` Просматриваем список доступных виртуальных окружений. Пример:\n",
    "\n",
    "```console\n",
    "(base) C:\\Users\\alpha>conda env list\n",
    "# conda environments:\n",
    "#\n",
    "base                  *  C:\\ProgramData\\Anaconda3\n",
    "pyTorch_1_5              C:\\Users\\alpha\\.conda\\envs\\pyTorch_1_5\n",
    "pyTorch_1_5v2            C:\\Users\\alpha\\.conda\\envs\\pyTorch_1_5v2\n",
    "```\n",
    "\n",
    "4. С помощью команды `activate` переходим в созданное окружение. В результате имя окружения перед приглашением должно измениться на имя выбранного окружения. Пример:\n",
    "\n",
    "```console\n",
    "(base) C:\\Users\\alpha>activate pyTorch_1_5v2\n",
    "\n",
    "(pyTorch_1_5v2) C:\\Users\\alpha>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <center> \n",
    "<img src=\"./img/lnnp2_anaconda_prompt2.png\" alt=\"Создание новго virtualenv\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Создание новго virtualenv</b>    \n",
    "</center>  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. На сайте https://pytorch.org выбираем конфигурацию в которой необходимо установить PyTorch и копируем строку из поля __Run this Command__  и выполняем ее в консоли в новом окружении, например:\n",
    "\n",
    "```console\n",
    "(pyTorch_1_5) C:\\Users\\alpha>conda install pytorch torchvision cpuonly -c pytorch\n",
    "```\n",
    "\n",
    "Соглашаемся на установку новых пакетов:\n",
    "\n",
    "```console\n",
    "The following NEW packages will be INSTALLED:\n",
    "\n",
    "  blas               pkgs/main/win-64::blas-1.0-mkl\n",
    "  cpuonly            pytorch/noarch::cpuonly-1.0-0\n",
    "  freetype           pkgs/main/win-64::freetype-2.9.1-ha9979f8_1\n",
    "  icc_rt             pkgs/main/win-64::icc_rt-2019.0.0-h0cc432a_1\n",
    "  intel-openmp       pkgs/main/win-64::intel-openmp-2020.0-166\n",
    "  jpeg               pkgs/main/win-64::jpeg-9b-hb83a4c4_2\n",
    "  libpng             pkgs/main/win-64::libpng-1.6.37-h2a8f88b_0\n",
    "  libtiff            pkgs/main/win-64::libtiff-4.1.0-h56a325e_0\n",
    "  mkl                pkgs/main/win-64::mkl-2020.0-166\n",
    "  mkl-service        pkgs/main/win-64::mkl-service-2.3.0-py37hb782905_0\n",
    "  mkl_fft            pkgs/main/win-64::mkl_fft-1.0.15-py37h14836fe_0\n",
    "  mkl_random         pkgs/main/win-64::mkl_random-1.1.0-py37h675688f_0\n",
    "  ninja              pkgs/main/win-64::ninja-1.9.0-py37h74a9793_0\n",
    "  numpy              pkgs/main/win-64::numpy-1.18.1-py37h93ca92e_0\n",
    "  numpy-base         pkgs/main/win-64::numpy-base-1.18.1-py37hc3f5095_1\n",
    "  olefile            pkgs/main/win-64::olefile-0.46-py37_0\n",
    "  pillow             pkgs/main/win-64::pillow-7.0.0-py37hcc1f983_0\n",
    "  pytorch            pytorch/win-64::pytorch-1.5.0-py3.7_cpu_0\n",
    "  six                pkgs/main/win-64::six-1.14.0-py37_0\n",
    "  tk                 pkgs/main/win-64::tk-8.6.8-hfa6e2cd_0\n",
    "  torchvision        pytorch/win-64::torchvision-0.6.0-py37_cpu\n",
    "  xz                 pkgs/main/win-64::xz-5.2.5-h62dcd97_0\n",
    "  zstd               pkgs/main/win-64::zstd-1.3.7-h508b16e_0\n",
    "\n",
    "\n",
    "Proceed ([y]/n)? y\n",
    "```\n",
    "\n",
    "6. Проверяем успешность установки PyTorch:\n",
    "  \n",
    "    * В консоли, в текущем виртуальном окружении стартуем консоль Python:\n",
    "\n",
    "```console\n",
    "(pyTorch_1_5) C:\\Users\\alpha>python\n",
    "Python 3.7.7 (default, Apr 15 2020, 05:09:04) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>>```\n",
    "   \n",
    "   * В консоли импортируем модуль torch, и пишем тривиальное выражение с использованием torch:\n",
    "```console\n",
    ">>> import torch\n",
    ">>> x = torch.rand(3)\n",
    "```\n",
    "\n",
    "* для выхода из консоли Python пишем:\n",
    "```console\n",
    ">>> exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "1. В новом виртуальном окружении выполняем комадну \n",
    "\n",
    "```console\n",
    "(pyTorch_1_5) C:\\Users\\alpha>pip install ipykernel\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Добавление нового виртуального окружения к Jupyter Notebook__\n",
    "\n",
    "1. В новом виртуальном окружении выполняем комадну настройки ipykernel (в параемтре __name__ передаем имя нового виртуального окружения):\n",
    "```console\n",
    "(pyTorch_1_5v2) C:\\Users\\alpha>python -m ipykernel install --user --name=pyTorch_1_5v2\n",
    "```\n",
    "* настройка `ipykernel` позволяет jupyter работать с разными языками (например: julia, R; кстати JuPyteR, называется так именно из-за поддержки работы с этими (и многими другими) языками) и разными версями Python (и, естественно, разными virtualenv). Базовое описание архитектуры jupyter дано, например, тут: https://jupyter.readthedocs.io/en/latest/architecture/how_jupyter_ipython_work.html\n",
    "\n",
    "* Примеры работы с `ipykernel` есть тут:\n",
    "    * (ищите поиском по странице `ipykernel`)  https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
    "    * и тут: http://queirozf.com/entries/jupyter-kernels-how-to-add-change-remove\n",
    "\n",
    "2. Стартуем Jupyter Notebook для нового виртуалного окружения:\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/lnnp2_anaconda_2.png\" alt=\"Старт Jupyter Notebook для нового виртуалного окружения\" style=\"width: 300px;\"/><br/>\n",
    "    <b>Старт Jupyter Notebook для нового виртуалного окружения</b>    \n",
    "</center>\n",
    "\n",
    "\n",
    "3. Стратуем Jupyter Notebook и убеждаемся что при создании нового ноутбука есть возможность выбрать новое окружение:\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/lnnp2_jn2.png\" alt=\"Создание ноутбука в новом virtualenv\" style=\"width: 650px;\"/><br/>\n",
    "    <b>Создание ноутбука в новом virtualenv</b>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Проверяем в каком окружении мы запустили ноутбук (звездочка должна стоять напротив нужного ноутбука):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "pyTorch_1_5              C:\\Users\\alpha\\.conda\\envs\\pyTorch_1_5\n",
      "base                  *  C:\\Users\\alpha\\.conda\\envs\\pyTorch_1_5v2\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Проверяем что в новом ноутбуке можно успешно работать с PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тензоры и опреации с ними в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Что понимается под тензором в TensorFlow, PyTorch и аналогичных инструментах?__\n",
    "\n",
    "* <em class=\"df\"></em> __Тензор (в линейной алгербре)__ — объект линейной алгебры, линейно преобразующий элементы одного линейного пространства в элементы другого. Частными случаями тензоров являются скаляры, векторы, билинейные формы и т. п.\n",
    "\n",
    "* Часто тензор представляют как многомерную таблицу $ d \\times d \\times \\cdots \\times d $, заполненную числами - компонентами тензора (где $d$ — размерность векторного пространства, над которым задан тензор, а число размерностей совпадает рангом (валентностью) тензора. В случае ранга 2 запись тензора на письме выглядит как матрица.\n",
    "\n",
    "* Запись тензора в виде многомерной таблицы возможна __только после выбора базиса (системы координат)__ (кроме скаляров - тензоров размерности 0). Сам тензор как \"геометрическая сущность\" от выбора базиса не зависит. Это можно наглядно видеть на примере вектора (тензора ранга 1) при смене системы координат: \n",
    "    * при смене системы координат __компоненты вектора__ (и в общем случае - тензора) __меняются__ определённым образом.\n",
    "    * но сам __вектор__ — как \"геометрическая сущность\", образом которого может быть просто направленный отрезок — __при смене системы координат не изменяется__. Это же относится и к общему случаю - тензору.\n",
    "\n",
    "* В TensorFlow, PyTorch и  аналогичных библиотеках ключевыми объектами являются __тензоры__, но:\n",
    "    * __это не настоящие тензоры линейной алгебры__, а просто __многомерные таблицы__. В частности:\n",
    "        * эти тензоры не прдедусматривают определение базиса и возможности его изменения.\n",
    "    * для тензов (многомерных таблиц) в TensorFlow определены различные операции, важные для построения графа потока вычислений для численногомоделирования ИНС и ряда других приложений.\n",
    "    \n",
    "* Далее под тензорами мы будем иметь в виду то, что под ними понимается в TensorFlow, PyTorch и других аналогичных библиотеках.\n",
    "* __Тензоры__ в TensorFlow, PyTorch и аналогичных библиотеках в очень многих аспектах __похожы на массивы NumPy__.\n",
    "    \n",
    "<center> \n",
    "<img src=\"./img/ker_5.png\" alt=\"\"Тензоры\" в TensorFlow и аналогичных инструментах.\" style=\"width: 600px;\"/><br/>\n",
    "    <b>\"Тензоры\" в TensorFlow и аналогичных инструментах.</b>    \n",
    "</center> \n",
    "\n",
    "Тензоры в TensorFlow по логике использования и интерфейсу очень близки к `ndarray` в NumPy.\n",
    "* тензор размерности 0 - скаляр\n",
    "* тензор размерности 1 - вектор (одномерный массив)\n",
    "* тензор размерности 2 - матрица (двухмерный массив массив)\n",
    "* тензор размерности N - N-мерный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Создание тензоров__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0469e-38])\n",
      "tensor([ 0.0000e+00,  0.0000e+00, -5.4389e-27])\n",
      "tensor([[4.3726e-05, 2.6881e-06, 4.2039e-45],\n",
      "        [0.0000e+00, 1.4013e-45, 0.0000e+00]])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "tensor([[[[8.9082e-39, 4.2246e-39, 1.0194e-38],\n",
      "          [9.1837e-39, 8.4490e-39, 1.0102e-38]],\n",
      "\n",
      "         [[1.0561e-38, 1.0286e-38, 7.7144e-39],\n",
      "          [1.0469e-38, 9.5510e-39, 4.5001e-39]]],\n",
      "\n",
      "\n",
      "        [[[4.8674e-39, 9.9184e-39, 9.0000e-39],\n",
      "          [1.0561e-38, 1.0653e-38, 4.1327e-39]],\n",
      "\n",
      "         [[8.9082e-39, 9.8265e-39, 9.4592e-39],\n",
      "          [1.0561e-38, 1.0653e-38, 1.0469e-38]]]])\n"
     ]
    }
   ],
   "source": [
    "# В pytorch все основано на операциях с тензорами\n",
    "# Тензоры могут иметь:\n",
    "# 0 измерений - скаляры\n",
    "# 1 измерение - векторы\n",
    "# 2 измерения - матрицы\n",
    "# 3, 4, ... измерения - тензоры\n",
    "\n",
    "# Создание не инициализизированного тензора: torch.empty(size)\n",
    "# Нужно помнить, что перед использованием такого тензора его обязательно нужно инициализировать!\n",
    "\n",
    "x = torch.empty(1) # scalar\n",
    "print(x)\n",
    "x = torch.empty(3) # vector, 1D\n",
    "print(x)\n",
    "x = torch.empty(2,3) # matrix, 2D\n",
    "print(x)\n",
    "x = torch.empty(2,2,3) # tensor, 3 dimensions\n",
    "print(x)\n",
    "x = torch.empty(2,2,2,3) # tensor, 4 dimensions\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.20556915e-311 3.16202013e-322 0.00000000e+000]\n",
      "  [1.20556915e-311 4.37257913e-005 2.68810072e-006]]\n",
      "\n",
      " [[4.20389539e-045 1.40129846e-045 8.37699992e+169]\n",
      "  [7.46072016e-038 8.24339157e-067 3.88813040e-033]]]\n"
     ]
    }
   ],
   "source": [
    "# Большинство операций с тензорами очень похожа на опреации с массивами NumPy, но часть имеют небольшие отличия: \n",
    "x_np = np.empty((2,2,3))\n",
    "print(x_np)\n",
    "# x_np = np.empty(2,2,3) # Ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7196, 0.0637, 0.9817],\n",
       "        [0.7202, 0.5812, 0.4477],\n",
       "        [0.9523, 0.2562, 0.3778],\n",
       "        [0.4160, 0.3518, 0.2820],\n",
       "        [0.6643, 0.2659, 0.4579]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание тензора, заполненного случайными значениями (равномерно распредленными в [0, 1]): torch.rand(size)\n",
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Создание тензоров заполненных:\n",
    "# нулями: \n",
    "x = torch.zeros(5, 3)\n",
    "print(x)\n",
    "# единицами: \n",
    "x = torch.ones(5, 3)\n",
    "print(x)\n",
    "# тензор c единицами на главной диагонали:\n",
    "x = torch.eye(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определение размера тензора:\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# для каждого тензора задан тип значений:\n",
    "print(x.dtype) # тип заданный автоматически\n",
    "\n",
    "# явное указание типа:\n",
    "x = torch.zeros(5, 3, dtype=torch.float16)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<img src=\"./img/lnnp2_types1.png\" alt=\"Типы тензоров в PyTorch и массивов в NumPy\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Типы тензоров в PyTorch и массивов в NumPy</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 2.1019e-44],\n",
       "        [0.0000e+00, 1.4013e-45, 0.0000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(2, 3) # аналогично torch.empty\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6768, 0.5198, 0.6978],\n",
       "        [0.1581, 0.2027, 0.3723]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание тензора из данных:\n",
    "x = torch.Tensor([[0.6768, 0.5198, 0.6978], \n",
    "                  [0.1581, 0.2027, 0.3723]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6, 51,  6],\n",
      "        [15,  0, 37]]) <class 'torch.Tensor'> torch.int64\n",
      "tensor([[ 6., 51.,  6.],\n",
      "        [15.,  0., 37.]], dtype=torch.float64) <class 'torch.Tensor'> torch.float64\n"
     ]
    }
   ],
   "source": [
    "# создание тензора из данных:\n",
    "x = torch.tensor([[6, 51, 6],\n",
    "                  [15, 0, 37]])\n",
    "print(x, type(x), x.dtype)\n",
    "\n",
    "x = torch.tensor([[6, 51, 6],\n",
    "                  [15, 0, 37]], dtype=torch.float64)\n",
    "print(x, type(x), x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание тензора из массива numpy:\n",
    "a = np.array([1, 2, 3])\n",
    "x = torch.from_numpy(a)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  2  3]\n",
      "tensor([11,  2,  3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Carful: If the Tensor is on the CPU (not the GPU),\n",
    "# both objects will share the same memory location, so changing one\n",
    "# will also change the other\n",
    "\n",
    "a[0] += 10\n",
    "print(a)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  2,  3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch to numpy with .numpy()\n",
    "# Специфика использования общего массива данных сохраняется!\n",
    "b = x.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# заполнение тензора значениями:\n",
    "x = torch.Tensor(2, 3)\n",
    "x.fill_(0.5) # функции, оканчивающиеся на _ меняют значение тензора слева от точки\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Справка по операциям создания тензоров тут: https://pytorch.org/docs/stable/torch.html#creation-ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Операции с тензорами__\n",
    "\n",
    "Арифметические операции и математические функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[0.2830, 0.7729],\n",
      "        [0.4965, 0.5944]])\n",
      "\n",
      "y:\n",
      " tensor([[0.3583, 0.9232],\n",
      "        [0.9019, 0.4403]])\n",
      "\n",
      "z = x + y:\n",
      " tensor([[0.6412, 1.6961],\n",
      "        [1.3984, 1.0347]])\n",
      "\n",
      "z = torch.add(x,y):\n",
      " tensor([[0.6412, 1.6961],\n",
      "        [1.3984, 1.0347]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "print(f'x:\\n {x}\\n')\n",
    "y = torch.rand(2, 2)\n",
    "print(f'y:\\n {y}\\n')\n",
    "\n",
    "# поэлементное сложение:\n",
    "z = x + y\n",
    "print(f'z = x + y:\\n {z}\\n')\n",
    "z = torch.add(x,y)\n",
    "print(f'z = torch.add(x,y):\\n {z}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y2:\n",
      " tensor([[0.3583, 0.9232],\n",
      "        [0.9019, 0.4403]])\n",
      "\n",
      "y2.add_(x) in place:\n",
      " tensor([[0.6412, 1.6961],\n",
      "        [1.3984, 1.0347]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y2 = y.clone().detach() # копирование содержимого тензора в новый тензор\n",
    "print(f'y2:\\n {y2}\\n')\n",
    "\n",
    "# операции \"in place\" (помещают результат в объект слева от точки) в pytorch оканчиваются на _ :\n",
    "y2.add_(x)\n",
    "print(f'y2.add_(x) in place:\\n {y2}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = torch.sub(x, y):\n",
      " tensor([[-0.0753, -0.1504],\n",
      "        [-0.4055,  0.1540]])\n",
      "\n",
      "z = torch.mul(x,y):\n",
      " tensor([[0.1014, 0.7135],\n",
      "        [0.4478, 0.2617]])\n",
      "\n",
      "z = torch.div(x,y):\n",
      " tensor([[0.7898, 0.8371],\n",
      "        [0.5505, 1.3498]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# вычитание:\n",
    "z = x - y\n",
    "z = torch.sub(x, y)\n",
    "print(f'z = torch.sub(x, y):\\n {z}\\n')\n",
    "\n",
    "# умножение (поэлементное!):\n",
    "z = x * y\n",
    "z = torch.mul(x,y)\n",
    "print(f'z = torch.mul(x,y):\\n {z}\\n')\n",
    "\n",
    "# деление:\n",
    "z = x / y\n",
    "z = torch.div(x,y)\n",
    "print(f'z = torch.div(x,y):\\n {z}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x - y:\n",
      " tensor([[-0.0753, -0.1504],\n",
      "        [-0.4055,  0.1540]])\n",
      "\n",
      "z = torch.abs(x - y):\n",
      " tensor([[0.0753, 0.1504],\n",
      "        [0.4055, 0.1540]])\n",
      "\n",
      "z.abs_():\n",
      " tensor([[0.0753, 0.1504],\n",
      "        [0.4055, 0.1540]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'x - y:\\n {x - y}\\n')\n",
    "\n",
    "z = torch.abs(x - y) # полэлементный рассчет модуля\n",
    "print(f'z = torch.abs(x - y):\\n {z}\\n')\n",
    "\n",
    "z = x - y\n",
    "z.abs_()\n",
    "print(f'z.abs_():\\n {z}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9602, 0.7159],\n",
       "        [0.8793, 0.8285]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cos(x) # поэлементный рассчет cos \n",
    "# x.cos_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5703, 0.6841],\n",
       "        [0.6216, 0.6444]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(x) # рассчет сигмоиды\n",
    "# x.sigmoid_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Справка по математическим опреациям тут: https://pytorch.org/docs/stable/torch.html#math-operations\n",
    "\n",
    "Операции среза:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7997, 0.4989, 0.1586],\n",
      "        [0.1758, 0.5943, 0.3483],\n",
      "        [0.4643, 0.4594, 0.6720],\n",
      "        [0.3840, 0.4478, 0.9048],\n",
      "        [0.2116, 0.7514, 0.1852]])\n",
      "tensor(0.5943)\n",
      "tensor([0.7997, 0.1758, 0.4643, 0.3840, 0.2116])\n",
      "tensor([0.1758, 0.5943, 0.3483])\n"
     ]
    }
   ],
   "source": [
    "# Операции среза (slicing) (работает аналогично NumPy):\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[1, 1]) # элемент с индексо 1, 1 (результат: тензор размерности 0!)\n",
    "print(x[:, 0]) # все строки, столбец 0\n",
    "print(x[1, :]) # строка 1, все столбцы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5943) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# тензор размерности 0 (скаляр), все равно остается типом 'torch.Tensor':\n",
    "print(x[1,1], type(x[1,1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5942692160606384\n"
     ]
    }
   ],
   "source": [
    "# получение самого значения из тензора размерности 0:\n",
    "print(x[1,1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17581236362457275\n",
      "0.5942692160606384\n",
      "0.348280131816864\n"
     ]
    }
   ],
   "source": [
    "# итерирование по тензору:\n",
    "for v in x[1, :]:\n",
    "    print(v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8380, -0.5930,  0.4406,  1.3580],\n",
      "        [ 1.4499,  0.3562,  0.7375,  0.7863],\n",
      "        [ 1.1890,  1.4015,  1.5559, -1.4440],\n",
      "        [-0.0319,  0.9698,  1.3826,  1.5762]]) torch.Size([4, 4]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Изменение формы тензора (reshape) с помощью torch.view():\n",
    "x = torch.randn(4, 4) # матрица 4 на 4\n",
    "print(x, x.size(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8380, -0.5930,  0.4406,  1.3580,  1.4499,  0.3562,  0.7375,  0.7863,\n",
      "         1.1890,  1.4015,  1.5559, -1.4440, -0.0319,  0.9698,  1.3826,  1.5762]) torch.Size([16]) \n",
      "\n",
      "tensor([[[-0.8380, -0.5930,  0.4406,  1.3580],\n",
      "         [ 1.4499,  0.3562,  0.7375,  0.7863]],\n",
      "\n",
      "        [[ 1.1890,  1.4015,  1.5559, -1.4440],\n",
      "         [-0.0319,  0.9698,  1.3826,  1.5762]]]) torch.Size([2, 2, 4]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x.view(16) # вектор из 16 компонент\n",
    "print(y, y.size(), '\\n')\n",
    "z = x.view(2, 2, 4) # тензор 2 на 2 на 4\n",
    "print(z, z.size(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8380, -0.5930,  0.4406,  1.3580,  1.4499,  0.3562,  0.7375,  0.7863],\n",
      "        [ 1.1890,  1.4015,  1.5559, -1.4440, -0.0319,  0.9698,  1.3826,  1.5762]]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "t = x.view(-1, 8)  # размер -1 означает, что размерность этой компоненты будет подобрана автоматически\n",
    "print(t, t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<img src=\"./img/lnnp2_resize1.png\" alt=\"Операции изменения размера матриц\" style=\"width: 300px;\"/><br/>\n",
    "    <b>Операции изменения размера матриц</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1830, 0.2154, 0.6007],\n",
      "        [0.4754, 0.8501, 0.3963]])\n",
      "tensor([[0.3986, 0.9731, 0.0460],\n",
      "        [0.7828, 0.7819, 0.3551]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(2,3)\n",
    "print(x1)\n",
    "y1 = torch.rand(2,3)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1830, 0.2154, 0.6007],\n",
      "        [0.4754, 0.8501, 0.3963],\n",
      "        [0.3986, 0.9731, 0.0460],\n",
      "        [0.7828, 0.7819, 0.3551]]) torch.Size([4, 3])\n",
      "tensor([[0.1830, 0.2154, 0.6007],\n",
      "        [0.4754, 0.8501, 0.3963],\n",
      "        [0.3986, 0.9731, 0.0460],\n",
      "        [0.7828, 0.7819, 0.3551]]) torch.Size([4, 3])\n",
      "tensor([[0.1830, 0.2154, 0.6007, 0.3986, 0.9731, 0.0460],\n",
      "        [0.4754, 0.8501, 0.3963, 0.7828, 0.7819, 0.3551]]) torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# Конкатенация (concatenation):\n",
    "\n",
    "# Concatenates 2 tensors on zeroth dimension:\n",
    "concat1 = torch.cat((x1, y1))\n",
    "print(concat1, concat1.size())         \n",
    "\n",
    "# Concatenates 2 tensors on zeroth dimension\n",
    "x = torch.rand(2,3)\n",
    "concat2 = torch.cat((x1, y1), dim=0)\n",
    "print(concat2, concat2.size()) \n",
    "\n",
    "# Concatenates 2 tensors on first dimension\n",
    "x = torch.rand(2,3)\n",
    "concat3 = torch.cat((x1, y1), dim=1)\n",
    "print(concat3, concat3.size())       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1249, 0.9633, 0.0452],\n",
      "        [0.0543, 0.3090, 0.6743]])\n",
      "(tensor([[0.1249, 0.9633, 0.0452]]), tensor([[0.0543, 0.3090, 0.6743]])) torch.Size([1, 3])\n",
      "tensor([[0.1249, 0.9633],\n",
      "        [0.0543, 0.3090]]) torch.Size([2, 2]) \n",
      " tensor([[0.0452],\n",
      "        [0.6743]]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Разбиение тензора (split): \n",
    "print(x1)\n",
    "\n",
    "splitted1 = x1.split(split_size=1, dim=0)\n",
    "print(splitted1, splitted1[0].size())       # 2 tensors of 2x2 and 1x2 size\n",
    "\n",
    "splitted2 = x1.split(split_size=2, dim=1)\n",
    "print(splitted2[0], splitted2[0].size(), '\\n', splitted2[1], splitted2[1].size())       # 2 tensors of 2x2 and 1x2 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1830, 0.2154, 0.6007],\n",
      "        [0.4754, 0.8501, 0.3963]])\n",
      "tensor([[0.3986, 0.9731, 0.0460],\n",
      "        [0.7828, 0.7819, 0.3551]])\n",
      "tensor([[[0.1830, 0.2154, 0.6007],\n",
      "         [0.4754, 0.8501, 0.3963]],\n",
      "\n",
      "        [[0.3986, 0.9731, 0.0460],\n",
      "         [0.7828, 0.7819, 0.3551]]]) torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# stack:\n",
    "print(x1)\n",
    "print(y1)\n",
    "\n",
    "stacked1 = torch.stack((x1, y1), dim=0)\n",
    "print(stacked1, stacked1.size()) # возвращает тензор: 2(в результате stak!) x 2 x 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1830, 0.2154, 0.6007],\n",
      "         [0.3986, 0.9731, 0.0460]],\n",
      "\n",
      "        [[0.4754, 0.8501, 0.3963],\n",
      "         [0.7828, 0.7819, 0.3551]]]) torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "stacked2 = torch.stack((x1, y1), dim=1)\n",
    "print(stacked2, stacked2.size()) # возвращает тензор: 2 x 2(в результате stak!) x 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7298],\n",
      "         [0.8666]],\n",
      "\n",
      "        [[0.9598],\n",
      "         [0.2414]],\n",
      "\n",
      "        [[0.4696],\n",
      "         [0.6808]]])\n",
      "tensor([[0.7298, 0.8666],\n",
      "        [0.9598, 0.2414],\n",
      "        [0.4696, 0.6808]])\n"
     ]
    }
   ],
   "source": [
    "#sqeeze and unsqueeze\n",
    "x2 = torch.rand(3, 2, 1) # a tensor of size 3x2x1\n",
    "print(x2)\n",
    "squeezed1 = x2.squeeze()\n",
    "print(squeezed1)  # remove the 1 sized dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3965, 0.5168, 0.6922])\n",
      "tensor([[0.3965, 0.5168, 0.6922]]) torch.Size([1, 3])\n",
      "tensor([[0.3965],\n",
      "        [0.5168],\n",
      "        [0.6922]]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "x3 = torch.rand(3)\n",
    "print(x3)\n",
    "\n",
    "with_fake_dimension1 = x3.unsqueeze(0)\n",
    "print(with_fake_dimension1, with_fake_dimension1.size()) # added a fake zeroth dimensionz \n",
    "\n",
    "with_fake_dimension2 = x3.unsqueeze(1)\n",
    "print(with_fake_dimension2, with_fake_dimension2.size()) # added a fake zeroth dimensionz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.]) tensor([0., 1., 2.])\n",
      "tensor([[0., 1., 2.],\n",
      "        [0., 2., 4.],\n",
      "        [0., 3., 6.],\n",
      "        [0., 4., 8.]]) torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Распространение (broadcasting) - так же как в NumPy:\n",
    "\n",
    "t1 = torch.arange(1.0, 5.0)\n",
    "t3 = torch.arange(0.0, 3.0)\n",
    "print(t1, t3)\n",
    "tm = t1.ger(t3)\n",
    "print(tm, tm.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-6ea1dd71dd31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "t1 * tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), torch.Size([4, 3]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.size(), tm.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.]])\n",
      "tensor([[0., 1., 2.],\n",
      "        [0., 2., 4.],\n",
      "        [0., 3., 6.],\n",
      "        [0., 4., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(t1.unsqueeze(1))\n",
    "print(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 0.,  4.,  8.],\n",
       "        [ 0.,  9., 18.],\n",
       "        [ 0., 16., 32.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.unsqueeze(1) * tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1]), torch.Size([4, 3]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.unsqueeze(1).size(), tm.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операции агрегации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30.)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.tensor(\n",
    "        [[0., 1., 2.],\n",
    "        [0., 2., 4.],\n",
    "        [0., 3., 6.],\n",
    "        [0., 4., 8.]])\n",
    "\n",
    "# суммирование по всем элементам:\n",
    "mat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., 10., 20.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# суммирование по оси 0:\n",
    "mat.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  6.,  9., 12.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# суммирование по оси 1:\n",
    "mat.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "# получение среднего значения:\n",
    "\n",
    "print(mat.mean())\n",
    "print(mat.mean(dim=0))\n",
    "print(mat.mean(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матричные операции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[0.7255, 0.9459],\n",
      "        [0.0180, 0.9965]])\n",
      "\n",
      "y:\n",
      " tensor([[0.5618, 0.6862],\n",
      "        [0.2016, 0.1258]])\n",
      "\n",
      "z = torch.mul(x,y):\n",
      " tensor([[0.4076, 0.6491],\n",
      "        [0.0036, 0.1253]])\n",
      "\n",
      "z = torch.mul(x,y):\n",
      " tensor([[0.4076, 0.6491],\n",
      "        [0.0036, 0.1253]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Умножение (поэлементное!):\n",
    "\n",
    "x = torch.rand(2, 2)\n",
    "print(f'x:\\n {x}\\n')\n",
    "y = torch.rand(2, 2)\n",
    "print(f'y:\\n {y}\\n')\n",
    "\n",
    "z = x * y\n",
    "print(f'z = torch.mul(x,y):\\n {z}\\n')\n",
    "z = torch.mul(x,y)\n",
    "print(f'z = torch.mul(x,y):\\n {z}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2450, -0.9958,  0.3320],\n",
      "        [-1.1015,  1.3226,  3.2153]]) torch.Size([2, 3])\n",
      "tensor([-0.0979,  2.0161, -0.1482]) torch.Size([3])\n",
      "tensor([-2.0808,  2.2977]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Умножение матрицы на вектор:\n",
    "\n",
    "# torch.mv(input, vec, out=None) → Tensor\n",
    "# Performs a matrix-vector product of the matrix input and the vector vec.\n",
    "# If input is a (n \\times m)(n×m) tensor, vec is a 1-D tensor of size mm , out will be 1-D of size nn .\n",
    "\n",
    "mat = torch.randn(2, 3)\n",
    "print(mat, mat.size())\n",
    "vec = torch.randn(3)\n",
    "print(vec, vec.size())\n",
    "res = torch.mv(mat, vec)\n",
    "print(res, res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6114,  0.1754, -0.1479],\n",
      "        [-0.8400, -1.4489, -0.1735]]) torch.Size([2, 3])\n",
      "tensor([[-0.2815,  0.7116, -1.1436],\n",
      "        [ 0.1062,  0.8233,  1.0840],\n",
      "        [ 0.4111,  1.1746, -0.1564]]) torch.Size([3, 3])\n",
      "tensor([[ 0.4115, -1.1759,  2.0561],\n",
      "        [ 0.0113, -1.9944, -0.5829]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Умножение матрицы на матрицу:\n",
    "\n",
    "# torch.mm(input, mat2, out=None) → Tensor\n",
    "# Performs a matrix multiplication of the matrices input and mat2.\n",
    "# If input is a (n×m) tensor, mat2 is a (m×p) tensor, out will be a (n×p) tensor.\n",
    "\n",
    "mat1 = torch.randn(2, 3)\n",
    "print(mat1, mat1.size())\n",
    "mat2 = torch.randn(3, 3)\n",
    "print(mat2, mat2.size())\n",
    "res = torch.mm(mat1, mat2)\n",
    "print(res, res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4115, -1.1759,  2.0561],\n",
       "        [ 0.0113, -1.9944, -0.5829]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1.mm(mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) torch.Size([3])\n",
      "tensor([1, 2]) torch.Size([2])\n",
      "tensor([[1, 2],\n",
      "        [2, 4],\n",
      "        [3, 6]]) torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Outer product of 2 vectors\n",
    "vec1 = torch.arange(1, 4)    # Size 3\n",
    "print(vec1, vec1.size())\n",
    "vec2 = torch.arange(1, 3)    # Size 2\n",
    "print(vec2, vec2.size())\n",
    "res = torch.ger(vec1, vec2) # vec1 - рассматривается как вектор-столбец; vec2 - рассматривается как вектор-строка\n",
    "print(res, res.size()) # Size 3x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [2, 4],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.ger(vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Функция matmul__\n",
    "\n",
    "* Matrix product of two tensors: `torch.matmul(input, other, out=None)` → Tensor\n",
    "* The behavior depends on the dimensionality of the tensors as follows:\n",
    "    * If __both tensors are 1-dimensional__, the __dot product (scalar) is returned__.\n",
    "    * If __both arguments are 2-dimensional__, the __matrix-matrix product is returned__.\n",
    "    * If the __first argument is 1-dimensional and the second argument is 2-dimensional__, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.\n",
    "    * If the __first argument is 2-dimensional and the second argument is 1-dimensional__, the matrix-vector product is returned.\n",
    "    * If __both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2)__, then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if input is a $j \\times 1 \\times n \\times m$ tensor and other is a $k \\times m \\times p$ tensor, out will be an $j \\times k \\times n \\times p$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(3*4).view(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.5219, -1.2146,  1.3170]) torch.Size([3])\n",
      "tensor([ 0.2303,  0.9512, -0.0546]) torch.Size([3])\n",
      "tensor(-1.8081) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# vector x vector\n",
    "tensor1 = torch.randn(3)\n",
    "print(tensor1, tensor1.size())\n",
    "tensor2 = torch.randn(3)\n",
    "print(tensor2, tensor2.size())\n",
    "res = torch.matmul(tensor1, tensor2)\n",
    "print(res, res.size()) # результат: скаляр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.8081)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вызов функции matmul можно выполнять с помощью оператора @:\n",
    "tensor1 @ tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]]) torch.Size([3, 4])\n",
      "tensor([0, 1, 2]) torch.Size([3])\n",
      "tensor([20, 23, 26, 29]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# vector x matrix \n",
    "tensor1 = torch.arange(3*4).view(3, 4)\n",
    "print(tensor1, tensor1.size())\n",
    "tensor2 = torch.arange(3)\n",
    "print(tensor2, tensor2.size())\n",
    "\n",
    "res = torch.matmul(tensor2, tensor1)\n",
    "print(res, res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 23, 26, 29])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# или:\n",
    "tensor2 @ tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]]) torch.Size([3, 4])\n",
      "tensor([0, 1, 2, 3]) torch.Size([4])\n",
      "tensor([14, 38, 62]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# matrix x vector\n",
    "tensor1 = torch.arange(3*4).view(3, 4)\n",
    "print(tensor1, tensor1.size())\n",
    "tensor2 = torch.arange(4)\n",
    "print(tensor2, tensor2.size())\n",
    "\n",
    "res = torch.matmul(tensor1, tensor2)\n",
    "print(res, res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 38, 62])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 @ tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0254,  0.7122, -2.5060,  0.5180],\n",
      "         [ 0.1458, -0.8855, -0.8308,  1.5698],\n",
      "         [-0.7911,  0.8250, -0.9246, -0.1922]],\n",
      "\n",
      "        [[-0.3259, -0.8213,  1.5900, -0.1392],\n",
      "         [-0.5806, -0.1336, -2.4994, -0.2150],\n",
      "         [-0.1507, -1.1597,  0.4157,  0.5377]],\n",
      "\n",
      "        [[ 0.1935, -0.6100, -0.0840, -0.2509],\n",
      "         [ 0.2435,  0.0852, -0.7656, -0.5838],\n",
      "         [-1.1525,  1.2272, -0.6801,  1.1422]],\n",
      "\n",
      "        [[ 1.5298,  0.4393,  1.0724, -1.3998],\n",
      "         [ 0.5910,  1.0698,  0.0492, -0.4684],\n",
      "         [-0.5860,  0.3775, -0.4975,  0.1747]],\n",
      "\n",
      "        [[ 0.6234, -1.6125,  0.0581, -0.9023],\n",
      "         [ 0.8492,  0.4678,  2.2095, -0.4018],\n",
      "         [-0.4183,  1.1057,  0.4946,  0.2117]],\n",
      "\n",
      "        [[-0.4135, -1.1620, -0.4104,  0.7465],\n",
      "         [-1.5399,  0.3171,  0.6196, -1.0964],\n",
      "         [-1.4138, -0.7486, -0.2011, -0.4922]],\n",
      "\n",
      "        [[-0.1190,  0.5037,  1.6496,  0.6041],\n",
      "         [ 0.0585,  1.0287, -0.0672,  0.6690],\n",
      "         [-0.2793, -0.8093, -1.3930, -0.2564]],\n",
      "\n",
      "        [[-0.2652,  0.3194,  0.1951,  0.4310],\n",
      "         [-0.9333, -2.0235, -0.0407,  1.4903],\n",
      "         [-0.2456, -0.3495,  0.5053,  0.7812]],\n",
      "\n",
      "        [[ 0.5721, -0.2114, -2.9636,  0.6201],\n",
      "         [-1.0270,  0.1474,  0.7315, -0.6116],\n",
      "         [ 1.0629,  0.8071,  0.5145,  0.4959]],\n",
      "\n",
      "        [[ 0.0760, -0.5637, -0.7976,  0.0616],\n",
      "         [-1.1659, -1.4374, -0.7419,  0.4414],\n",
      "         [-0.4875, -0.7246,  0.1285, -0.3838]]]) torch.Size([10, 3, 4]) \n",
      "------------\n",
      "tensor([ 0.1706, -0.5846,  0.1843, -1.3673]) torch.Size([4]) \n",
      "------------\n",
      "tensor([[-1.5908, -1.7570, -0.5249],\n",
      "        [ 0.9079, -0.1876, -0.0064],\n",
      "        [ 0.7172,  0.6489, -2.6011],\n",
      "        [ 2.1158,  0.1250, -0.6511],\n",
      "        [ 2.2935,  0.8279, -0.9160],\n",
      "        [-0.4876,  1.1653,  0.8324],\n",
      "        [-0.8368, -1.5186,  0.5194],\n",
      "        [-0.7854, -1.0215, -0.8126],\n",
      "        [-1.1729,  0.7098, -0.8738],\n",
      "        [ 0.1113, -0.0989,  0.8890]]) torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "# batched matrix x broadcasted vector\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "print(tensor1, tensor1.size(), '\\n------------')\n",
    "tensor2 = torch.randn(4)\n",
    "print(tensor2, tensor2.size(), '\\n------------')\n",
    "res = torch.matmul(tensor1, tensor2)\n",
    "print(res, res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 4]) \n",
      "------------\n",
      "torch.Size([10, 4, 5]) \n",
      "------------\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# batched matrix x batched matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "print(tensor1.size(), '\\n------------')\n",
    "tensor2 = torch.randn(10, 4, 5)\n",
    "print(tensor2.size(), '\\n------------')\n",
    "res = torch.matmul(tensor1, tensor2)\n",
    "print(res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 4]) \n",
      "------------\n",
      "tensor([[ 2.3362,  0.1706,  0.4528,  0.8596,  1.0453],\n",
      "        [-0.0351,  0.2272, -0.4056, -0.2502,  0.9134],\n",
      "        [-1.5136,  0.4217,  1.3492,  0.6766,  0.9003],\n",
      "        [-1.6946, -1.3057,  1.5708,  0.5506, -2.0987]]) torch.Size([4, 5]) \n",
      "------------\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# batched matrix x broadcasted matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "print(tensor1.size(), '\\n------------')\n",
    "tensor2 = torch.randn(4, 5)\n",
    "print(tensor2, tensor2.size(), '\\n------------')\n",
    "res = torch.matmul(tensor1, tensor2)\n",
    "print(res.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Применение тензоров:  прямое распространение сигналов и оценка ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Постановка задачи__ \n",
    "\n",
    "* У нас есть набор данных $D$, состоящий из пар $(\\pmb{x}, \\pmb{y})$, где $\\pmb{x}$ - признаки, а $\\pmb{y}$ - правильный ответ. \n",
    "* Модель сети $f_L$, имеющей $L$ слоев с весами $\\pmb{\\theta}$ (совокупность весов нейронов из всех слоев) на этих данных делает некоторые предсказания $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})$\n",
    "* Задана функция ошибки $E$, которую можно подсчитать на каждом примере: $E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$ (например, это может быть квадрат или модуль отклонения $\\hat{\\pmb{y}}$ от $\\pmb{y}$ в случае регрессии или перекрестная энтропия в случае классификации)\n",
    "* Тогда суммарная ошибка на наборе данных $D$ будет функцией от параметров модели: $E(\\pmb{\\theta})$ и определяется как $E(\\pmb{\\theta})=\\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки</b>    \n",
    "</center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямое распространение сигналов__\n",
    "\n",
    "* Модель нейронной сети это иерархия (она может быть простой и очень сложной) связанных (последовательно применяемых) функций слоев:\n",
    "    * т.е. модель сети $f_L$ может быть представленна как суперпозиция из $L$ слоев $h^i\\text{, }i \\in \\{1, \\ldots, L\\}$, каждый из которых параметризуется своими весами $w_i$:\n",
    "$$f_L(\\pmb{x}, \\pmb{\\theta})=f_L(\\pmb{x}, \\pmb{w}_1, \\ldots, \\pmb{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_11.png\" alt=\"многослойный перцептрон с двумя скрытыми слоями\" style=\"width: 600px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример модели сети: многослойный перцептрон с двумя скрытыми слоями</b>    \n",
    "</center> \n",
    "\n",
    "* Прямое распространение сигналов по модели (в частности: нейронной сети) реализуется с помощощью __прямого прохода (forward pass)__: входящая информация (вектор $\\pmb{x}$) распространяется через сеть $f_L$ с учетом весов связей $\\pmb{\\theta}$, расчитывается выходной вектор $\\hat{\\pmb{y}}=f_L(\\pmb{x}, \\pmb{\\theta})$ .\n",
    "    * Каждый слой нейронной сети - это последовательно применяемая функция слоя, которая рассчитывается при помощи операций с тензорами.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_12.png\" alt=\"пример прямого прохода\" style=\"width: 600px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример прямого прохода</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Почему Тензор?\n",
    "# Персептрон на тензорах\n",
    "# Два слоя перспетронов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для решения этой задачи и используется алгоритм обратного распространения ошибки (backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализация на PyTorch\n",
    "# Линейная регрессия\n",
    "# Персептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Последовательность операций с тензормаи используется для расчета результата:\n",
    "    * Прямой проход (forward pass): входящая информация (вектор $\\pmb{x}$) распространяется через сеть с учетом весов связей, расчитывается выходной вектор $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})= h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$\n",
    "    * Оценки ошибки $E(\\hat{\\pmb{y}}, \\pmb{y})$ на множестве правильных ответов: $\\pmb{y}$.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица X: \n",
      "tensor([[ 1., 40.],\n",
      "        [ 2., 30.],\n",
      "        [ 3., 20.],\n",
      "        [ 4., 10.]])\n",
      "X.size = torch.Size([4, 2])\n",
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n"
     ]
    }
   ],
   "source": [
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "\n",
    "# вариант исходных данны #1 (2й признак всегда равен 0):\n",
    "# X = torch.tensor([[1., 0.],\n",
    "#                   [2., 0.],\n",
    "#                   [3., 0.],\n",
    "#                   [4., 0.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "# вариант исходных данны #2 (2й признак используется и существенно больше 1го):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "print(f'Матрица X: \\n{X}')\n",
    "print(f'X.size = {X.size()}') \n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "\n",
    "# вариант #1:\n",
    "# w_ans = torch.tensor([2., 0.], dtype=torch.float32)\n",
    "\n",
    "# вариант #2:\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "print(f'w:\\n{w}')\n",
    "\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "# градиент: \n",
    "# рассчитан аналитически по модели и функции потерь:\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2 * (w*x - y) * x\n",
    "def gradient(x, y, y_pred):   \n",
    "#     print(f'''y = {y},\n",
    "#     y_pred = {y_pred},\n",
    "#     (2* (y_pred - y)).unsqueeze(1) = {(2* (y_pred - y)).unsqueeze(1)},\n",
    "#     x = {x},\n",
    "#     ((2* (y_pred - y)).unsqueeze(1) * x) = {((2* (y_pred - y)).unsqueeze(1) * x)},\n",
    "#     ((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0) = {((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0)}''')\n",
    "    return ((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: w = tensor([0.16900, 2.21000]), y_pred = tensor([0., 0., 0., 0.]), loss = 980.00000000\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 5: w = tensor([0.13813, 0.24422]), y_pred = tensor([81.70274, 61.57523, 41.44772, 21.32021]), loss = 646.58898926\n",
      "gradient = tensor([  77.23860, 1378.76135])\n",
      "epoch 10: w = tensor([0.33966, 1.82453]), y_pred = tensor([15.22918, 11.70162,  8.17406,  4.64650]), loss = 427.49359131\n",
      "gradient = tensor([  -89.12970, -1114.91895])\n",
      "epoch 15: w = tensor([0.34364, 0.53338]), y_pred = tensor([68.78387, 52.09386, 35.40386, 18.71385]), loss = 283.42648315\n",
      "gradient = tensor([ 47.01929, 904.69324])\n",
      "epoch 20: w = tensor([0.49880, 1.56850]), y_pred = tensor([25.13912, 19.37721, 13.61530,  7.85340]), loss = 188.61242676\n",
      "gradient = tensor([ -61.92349, -731.13959])\n",
      "epoch 25: w = tensor([0.52316, 0.72007]), y_pred = tensor([60.23328, 45.87371, 31.51414, 17.15457]), loss = 126.13953400\n",
      "gradient = tensor([ 27.57071, 593.68567])\n",
      "epoch 30: w = tensor([0.64554, 1.39771]), y_pred = tensor([31.56791, 24.41181, 17.25570, 10.09960]), loss = 84.91027832\n",
      "gradient = tensor([ -43.72150, -479.40973])\n",
      "epoch 35: w = tensor([0.68103, 0.83984]), y_pred = tensor([54.55604, 41.79293, 29.02982, 16.26671]), loss = 57.64197159\n",
      "gradient = tensor([ 15.14910, 389.64639])\n",
      "epoch 40: w = tensor([0.77979, 1.28313]), y_pred = tensor([35.72054, 27.71401, 19.70748, 11.70095]), loss = 39.55477524\n",
      "gradient = tensor([ -31.46259, -314.29950])\n",
      "epoch 45: w = tensor([0.82057, 0.91600]), y_pred = tensor([50.77066, 39.11558, 27.46050, 15.80541]), loss = 27.51074600\n",
      "gradient = tensor([  7.30249, 255.77905])\n",
      "epoch 50: w = tensor([0.90194, 1.20568]), y_pred = tensor([38.38666, 29.87982, 21.37299, 12.86615]), loss = 19.44941521\n",
      "gradient = tensor([ -23.13505, -206.00873])\n",
      "epoch 55: w = tensor([0.94440, 0.96380]), y_pred = tensor([48.23264, 37.35892, 26.48521, 15.61149]), loss = 14.01713943\n",
      "gradient = tensor([  2.42605, 167.94620])\n",
      "epoch 60: w = tensor([1.01265, 1.15282]), y_pred = tensor([40.08362, 31.30023, 22.51685, 13.73346]), loss = 10.32425213\n",
      "gradient = tensor([ -17.41577, -134.98840])\n",
      "epoch 65: w = tensor([1.05460, 0.99321]), y_pred = tensor([46.51847, 36.20624, 25.89402, 15.58179]), loss = 7.78575230\n",
      "gradient = tensor([ -0.52992, 110.31209])\n",
      "epoch 70: w = tensor([1.11272, 1.11631]), y_pred = tensor([41.15016, 32.23169, 23.31323, 14.39476]), loss = 6.01640034\n",
      "gradient = tensor([-13.43386, -88.41534])\n",
      "epoch 75: w = tensor([1.15289, 1.01076]), y_pred = tensor([45.34982, 35.44982, 25.54981, 15.64980]), loss = 4.76234531\n",
      "gradient = tensor([-2.25095, 72.49085])\n",
      "epoch 80: w = tensor([1.20299, 1.09071]), y_pred = tensor([41.80791, 32.84242, 23.87693, 14.91144]), loss = 3.85587311\n",
      "gradient = tensor([-10.61534, -57.87884])\n",
      "epoch 85: w = tensor([1.24068, 1.02071]), y_pred = tensor([44.54358, 34.95336, 25.36314, 15.77292]), loss = 3.18603969\n",
      "gradient = tensor([-3.18430, 47.66795])\n",
      "epoch 90: w = tensor([1.28429, 1.07244]), y_pred = tensor([42.20184, 33.24281, 24.28377, 15.32473]), loss = 2.67915201\n",
      "gradient = tensor([ -8.58116, -37.85973])\n",
      "epoch 95: w = tensor([1.31920, 1.02583]), y_pred = tensor([43.97911, 34.62745, 25.27578, 15.92412]), loss = 2.28608990\n",
      "gradient = tensor([-3.62110, 31.37226])\n",
      "epoch 100: w = tensor([1.35745, 1.05912]), y_pred = tensor([42.42657, 33.50523, 24.58390, 15.66256]), loss = 1.97393024\n",
      "gradient = tensor([ -7.08051, -24.73838])\n",
      "epoch 105: w = tensor([1.38948, 1.02793]), y_pred = tensor([43.57692, 34.41344, 25.24997, 16.08649]), loss = 1.72041523\n",
      "gradient = tensor([-3.75015, 20.67223])\n",
      "epoch 110: w = tensor([1.42322, 1.04919]), y_pred = tensor([42.54377, 33.67719, 24.81061, 15.94404]), loss = 1.51038074\n",
      "gradient = tensor([ -5.94694, -16.14055])\n",
      "epoch 115: w = tensor([1.45243, 1.02818]), y_pred = tensor([43.28445, 34.27287, 25.26129, 16.24971]), loss = 1.33336711\n",
      "gradient = tensor([-3.69355, 13.64342])\n",
      "epoch 120: w = tensor([1.48233, 1.04161]), y_pred = tensor([42.59350, 33.78979, 24.98609, 16.18238]), loss = 1.18204284\n",
      "gradient = tensor([ -5.06955, -10.51022])\n",
      "epoch 125: w = tensor([1.50884, 1.02733]), y_pred = tensor([43.06691, 34.18049, 25.29408, 16.40766]), loss = 1.05118954\n",
      "gradient = tensor([-3.52962,  9.02462])\n",
      "epoch 130: w = tensor([1.53541, 1.03567]), y_pred = tensor([42.60181, 33.86351, 25.12522, 16.38693]), loss = 0.93700927\n",
      "gradient = tensor([-4.37389, -6.82430])\n",
      "epoch 135: w = tensor([1.55940, 1.02587]), y_pred = tensor([42.90114, 34.11973, 25.33832, 16.55691]), loss = 0.83667958\n",
      "gradient = tensor([-3.30841,  5.98641])\n",
      "epoch 140: w = tensor([1.58308, 1.03092]), y_pred = tensor([42.58543, 33.91173, 25.23804, 16.56435]), loss = 0.74805033\n",
      "gradient = tensor([-3.80980, -4.41338])\n",
      "epoch 145: w = tensor([1.60474, 1.02409]), y_pred = tensor([42.77172, 34.07974, 25.38776, 16.69577]), loss = 0.66943973\n",
      "gradient = tensor([-3.06122,  3.98693])\n",
      "epoch 150: w = tensor([1.62588, 1.02703]), y_pred = tensor([42.55507, 33.94322, 25.33137, 16.71952]), loss = 0.59950614\n",
      "gradient = tensor([-3.34315, -2.83900])\n",
      "epoch 155: w = tensor([1.64540, 1.02219]), y_pred = tensor([42.66827, 34.05339, 25.43850, 16.82361]), loss = 0.53715116\n",
      "gradient = tensor([-2.80750,  2.66936])\n",
      "epoch 160: w = tensor([1.66429, 1.02378]), y_pred = tensor([42.51758, 33.96376, 25.40994, 16.85612]), loss = 0.48146009\n",
      "gradient = tensor([-2.95031, -1.81206])\n",
      "epoch 165: w = tensor([1.68186, 1.02029]), y_pred = tensor([42.58377, 34.03600, 25.48822, 16.94045]), loss = 0.43166173\n",
      "gradient = tensor([-2.55889,  1.79972])\n",
      "epoch 170: w = tensor([1.69877, 1.02103]), y_pred = tensor([42.47721, 33.97713, 25.47704, 16.97695]), loss = 0.38709140\n",
      "gradient = tensor([-2.61480, -1.14371])\n",
      "epoch 175: w = tensor([1.71457, 1.01845]), y_pred = tensor([42.51338, 34.02448, 25.53557, 17.04667]), loss = 0.34717295\n",
      "gradient = tensor([-2.32213,  1.22391])\n",
      "epoch 180: w = tensor([1.72971, 1.01867]), y_pred = tensor([42.43659, 33.98581, 25.53503, 17.08425]), loss = 0.31140509\n",
      "gradient = tensor([-2.32486, -0.70952])\n",
      "epoch 185: w = tensor([1.74391, 1.01672]), y_pred = tensor([42.45380, 34.01683, 25.57986, 17.14289]), loss = 0.27934361\n",
      "gradient = tensor([-2.10069,  0.84160])\n",
      "epoch 190: w = tensor([1.75748, 1.01662]), y_pred = tensor([42.39725, 33.99142, 25.58560, 17.17977]), loss = 0.25059801\n",
      "gradient = tensor([-2.07201, -0.42871])\n",
      "epoch 195: w = tensor([1.77024, 1.01511]), y_pred = tensor([42.40269, 34.01173, 25.62078, 17.22982]), loss = 0.22481927\n",
      "gradient = tensor([-1.89610,  0.58674])\n",
      "epoch 200: w = tensor([1.78240, 1.01482]), y_pred = tensor([42.36006, 33.99504, 25.63001, 17.26499]), loss = 0.20169993\n",
      "gradient = tensor([-1.84993, -0.24814])\n",
      "epoch 205: w = tensor([1.79385, 1.01363]), y_pred = tensor([42.35838, 34.00832, 25.65827, 17.30822]), loss = 0.18096074\n",
      "gradient = tensor([-1.70864,  0.41626])\n",
      "epoch 210: w = tensor([1.80476, 1.01324]), y_pred = tensor([42.32542, 33.99732, 25.66922, 17.34113]), loss = 0.16235729\n",
      "gradient = tensor([-1.65388, -0.13396])\n",
      "epoch 215: w = tensor([1.81504, 1.01227]), y_pred = tensor([42.31963, 34.00603, 25.69242, 17.37882]), loss = 0.14566770\n",
      "gradient = tensor([-1.53788,  0.30142])\n",
      "epoch 220: w = tensor([1.82482, 1.01185]), y_pred = tensor([42.29353, 33.99876, 25.70400, 17.40924]), loss = 0.13069558\n",
      "gradient = tensor([-1.48000, -0.06169])\n",
      "epoch 225: w = tensor([1.83405, 1.01104]), y_pred = tensor([42.28552, 34.00446, 25.72339, 17.44233]), loss = 0.11726224\n",
      "gradient = tensor([-1.38304,  0.22278])\n",
      "epoch 230: w = tensor([1.84282, 1.01060]), y_pred = tensor([42.26440, 33.99967, 25.73494, 17.47021]), loss = 0.10521053\n",
      "gradient = tensor([-1.32529, -0.01637])\n",
      "epoch 235: w = tensor([1.85111, 1.00993]), y_pred = tensor([42.25534, 34.00337, 25.75140, 17.49942]), loss = 0.09439759\n",
      "gradient = tensor([-1.24302,  0.16842])\n",
      "epoch 240: w = tensor([1.85897, 1.00950]), y_pred = tensor([42.23791, 34.00021, 25.76252, 17.52483]), loss = 0.08469677\n",
      "gradient = tensor([-1.18740,  0.01068])\n",
      "epoch 245: w = tensor([1.86641, 1.00892]), y_pred = tensor([42.22856, 34.00261, 25.77667, 17.55073]), loss = 0.07599217\n",
      "gradient = tensor([-1.11665,  0.13068])\n",
      "epoch 250: w = tensor([1.87346, 1.00851]), y_pred = tensor([42.21390, 34.00052, 25.78715, 17.57377]), loss = 0.06818256\n",
      "gradient = tensor([-1.06427,  0.02612])\n",
      "epoch 255: w = tensor([1.88014, 1.00801]), y_pred = tensor([42.20472, 34.00208, 25.79945, 17.59681]), loss = 0.06117591\n",
      "gradient = tensor([-1.00278,  0.10420])\n",
      "epoch 260: w = tensor([1.88647, 1.00763]), y_pred = tensor([42.19220, 34.00068, 25.80916, 17.61764]), loss = 0.05488885\n",
      "gradient = tensor([-0.95419,  0.03394])\n",
      "epoch 265: w = tensor([1.89246, 1.00719]), y_pred = tensor([42.18345, 34.00169, 25.81994, 17.63818]), loss = 0.04924808\n",
      "gradient = tensor([-0.90031,  0.08475])\n",
      "epoch 270: w = tensor([1.89813, 1.00684]), y_pred = tensor([42.17264, 34.00076, 25.82887, 17.65699]), loss = 0.04418735\n",
      "gradient = tensor([-0.85564,  0.03786])\n",
      "epoch 275: w = tensor([1.90351, 1.00646]), y_pred = tensor([42.16444, 34.00140, 25.83836, 17.67532]), loss = 0.03964623\n",
      "gradient = tensor([-0.80819,  0.07013])\n",
      "epoch 280: w = tensor([1.90860, 1.00614]), y_pred = tensor([42.15502, 34.00077, 25.84652, 17.69227]), loss = 0.03557184\n",
      "gradient = tensor([-0.76740,  0.03853])\n",
      "epoch 285: w = tensor([1.91342, 1.00580]), y_pred = tensor([42.14744, 34.00118, 25.85492, 17.70866]), loss = 0.03191639\n",
      "gradient = tensor([-0.72539,  0.05912])\n",
      "epoch 290: w = tensor([1.91799, 1.00550]), y_pred = tensor([42.13918, 34.00076, 25.86233, 17.72391]), loss = 0.02863660\n",
      "gradient = tensor([-0.68833,  0.03774])\n",
      "epoch 295: w = tensor([1.92232, 1.00520]), y_pred = tensor([42.13222, 34.00101, 25.86980, 17.73858]), loss = 0.02569385\n",
      "gradient = tensor([-0.65102,  0.05041])\n",
      "epoch 300: w = tensor([1.92642, 1.00494]), y_pred = tensor([42.12493, 34.00072, 25.87651, 17.75230]), loss = 0.02305340\n",
      "gradient = tensor([-0.61746,  0.03589])\n",
      "epoch 305: w = tensor([1.93030, 1.00467]), y_pred = tensor([42.11858, 34.00087, 25.88315, 17.76544]), loss = 0.02068412\n",
      "gradient = tensor([-0.58424,  0.04342])\n",
      "epoch 310: w = tensor([1.93398, 1.00443]), y_pred = tensor([42.11213, 34.00067, 25.88922, 17.77776]), loss = 0.01855873\n",
      "gradient = tensor([-0.55391,  0.03366])\n",
      "epoch 315: w = tensor([1.93747, 1.00419]), y_pred = tensor([42.10637, 34.00076, 25.89515, 17.78953]), loss = 0.01665138\n",
      "gradient = tensor([-0.52427,  0.03789])\n",
      "epoch 320: w = tensor([1.94077, 1.00397]), y_pred = tensor([42.10064, 34.00063, 25.90062, 17.80061]), loss = 0.01494033\n",
      "gradient = tensor([-0.49691,  0.03132])\n",
      "epoch 325: w = tensor([1.94389, 1.00376]), y_pred = tensor([42.09541, 34.00066, 25.90591, 17.81116]), loss = 0.01340491\n",
      "gradient = tensor([-0.47046,  0.03308])\n",
      "epoch 330: w = tensor([1.94685, 1.00356]), y_pred = tensor([42.09031, 34.00057, 25.91084, 17.82110]), loss = 0.01202754\n",
      "gradient = tensor([-0.44581,  0.02867])\n",
      "epoch 335: w = tensor([1.94966, 1.00337]), y_pred = tensor([42.08558, 34.00058, 25.91557, 17.83055]), loss = 0.01079139\n",
      "gradient = tensor([-0.42217,  0.02872])\n",
      "epoch 340: w = tensor([1.95231, 1.00320]), y_pred = tensor([42.08104, 34.00053, 25.92001, 17.83949]), loss = 0.00968240\n",
      "gradient = tensor([-0.39996,  0.02622])\n",
      "epoch 345: w = tensor([1.95483, 1.00303]), y_pred = tensor([42.07679, 34.00052, 25.92424, 17.84797]), loss = 0.00868748\n",
      "gradient = tensor([-0.37878,  0.02594])\n",
      "epoch 350: w = tensor([1.95722, 1.00287]), y_pred = tensor([42.07271, 34.00047, 25.92823, 17.85599]), loss = 0.00779471\n",
      "gradient = tensor([-0.35886,  0.02347])\n",
      "epoch 355: w = tensor([1.95947, 1.00272]), y_pred = tensor([42.06890, 34.00047, 25.93203, 17.86359]), loss = 0.00699359\n",
      "gradient = tensor([-0.33986,  0.02316])\n",
      "epoch 360: w = tensor([1.96161, 1.00257]), y_pred = tensor([42.06524, 34.00042, 25.93560, 17.87078]), loss = 0.00627505\n",
      "gradient = tensor([-0.32198,  0.02117])\n",
      "epoch 365: w = tensor([1.96364, 1.00244]), y_pred = tensor([42.06182, 34.00042, 25.93901, 17.87761]), loss = 0.00563018\n",
      "gradient = tensor([-0.30494,  0.02073])\n",
      "epoch 370: w = tensor([1.96556, 1.00231]), y_pred = tensor([42.05854, 34.00038, 25.94222, 17.88406]), loss = 0.00505164\n",
      "gradient = tensor([-0.28889,  0.01907])\n",
      "epoch 375: w = tensor([1.96737, 1.00219]), y_pred = tensor([42.05547, 34.00037, 25.94528, 17.89019]), loss = 0.00453248\n",
      "gradient = tensor([-0.27360,  0.01865])\n",
      "epoch 380: w = tensor([1.96910, 1.00207]), y_pred = tensor([42.05252, 34.00034, 25.94816, 17.89598]), loss = 0.00406663\n",
      "gradient = tensor([-0.25919,  0.01716])\n",
      "epoch 385: w = tensor([1.97073, 1.00196]), y_pred = tensor([42.04977, 34.00034, 25.95090, 17.90147]), loss = 0.00364873\n",
      "gradient = tensor([-0.24547,  0.01686])\n",
      "epoch 390: w = tensor([1.97227, 1.00186]), y_pred = tensor([42.04712, 34.00031, 25.95349, 17.90667]), loss = 0.00327375\n",
      "gradient = tensor([-0.23257,  0.01525])\n",
      "epoch 395: w = tensor([1.97374, 1.00176]), y_pred = tensor([42.04465, 34.00030, 25.95595, 17.91160]), loss = 0.00293732\n",
      "gradient = tensor([-0.22025,  0.01501])\n",
      "epoch 400: w = tensor([1.97512, 1.00167]), y_pred = tensor([42.04229, 34.00028, 25.95827, 17.91626]), loss = 0.00263548\n",
      "gradient = tensor([-0.20865,  0.01399])\n",
      "epoch 405: w = tensor([1.97643, 1.00158]), y_pred = tensor([42.04005, 34.00026, 25.96047, 17.92068]), loss = 0.00236459\n",
      "gradient = tensor([-0.19764,  0.01316])\n",
      "epoch 410: w = tensor([1.97768, 1.00150]), y_pred = tensor([42.03794, 34.00025, 25.96256, 17.92487]), loss = 0.00212172\n",
      "gradient = tensor([-0.18721,  0.01259])\n",
      "epoch 415: w = tensor([1.97886, 1.00142]), y_pred = tensor([42.03594, 34.00024, 25.96453, 17.92883]), loss = 0.00190360\n",
      "gradient = tensor([-0.17732,  0.01196])\n",
      "epoch 420: w = tensor([1.97997, 1.00134]), y_pred = tensor([42.03405, 34.00023, 25.96641, 17.93259]), loss = 0.00170798\n",
      "gradient = tensor([-0.16796,  0.01138])\n",
      "epoch 425: w = tensor([1.98103, 1.00127]), y_pred = tensor([42.03225, 34.00022, 25.96818, 17.93615]), loss = 0.00153252\n",
      "gradient = tensor([-0.15909,  0.01087])\n",
      "epoch 430: w = tensor([1.98203, 1.00120]), y_pred = tensor([42.03054, 34.00020, 25.96986, 17.93951]), loss = 0.00137501\n",
      "gradient = tensor([-0.15071,  0.01001])\n",
      "epoch 435: w = tensor([1.98298, 1.00114]), y_pred = tensor([42.02893, 34.00019, 25.97145, 17.94271]), loss = 0.00123367\n",
      "gradient = tensor([-0.14276,  0.00950])\n",
      "epoch 440: w = tensor([1.98388, 1.00108]), y_pred = tensor([42.02741, 34.00019, 25.97296, 17.94573]), loss = 0.00110684\n",
      "gradient = tensor([-0.13520,  0.00934])\n",
      "epoch 445: w = tensor([1.98473, 1.00102]), y_pred = tensor([42.02594, 34.00016, 25.97437, 17.94859]), loss = 0.00099315\n",
      "gradient = tensor([-0.12813,  0.00793])\n",
      "epoch 450: w = tensor([1.98553, 1.00097]), y_pred = tensor([42.02460, 34.00017, 25.97574, 17.95131]), loss = 0.00089105\n",
      "gradient = tensor([-0.12130,  0.00848])\n",
      "epoch 455: w = tensor([1.98630, 1.00092]), y_pred = tensor([42.02329, 34.00015, 25.97701, 17.95388]), loss = 0.00079952\n",
      "gradient = tensor([-0.11493,  0.00759])\n",
      "epoch 460: w = tensor([1.98702, 1.00087]), y_pred = tensor([42.02205, 34.00014, 25.97823, 17.95631]), loss = 0.00071731\n",
      "gradient = tensor([-0.10887,  0.00698])\n",
      "epoch 465: w = tensor([1.98771, 1.00082]), y_pred = tensor([42.02092, 34.00015, 25.97939, 17.95862]), loss = 0.00064363\n",
      "gradient = tensor([-0.10306,  0.00759])\n",
      "epoch 470: w = tensor([1.98835, 1.00078]), y_pred = tensor([42.01978, 34.00011, 25.98046, 17.96080]), loss = 0.00057748\n",
      "gradient = tensor([-0.09772,  0.00578])\n",
      "epoch 475: w = tensor([1.98897, 1.00074]), y_pred = tensor([42.01876, 34.00014, 25.98150, 17.96288]), loss = 0.00051811\n",
      "gradient = tensor([-0.09247,  0.00677])\n",
      "epoch 480: w = tensor([1.98955, 1.00070]), y_pred = tensor([42.01775, 34.00011, 25.98247, 17.96483]), loss = 0.00046487\n",
      "gradient = tensor([-0.08766,  0.00547])\n",
      "epoch 485: w = tensor([1.99010, 1.00066]), y_pred = tensor([42.01683, 34.00012, 25.98340, 17.96669]), loss = 0.00041709\n",
      "gradient = tensor([-0.08298,  0.00595])\n",
      "epoch 490: w = tensor([1.99063, 1.00063]), y_pred = tensor([42.01592, 34.00009, 25.98427, 17.96844]), loss = 0.00037425\n",
      "gradient = tensor([-0.07866,  0.00471])\n",
      "epoch 495: w = tensor([1.99112, 1.00060]), y_pred = tensor([42.01510, 34.00010, 25.98511, 17.97011]), loss = 0.00033581\n",
      "gradient = tensor([-0.07447,  0.00512])\n",
      "epoch 500: w = tensor([1.99159, 1.00056]), y_pred = tensor([42.01431, 34.00010, 25.98589, 17.97169]), loss = 0.00030129\n",
      "gradient = tensor([-0.07053,  0.00497])\n",
      "epoch 505: w = tensor([1.99203, 1.00053]), y_pred = tensor([42.01354, 34.00008, 25.98663, 17.97318]), loss = 0.00027033\n",
      "gradient = tensor([-0.06684,  0.00425])\n",
      "epoch 510: w = tensor([1.99245, 1.00051]), y_pred = tensor([42.01283, 34.00009, 25.98734, 17.97460]), loss = 0.00024254\n",
      "gradient = tensor([-0.06329,  0.00439])\n",
      "epoch 515: w = tensor([1.99285, 1.00048]), y_pred = tensor([42.01215, 34.00007, 25.98800, 17.97593]), loss = 0.00021764\n",
      "gradient = tensor([-0.05998,  0.00373])\n",
      "epoch 520: w = tensor([1.99323, 1.00045]), y_pred = tensor([42.01151, 34.00008, 25.98864, 17.97721]), loss = 0.00019526\n",
      "gradient = tensor([-0.05680,  0.00376])\n",
      "epoch 525: w = tensor([1.99359, 1.00043]), y_pred = tensor([42.01090, 34.00007, 25.98924, 17.97841]), loss = 0.00017518\n",
      "gradient = tensor([-0.05380,  0.00354])\n",
      "epoch 530: w = tensor([1.99392, 1.00041]), y_pred = tensor([42.01033, 34.00007, 25.98981, 17.97955]), loss = 0.00015719\n",
      "gradient = tensor([-0.05095,  0.00349])\n",
      "epoch 535: w = tensor([1.99425, 1.00039]), y_pred = tensor([42.00978, 34.00006, 25.99035, 17.98063]), loss = 0.00014103\n",
      "gradient = tensor([-0.04827,  0.00313])\n",
      "epoch 540: w = tensor([1.99455, 1.00037]), y_pred = tensor([42.00927, 34.00006, 25.99086, 17.98165]), loss = 0.00012652\n",
      "gradient = tensor([-0.04570,  0.00323])\n",
      "epoch 545: w = tensor([1.99484, 1.00035]), y_pred = tensor([42.00878, 34.00006, 25.99134, 17.98262]), loss = 0.00011353\n",
      "gradient = tensor([-0.04331,  0.00290])\n",
      "epoch 550: w = tensor([1.99511, 1.00033]), y_pred = tensor([42.00831, 34.00005, 25.99179, 17.98354]), loss = 0.00010187\n",
      "gradient = tensor([-0.04103,  0.00267])\n",
      "epoch 555: w = tensor([1.99537, 1.00031]), y_pred = tensor([42.00787, 34.00005, 25.99223, 17.98441]), loss = 0.00009140\n",
      "gradient = tensor([-0.03886,  0.00257])\n",
      "epoch 560: w = tensor([1.99561, 1.00029]), y_pred = tensor([42.00747, 34.00006, 25.99264, 17.98523]), loss = 0.00008202\n",
      "gradient = tensor([-0.03678,  0.00282])\n",
      "epoch 565: w = tensor([1.99584, 1.00028]), y_pred = tensor([42.00705, 34.00004, 25.99302, 17.98600]), loss = 0.00007359\n",
      "gradient = tensor([-0.03490,  0.00187])\n",
      "epoch 570: w = tensor([1.99606, 1.00026]), y_pred = tensor([42.00670, 34.00005, 25.99340, 17.98675]), loss = 0.00006602\n",
      "gradient = tensor([-0.03301,  0.00237])\n",
      "epoch 575: w = tensor([1.99627, 1.00025]), y_pred = tensor([42.00633, 34.00003, 25.99374, 17.98744]), loss = 0.00005925\n",
      "gradient = tensor([-0.03131,  0.00177])\n",
      "epoch 580: w = tensor([1.99647, 1.00024]), y_pred = tensor([42.00601, 34.00005, 25.99408, 17.98811]), loss = 0.00005316\n",
      "gradient = tensor([-0.02962,  0.00223])\n",
      "epoch 585: w = tensor([1.99665, 1.00022]), y_pred = tensor([42.00569, 34.00003, 25.99439, 17.98874]), loss = 0.00004769\n",
      "gradient = tensor([-0.02807,  0.00181])\n",
      "epoch 590: w = tensor([1.99683, 1.00021]), y_pred = tensor([42.00538, 34.00003, 25.99468, 17.98933]), loss = 0.00004279\n",
      "gradient = tensor([-0.02660,  0.00154])\n",
      "epoch 595: w = tensor([1.99700, 1.00020]), y_pred = tensor([42.00511, 34.00004, 25.99497, 17.98989]), loss = 0.00003839\n",
      "gradient = tensor([-0.02516,  0.00196])\n",
      "epoch 600: w = tensor([1.99716, 1.00019]), y_pred = tensor([42.00483, 34.00003, 25.99523, 17.99043]), loss = 0.00003444\n",
      "gradient = tensor([-0.02386,  0.00145])\n",
      "epoch 605: w = tensor([1.99731, 1.00018]), y_pred = tensor([42.00458, 34.00003, 25.99548, 17.99093]), loss = 0.00003091\n",
      "gradient = tensor([-0.02259,  0.00156])\n",
      "epoch 610: w = tensor([1.99745, 1.00017]), y_pred = tensor([42.00435, 34.00003, 25.99572, 17.99141]), loss = 0.00002773\n",
      "gradient = tensor([-0.02138,  0.00179])\n",
      "epoch 615: w = tensor([1.99758, 1.00016]), y_pred = tensor([42.00410, 34.00002, 25.99594, 17.99186]), loss = 0.00002489\n",
      "gradient = tensor([-0.02030,  0.00092])\n",
      "epoch 620: w = tensor([1.99771, 1.00015]), y_pred = tensor([42.00390, 34.00003, 25.99616, 17.99229]), loss = 0.00002233\n",
      "gradient = tensor([-0.01918,  0.00168])\n",
      "epoch 625: w = tensor([1.99783, 1.00015]), y_pred = tensor([42.00368, 34.00002, 25.99636, 17.99270]), loss = 0.00002004\n",
      "gradient = tensor([-0.01821,  0.00098])\n",
      "epoch 630: w = tensor([1.99795, 1.00014]), y_pred = tensor([42.00349, 34.00002, 25.99655, 17.99308]), loss = 0.00001798\n",
      "gradient = tensor([-0.01723,  0.00118])\n",
      "epoch 635: w = tensor([1.99805, 1.00013]), y_pred = tensor([42.00330, 34.00002, 25.99673, 17.99345]), loss = 0.00001613\n",
      "gradient = tensor([-0.01633,  0.00100])\n",
      "epoch 640: w = tensor([1.99816, 1.00012]), y_pred = tensor([42.00313, 34.00002, 25.99691, 17.99379]), loss = 0.00001448\n",
      "gradient = tensor([-0.01547,  0.00095])\n",
      "epoch 645: w = tensor([1.99825, 1.00012]), y_pred = tensor([42.00298, 34.00003, 25.99708, 17.99413]), loss = 0.00001298\n",
      "gradient = tensor([-0.01461,  0.00145])\n",
      "epoch 650: w = tensor([1.99835, 1.00011]), y_pred = tensor([42.00280, 34.00001, 25.99722, 17.99443]), loss = 0.00001165\n",
      "gradient = tensor([-0.01390,  0.00051])\n",
      "epoch 655: w = tensor([1.99843, 1.00010]), y_pred = tensor([42.00267, 34.00002, 25.99738, 17.99473]), loss = 0.00001045\n",
      "gradient = tensor([-0.01312,  0.00114])\n",
      "epoch 660: w = tensor([1.99852, 1.00010]), y_pred = tensor([42.00251, 34.00001, 25.99751, 17.99500]), loss = 0.00000938\n",
      "gradient = tensor([-0.01247,  0.00051])\n",
      "epoch 665: w = tensor([1.99859, 1.00009]), y_pred = tensor([42.00239, 34.00002, 25.99764, 17.99527]), loss = 0.00000841\n",
      "gradient = tensor([-0.01178,  0.00090])\n",
      "epoch 670: w = tensor([1.99867, 1.00009]), y_pred = tensor([42.00225, 34.00001, 25.99776, 17.99552]), loss = 0.00000755\n",
      "gradient = tensor([-0.01119,  0.00039])\n",
      "epoch 675: w = tensor([1.99874, 1.00008]), y_pred = tensor([42.00216, 34.00003, 25.99789, 17.99576]), loss = 0.00000678\n",
      "gradient = tensor([-0.01054,  0.00137])\n",
      "epoch 680: w = tensor([1.99881, 1.00008]), y_pred = tensor([42.00201, 34.00000, 25.99799, 17.99597]), loss = 0.00000608\n",
      "gradient = tensor([-1.00603e-02,  8.58307e-05])\n",
      "epoch 685: w = tensor([1.99887, 1.00008]), y_pred = tensor([42.00194, 34.00002, 25.99811, 17.99619]), loss = 0.00000546\n",
      "gradient = tensor([-0.00946,  0.00114])\n",
      "epoch 690: w = tensor([1.99893, 1.00007]), y_pred = tensor([42.00181, 34.00000, 25.99820, 17.99639]), loss = 0.00000489\n",
      "gradient = tensor([-9.02748e-03,  5.72205e-05])\n",
      "epoch 695: w = tensor([1.99898, 1.00007]), y_pred = tensor([42.00173, 34.00002, 25.99830, 17.99658]), loss = 0.00000439\n",
      "gradient = tensor([-0.00851,  0.00069])\n",
      "epoch 700: w = tensor([1.99904, 1.00006]), y_pred = tensor([42.00164, 34.00002, 25.99839, 17.99677]), loss = 0.00000394\n",
      "gradient = tensor([-0.00805,  0.00076])\n",
      "epoch 705: w = tensor([1.99909, 1.00006]), y_pred = tensor([42.00154, 34.00001, 25.99847, 17.99693]), loss = 0.00000353\n",
      "gradient = tensor([-0.00765,  0.00038])\n",
      "epoch 710: w = tensor([1.99914, 1.00006]), y_pred = tensor([42.00146, 34.00000, 25.99855, 17.99709]), loss = 0.00000317\n",
      "gradient = tensor([-0.00726,  0.00015])\n",
      "epoch 715: w = tensor([1.99918, 1.00005]), y_pred = tensor([42.00139, 34.00002, 25.99863, 17.99725]), loss = 0.00000284\n",
      "gradient = tensor([-0.00684,  0.00063])\n",
      "epoch 720: w = tensor([1.99923, 1.00005]), y_pred = tensor([42.00131, 34.00000, 25.99870, 17.99739]), loss = 0.00000255\n",
      "gradient = tensor([-6.51836e-03,  9.53674e-05])\n",
      "epoch 725: w = tensor([1.99927, 1.00005]), y_pred = tensor([42.00126, 34.00002, 25.99878, 17.99753]), loss = 0.00000229\n",
      "gradient = tensor([-0.00612,  0.00083])\n",
      "epoch 730: w = tensor([1.99931, 1.00005]), y_pred = tensor([42.00117, 34.00000, 25.99883, 17.99766]), loss = 0.00000206\n",
      "gradient = tensor([-0.00586,  0.00000])\n",
      "epoch 735: w = tensor([1.99934, 1.00004]), y_pred = tensor([42.00113, 34.00002, 25.99890, 17.99779]), loss = 0.00000184\n",
      "gradient = tensor([-0.00549,  0.00076])\n",
      "epoch 740: w = tensor([1.99938, 1.00004]), y_pred = tensor([42.00105, 34.00000, 25.99895, 17.99790]), loss = 0.00000166\n",
      "gradient = tensor([-5.25570e-03, -3.81470e-05])\n",
      "epoch 745: w = tensor([1.99941, 1.00004]), y_pred = tensor([42.00101, 34.00001, 25.99901, 17.99801]), loss = 0.00000149\n",
      "gradient = tensor([-0.00494,  0.00058])\n",
      "epoch 750: w = tensor([1.99944, 1.00004]), y_pred = tensor([42.00096, 34.00001, 25.99907, 17.99812]), loss = 0.00000133\n",
      "gradient = tensor([-0.00467,  0.00057])\n",
      "epoch 755: w = tensor([1.99947, 1.00004]), y_pred = tensor([42.00090, 34.00000, 25.99911, 17.99822]), loss = 0.00000119\n",
      "gradient = tensor([-0.00445,  0.00010])\n",
      "epoch 760: w = tensor([1.99950, 1.00003]), y_pred = tensor([42.00086, 34.00001, 25.99916, 17.99831]), loss = 0.00000107\n",
      "gradient = tensor([-0.00419,  0.00046])\n",
      "epoch 765: w = tensor([1.99952, 1.00003]), y_pred = tensor([42.00081, 34.00000, 25.99920, 17.99840]), loss = 0.00000096\n",
      "gradient = tensor([-0.00398,  0.00027])\n",
      "epoch 770: w = tensor([1.99955, 1.00003]), y_pred = tensor([42.00076, 34.00000, 25.99924, 17.99848]), loss = 0.00000086\n",
      "gradient = tensor([-0.00379,  0.00010])\n",
      "epoch 775: w = tensor([1.99957, 1.00003]), y_pred = tensor([42.00073, 34.00001, 25.99929, 17.99857]), loss = 0.00000077\n",
      "gradient = tensor([-0.00357,  0.00038])\n",
      "epoch 780: w = tensor([1.99960, 1.00003]), y_pred = tensor([42.00068, 34.00000, 25.99932, 17.99864]), loss = 0.00000069\n",
      "gradient = tensor([-3.39699e-03,  6.67572e-05])\n",
      "epoch 785: w = tensor([1.99962, 1.00003]), y_pred = tensor([42.00065, 34.00001, 25.99936, 17.99871]), loss = 0.00000062\n",
      "gradient = tensor([-0.00320,  0.00031])\n",
      "epoch 790: w = tensor([1.99964, 1.00002]), y_pred = tensor([42.00062, 34.00001, 25.99940, 17.99878]), loss = 0.00000056\n",
      "gradient = tensor([-0.00303,  0.00041])\n",
      "epoch 795: w = tensor([1.99966, 1.00002]), y_pred = tensor([42.00059, 34.00001, 25.99943, 17.99885]), loss = 0.00000050\n",
      "gradient = tensor([-0.00287,  0.00035])\n",
      "epoch 800: w = tensor([1.99967, 1.00002]), y_pred = tensor([42.00055, 34.00000, 25.99945, 17.99891]), loss = 0.00000045\n",
      "gradient = tensor([-2.73609e-03,  3.81470e-05])\n",
      "epoch 805: w = tensor([1.99969, 1.00002]), y_pred = tensor([42.00053, 34.00001, 25.99949, 17.99896]), loss = 0.00000040\n",
      "gradient = tensor([-0.00257,  0.00031])\n",
      "epoch 810: w = tensor([1.99971, 1.00002]), y_pred = tensor([42.00049, 34.00000, 25.99951, 17.99902]), loss = 0.00000036\n",
      "gradient = tensor([-0.00246,  0.00000])\n",
      "epoch 815: w = tensor([1.99972, 1.00002]), y_pred = tensor([42.00047, 34.00000, 25.99954, 17.99907]), loss = 0.00000033\n",
      "gradient = tensor([-0.00232,  0.00015])\n",
      "epoch 820: w = tensor([1.99974, 1.00002]), y_pred = tensor([42.00045, 34.00000, 25.99956, 17.99912]), loss = 0.00000029\n",
      "gradient = tensor([-0.00219,  0.00019])\n",
      "epoch 825: w = tensor([1.99975, 1.00002]), y_pred = tensor([42.00042, 34.00000, 25.99958, 17.99916]), loss = 0.00000026\n",
      "gradient = tensor([-0.00208,  0.00011])\n",
      "epoch 830: w = tensor([1.99977, 1.00002]), y_pred = tensor([42.00040, 34.00000, 25.99960, 17.99921]), loss = 0.00000024\n",
      "gradient = tensor([-1.97983e-03,  9.53674e-06])\n",
      "epoch 835: w = tensor([1.99978, 1.00001]), y_pred = tensor([42.00038, 34.00000, 25.99963, 17.99925]), loss = 0.00000021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient = tensor([-0.00186,  0.00025])\n",
      "epoch 840: w = tensor([1.99979, 1.00001]), y_pred = tensor([42.00035, 34.00000, 25.99965, 17.99929]), loss = 0.00000019\n",
      "gradient = tensor([-0.00177,  0.00000])\n",
      "epoch 845: w = tensor([1.99980, 1.00001]), y_pred = tensor([42.00034, 34.00000, 25.99967, 17.99933]), loss = 0.00000017\n",
      "gradient = tensor([-0.00167,  0.00015])\n",
      "epoch 850: w = tensor([1.99981, 1.00001]), y_pred = tensor([42.00032, 34.00000, 25.99968, 17.99936]), loss = 0.00000015\n",
      "gradient = tensor([-0.00158,  0.00020])\n",
      "epoch 855: w = tensor([1.99982, 1.00001]), y_pred = tensor([42.00030, 34.00000, 25.99970, 17.99940]), loss = 0.00000014\n",
      "gradient = tensor([-0.00151,  0.00000])\n",
      "epoch 860: w = tensor([1.99983, 1.00001]), y_pred = tensor([42.00028, 33.99999, 25.99971, 17.99943]), loss = 0.00000012\n",
      "gradient = tensor([-0.00145, -0.00031])\n",
      "epoch 865: w = tensor([1.99984, 1.00001]), y_pred = tensor([42.00027, 34.00000, 25.99973, 17.99946]), loss = 0.00000011\n",
      "gradient = tensor([-0.00134,  0.00018])\n",
      "epoch 870: w = tensor([1.99985, 1.00001]), y_pred = tensor([42.00026, 34.00000, 25.99975, 17.99949]), loss = 0.00000010\n",
      "gradient = tensor([-0.00127,  0.00023])\n",
      "epoch 875: w = tensor([1.99986, 1.00001]), y_pred = tensor([42.00023, 33.99999, 25.99975, 17.99951]), loss = 0.00000009\n",
      "gradient = tensor([-0.00124, -0.00038])\n",
      "epoch 880: w = tensor([1.99986, 1.00001]), y_pred = tensor([42.00024, 34.00001, 25.99978, 17.99954]), loss = 0.00000008\n",
      "gradient = tensor([-0.00111,  0.00054])\n",
      "epoch 885: w = tensor([1.99987, 1.00001]), y_pred = tensor([42.00021, 33.99999, 25.99978, 17.99956]), loss = 0.00000007\n",
      "gradient = tensor([-0.00111, -0.00032])\n",
      "epoch 890: w = tensor([1.99988, 1.00001]), y_pred = tensor([42.00024, 34.00002, 25.99981, 17.99960]), loss = 0.00000006\n",
      "gradient = tensor([-0.00095,  0.00114])\n",
      "epoch 895: w = tensor([1.99988, 1.00001]), y_pred = tensor([42.00018, 33.99998, 25.99980, 17.99961]), loss = 0.00000006\n",
      "gradient = tensor([-0.00102, -0.00066])\n",
      "epoch 900: w = tensor([1.99989, 1.00001]), y_pred = tensor([42.00020, 34.00001, 25.99982, 17.99963]), loss = 0.00000005\n",
      "gradient = tensor([-0.00089,  0.00051])\n",
      "epoch 905: w = tensor([1.99990, 1.00001]), y_pred = tensor([42.00016, 33.99999, 25.99982, 17.99965]), loss = 0.00000005\n",
      "gradient = tensor([-0.00092, -0.00057])\n",
      "epoch 910: w = tensor([1.99990, 1.00001]), y_pred = tensor([42.00018, 34.00001, 25.99984, 17.99967]), loss = 0.00000004\n",
      "gradient = tensor([-0.00081,  0.00035])\n",
      "epoch 915: w = tensor([1.99991, 1.00001]), y_pred = tensor([42.00016, 34.00000, 25.99985, 17.99969]), loss = 0.00000004\n",
      "gradient = tensor([-0.00077,  0.00025])\n",
      "epoch 920: w = tensor([1.99991, 1.00001]), y_pred = tensor([42.00015, 34.00000, 25.99985, 17.99970]), loss = 0.00000003\n",
      "gradient = tensor([-7.50542e-04, -2.86102e-05])\n",
      "epoch 925: w = tensor([1.99992, 1.00001]), y_pred = tensor([42.00014, 34.00000, 25.99986, 17.99972]), loss = 0.00000003\n",
      "gradient = tensor([-0.00070,  0.00015])\n",
      "epoch 930: w = tensor([1.99992, 1.00001]), y_pred = tensor([42.00013, 33.99999, 25.99986, 17.99973]), loss = 0.00000003\n",
      "gradient = tensor([-0.00069, -0.00031])\n",
      "epoch 935: w = tensor([1.99992, 1.00000]), y_pred = tensor([42.00013, 34.00000, 25.99988, 17.99975]), loss = 0.00000002\n",
      "gradient = tensor([-0.00062,  0.00025])\n",
      "epoch 940: w = tensor([1.99993, 1.00001]), y_pred = tensor([42.00011, 33.99999, 25.99988, 17.99976]), loss = 0.00000002\n",
      "gradient = tensor([-0.00062, -0.00035])\n",
      "epoch 945: w = tensor([1.99993, 1.00000]), y_pred = tensor([42.00012, 34.00000, 25.99989, 17.99977]), loss = 0.00000002\n",
      "gradient = tensor([-0.00055,  0.00019])\n",
      "epoch 950: w = tensor([1.99994, 1.00000]), y_pred = tensor([42.00010, 34.00000, 25.99989, 17.99978]), loss = 0.00000002\n",
      "gradient = tensor([-0.00055, -0.00019])\n",
      "epoch 955: w = tensor([1.99994, 1.00000]), y_pred = tensor([42.00011, 34.00000, 25.99990, 17.99980]), loss = 0.00000002\n",
      "gradient = tensor([-0.00050,  0.00019])\n",
      "epoch 960: w = tensor([1.99994, 1.00000]), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99981]), loss = 0.00000001\n",
      "gradient = tensor([-4.83513e-04, -2.86102e-05])\n",
      "epoch 965: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00010, 34.00000, 25.99991, 17.99982]), loss = 0.00000001\n",
      "gradient = tensor([-4.52995e-04,  9.53674e-05])\n",
      "epoch 970: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00008, 33.99999, 25.99991, 17.99982]), loss = 0.00000001\n",
      "gradient = tensor([-0.00046, -0.00038])\n",
      "epoch 975: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00010, 34.00001, 25.99993, 17.99984]), loss = 0.00000001\n",
      "gradient = tensor([-0.00037,  0.00063])\n",
      "epoch 980: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00007, 34.00000, 25.99992, 17.99984]), loss = 0.00000001\n",
      "gradient = tensor([-0.00040, -0.00019])\n",
      "epoch 985: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00008, 34.00001, 25.99993, 17.99986]), loss = 0.00000001\n",
      "gradient = tensor([-0.00034,  0.00038])\n",
      "epoch 990: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00007, 34.00000, 25.99993, 17.99986]), loss = 0.00000001\n",
      "gradient = tensor([-3.47137e-04, -9.53674e-06])\n",
      "epoch 995: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00006, 34.00000, 25.99993, 17.99987]), loss = 0.00000001\n",
      "gradient = tensor([-0.00034, -0.00019])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988]), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# вариант #1:\n",
    "# learning_rate = 0.05\n",
    "# n_iters = 20 + 1\n",
    "\n",
    "# вариант #2:\n",
    "learning_rate = 0.0013\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "# основной цикл:\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Обучение модели нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad argument\n",
    "# This will tell pytorch that it will need to calculate the gradients for this tensor\n",
    "# later in your optimization steps\n",
    "# i.e. this is a variable in your model that you want to optimize\n",
    "x = torch.tensor([5.5, 3], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Проблема обучения модели нейронной сети__\n",
    "\n",
    "* <em class=\"nt\"></em> __основная проблема__ это не применение модели к входным данным $\\pmb{x}$ и оцнка ошибки на правильных ответах $\\pmb{y}$, а __обучение модели__ (опредление наилучших параметров модели $\\pmb{\\theta}$). \n",
    "     * В случае нейронной сети обучение сводится к поиску весов слоев сети $\\pmb{\\theta}=(\\pmb{w}_1, \\ldots, \\pmb{w}_L)$, которые в совокупности являются параметрами модели $\\pmb{\\theta}$.\n",
    "\n",
    "* Формально: цель обучения - найти оптимальное значение параметров $\\theta^{*}$, минимизирующих ошибку на обучающией выборке $D$: \n",
    "$$\\theta^{*} = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ E(\\pmb{\\theta}) = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "* Т.е. задача обучения сводится к задаче оптимизации.\n",
    "    * <em class=\"nt\"></em> На самом деле __все сложнее__: хороший результат на $D$ может плохо обобщаться (модель может давать низкое качество на другой выборке из той же генеральной совокупности) - __проблема переобучения__.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p1_v1.png\" alt=\"Приниципиальная логика обучения нейронной сети\" style=\"width: 800px;\"/><br/>\n",
    "    <b>Приниципиальная логика обучения нейронной сети</b>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямой проход и оценка ошибки__\n",
    "\n",
    "* __Прямой проход__ (forward pass): входящая информация (вектор $\\pmb{x}$) распространяется через сеть с учетом весов связей, расчитывается выходной вектор $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})= h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_12.png\" alt=\"пример прямого прохода\" style=\"width: 300px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример прямого прохода</b>    \n",
    "</center> \n",
    "\n",
    "* __Оценки ошибки__ $E(\\hat{\\pmb{y}}, \\pmb{y})$ на множестве правильных ответов: $\\pmb{y}$.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки в общей логике обучения нейронной сети</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача оптимизации__\n",
    "\n",
    "* Задача: корректировка весов сети (параметров модели $\\pmb{\\theta}$) на основе информации об ошибке на обучающих примерах $E(\\hat{\\pmb{y}}, \\pmb{y})$.\n",
    "    * Решение: использовать методы оптимизации, основанные на __методе градиентного спуска__.\n",
    "    \n",
    "\n",
    "* __Метод градиентныого спуска__ - метод нахождения локального экстремума (минимума или максимума) функции с помощью движения вдоль градиента. В нашем случае шаг метода градиентного спуска выглядит следующим образом:\n",
    "$$\\pmb{\\theta}_t = \\pmb{\\theta}_{t-1}-\\gamma\\nabla_\\theta E(\\pmb{\\theta}_{t-1}) = \\pmb{\\theta}_{t-1}-\\gamma \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} \\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "\n",
    "* <em class=\"nt\"></em> Выполнение на каждом шаге градиентого спуска суммирование по всем $(\\pmb{x}, \\pmb{y}) \\in D$ __обычно слшиком неэффективно__\n",
    "\n",
    "\n",
    "* Для выпуклых функций __задача локальной оптимизации__ - найти локальный минимум (максимум) автоматически превращается в __задачу глобальной оптимизации__ - найти точку, в которой достигается наименьшее (наибольшее) значение функции, то есть самую низкую (высокую) точку среди всех.\n",
    "* Оптимизировать веса одного перцептрона - выпуклая задача, но __для большой нейронной сети  целевая функция не является выпуклой__.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_15.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 500px;\"/><br/>\n",
    "    <b>Пример работы градиентного спуска для функции двух переменных</b>    \n",
    "</center>\n",
    "\n",
    "* У нейронных сетей функция ошибки может задавать __очень сложный ландшафт__ с огромным числом локальных максимумов и минимумов. Это свойство необходимо для обеспечения выразительности нейронных сетей, позволяющей им решать так много разных задач.\n",
    "\n",
    "\n",
    "* <em class=\"nt\"></em> для использования методов, основанных на методе градиентного спуска __необходимо знать градиент функции потерь по параметрам модели__: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$. Этот градиент определяет вектор (\"направление\") изменения параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Проблема поиска градиента__\n",
    "\n",
    "* <em class=\"qs\"></em> Проблема: как найти градиент для нейронной сети: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$?\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p3_v1.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Проблема поиска градиента в общей логике обучения нейронной сети</b>    \n",
    "</center> \n",
    "\n",
    "* Для решения этой задачи и используется __алгоритм обратного распространения ошибки__ (backpropagation). Суть алгоритма:\n",
    "    * рассчитывается ошибка между выходным вектором сети $\\hat{\\pmb{y}}$ и правильным ответом обучающего примера $\\pmb{y}$\n",
    "    * ошибка распростаняется от результата к источнику (в обратную сторону) для корректировки весов\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_13.png\" alt=\"Пример обратного распространения ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em>Пример обратного распространения ошибки</b>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Рассчет градиента суперпозиции двух функций нескольких переменных__\n",
    "\n",
    "* Сначала рассмотрим подзадачу: как рассчитать градиент для $f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L)$? \n",
    "\n",
    "* Для этого нам нужно будет __рассчитывать градиент суперпозиции (сложной функции)__ состоящей из последовательного применения функций слоев $h^i$.\n",
    "\n",
    "* Вспомним, как рассчитать производную (градиент) суперпозиции нескольких функций.\n",
    "    * Пусть $z=f(y)$, $y=g(x)$\n",
    "    * Тогда производная суперпозиции функций (правило дифференцирования сложной функции (chain rule)): $\\frac{\\mathrm{d} z}{\\mathrm{d} x}=\\frac{\\mathrm{d} z}{\\mathrm{d} y}\\frac{\\mathrm{d} y}{\\mathrm{d} x}$\n",
    "    * Если $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{y} \\in \\mathbb{R}^m$, а $\\mathbf{z} \\in \\mathbb{R}$, то: $\\frac{\\partial z }{\\partial x_i} = \\sum_j \\frac{\\partial z}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i}$\n",
    "\n",
    "<center> \n",
    "    \n",
    "__Примеры рассчета градиента суперпозиции двух функций нескольких переменных:__\n",
    "\n",
    "<img src=\"./img/ann_18.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "Т.е. нам нужны градиенты по всем возможным путям (рассмотренным в обработном порядке) завимиостей переменных.\n",
    "\n",
    "Запись этой же задачи в векторной нотации: \n",
    "* $\\frac{\\mathrm{d} z}{\\mathrm{d} \\mathbf{x}} = \\nabla_x (z)= \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial x_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial x_n} \\end{pmatrix}=\\left ( \\frac{\\mathrm{d} \\mathbf{y}}{\\mathrm{d} \\mathbf{x}} \\right )^T \\cdot \\nabla_y (z) = J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\nabla_y (z)= J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial y_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial y_m} \\end{pmatrix}$    \n",
    "* Где $J$ это Якобиан: $$J(\\mathbf{y}(\\mathbf{x})) = \\begin{pmatrix}\n",
    "    \\dfrac{\\partial y_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_1}{\\partial x_n}\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial y_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_m}{\\partial x_n} \\end{pmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача поиска градиента: $\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})$__\n",
    "\n",
    "* Перейдем от $f_L$ к последовательному рассчету функций слоев $h^i$:\n",
    "$$\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L ), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L), \\mathbf{y})$$\n",
    "\n",
    "* Обозначим через $\\mathbf{a}^l$ результат рассчета функции активации на слое $l$: $\\mathbf{a}^l=h^l(\\mathbf{x}_l,\\mathbf{w}_l)$. Тогда: $\\mathbf{x}_{l+1}=\\mathbf{a}_l$ (вход следущего слоя является результатом рассчета функции активации предыдущего слоя)\n",
    "\n",
    "* Тогда можно записать: $\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_\\theta E(\\mathbf{a}^L, \\mathbf{y})$. Функция потерь $E(\\mathbf{a}^L, \\mathbf{y})$ зависит от $\\mathbf{a}^L$, $\\mathbf{a}^L$ от $\\mathbf{a}^{L-1}$, ..., $\\mathbf{a}^{l+1}$ от $\\mathbf{a}^{l}$\n",
    "\n",
    "* Исходя из этого представления можно градиенты явесов $l$-го слоя можно записать как: \n",
    "$$\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\color{blue}{ \\dfrac{\\partial E}{\\partial \\mathbf{a}_L} \\cdot \\dfrac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\cdot \\cdots \\cdot \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}} \\cdot \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$$\n",
    "\n",
    "* Произведение всех сомножетелей кроме последнего является градиентом функции потерь по результатам рассчета функции активации слоя $l$:\n",
    "$$\\color{blue} {\\dfrac{\\partial E}{\\partial \\mathbf{a}_L} \\cdot \\dfrac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\cdot \\cdots \\cdot \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}} = \\color{blue} {\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$$\n",
    "\n",
    "* Тогда:\n",
    "$$\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\left ( \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$$ для рассчета $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ нам нужен только якобиан функции активации $l$-го слоя по параметрам слоя $\\mathbf{w}_{l}$. \n",
    "\n",
    "* Градиент функции потерь по результатам рассчета функции активации слоя  $l$ может быть рассчитан рекурсивно по результатам слоя $l$, собственно тут и происходит __обратное распространение__:\n",
    "$\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}=\\left ( \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}} \\right )^T \\cdot \\dfrac{\\partial E}{\\partial \\mathbf{a}_{l+1}}=\\left ( \\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{x}_{l+1}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l+1}}}$ для рассчета $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{x}_{l+1}}}$ нам нужен только якобиан функции активации $l+1$-го слоя по входным значениям слоя $\\mathbf{x}_{l+1}$\n",
    "\n",
    "* Т.е. чтобы проводить обратное распространение ошибки, нам на каждом слое (например $l$-м) нужно рассчитывать два якобиана:\n",
    "    * якобиан функции активации $l$-го слоя по параметрам слоя $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ - он позволит рассчитать градиент $\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}$ и сделать очередной шаг градиентного спуска для параметров этого слоя: $\\mathbf{w}_l^{t+1} = \\mathbf{w}_l^{t}-\\gamma\\nabla_{w_l} E(\\mathbf{w}^{t})=\\mathbf{w}_l^{t}-\\gamma \\dfrac{\\partial E(\\mathbf{w}^{t})}{\\partial \\mathbf{w}_l}$\n",
    "    * якобиан функции активации $l$-го слоя по входным значениям слоя: $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}}$ - он позволит распространить ошибку на низлежащие слои.\n",
    "\n",
    "Такми образом при обучении нам нужна только __очень локальная информация, содержащаяся в самом слое__. Т.е.:\n",
    "* __нет нобходимости знать как устроены сосоедние слои__: между слоями __очень простой интерфейс__ \n",
    "* т.е. __можно создать модульную архитектуру для слоев нейронной сети__: каждый модуль рассчитывает значение функции активации на основе выходов на прямом проходе и распространяет ошибку пришедшую на выходы на обратном проходе; все модули станадртным образом стыкуются друг с другом\n",
    "* при модульной архитектуре граф нейронной сети может быть очень сложным, но его __рассчет выполняется по одной простой и универсальной схеме__\n",
    "* внутри __модули могут быть сложно устроены, это никак не меняет логику остальных модулей и всего процесса обучения__, главное чтобы модуль корректно выполнял прямой и обратный проход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "##  Дифференцируемое программирование и реализация обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Почему Tensor *Flow*?__\n",
    "\n",
    "<em class=\"qs\"></em> Как реализовать __алгоритм обратного распространения ошибки__ удобно для использования в задачах моделирования ИНС?\n",
    "\n",
    "Основная абстракция TensorFlow, PyTorch и других аналогичных библиотеках - __граф потока вычислений__.\n",
    "\n",
    "* Рассматриваемые библиотеки обычно:\n",
    "    1. задают __граф потока вычислений__ (формирует объект отложенных вычислений)\n",
    "    2. запускают __процедуру выполненния отложенных вычислений__ и получает __результаты__ вычислений (в т.ч. ошибку модели).\n",
    "* Возможность в явном виде работать с графом потока вычислений дает большое приемущество для __автоматического решения задачи обратного распространения ошибки__, являющейся составляющей адачей обучения модели ИНС.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ker_6.png\" alt=\"Принцип устройства графа потока вычислений в TensorFlow\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Принцип устройства графа потока вычислений в TensorFlow</b>    \n",
    "</center>\n",
    "\n",
    "* Нейронная сеть это иерархия (она может быть простой и очень сложной) связанных (последовательно применяемых) функций слоев ИНС. Модель сети $f_L$ может быть представленна как суперпозиция из $L$ слоев $h^i\\text{, }i \\in \\{1, \\ldots, L\\}$, каждый из которых параметризуется своими весами $w_i$:\n",
    "$$f_L(\\pmb{x}, \\pmb{\\theta})=f_L(\\pmb{x}, \\pmb{w}_1, \\ldots, \\pmb{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$$\n",
    "\n",
    "* Вычисление функций слоев и взаимосвязи между слоями формируют граф потока вычислений в библиотеке моделирования ИНС.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_16.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Примеры иерархий в нейронных сетях</b>    \n",
    "</center>\n",
    "\n",
    "\n",
    "* По сути, ИНС это композиция модулей, представляющих собой слои нейронной сети:\n",
    "    * если сеть прямого распространения (feedforward), то все просто\n",
    "    * если сеть является направленным ациклическим графом, то существует правильный порядок применения функций\n",
    "    * в случае, если есть циклы, образующие рекуррентные связи, то существуют специальные подходы (будут рассмотрены позднее)\n",
    "\n",
    "\n",
    "* На обратном проходе (при обратном распространении ошибки) нам необходимо __дифференциировать сложную функцию__ многослойной ИНС\n",
    "$$\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L ), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L), \\mathbf{y})$$\n",
    "* алгоритм обратного распространения ошибки позволяет свести эту задачу к дифференциированию составляющих функций, но для этого необходимо __храниить информацию о виде и взаимосвязях функций задействованных в расчете модели ИНС__, именно эта информация и хранится в графе потока вычислений. Это позволяет организовать __автоматическое дифференциирование__ сложной функци многослойной ИНС."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Дифференциируемое программирование__\n",
    "\n",
    "<em class=\"df\"></em> __Дифференциируемое программирование__ (differentiable programming) - парадигма программирования при которой программа (функция рассчета значения) может быть продифференциирована в любой точке, обычно с помощью __автоматического диффиренциирования__. \n",
    "\n",
    "Это свойство позволяет использовать к программе __методы оптимизации основанные на рассчете градиента__, обычно - __методы градиентного спуска__.\n",
    "\n",
    "Дифференциируемое программирование используется в:\n",
    "* глубоком обучении\n",
    "* глубоком обучении комбинированном с физическими моделями в робототехнике\n",
    "* специализированных методах трассировки лучей\n",
    "* обработке изображений\n",
    "\n",
    "Большинство фреймоврков для дифференциируемого программирования использует граф потока вычислений определяющий выполнение программы и ее структуры данных.\n",
    "\n",
    "Основные классы фреймворков для дифференциируемого программирования:\n",
    "* __статические__ - они компилируют граф потока вычислений. Типичные представители: TensorFlow, Theano и др. Плюсы и минусы\n",
    "    * <em class=\"pl\"></em> могут использовать оптимизацию при компиляции\n",
    "    * <em class=\"pl\"></em> легче масштабирются на большие системы\n",
    "    * <em class=\"mn\"></em> статичность ограничевает интерактивность\n",
    "    * <em class=\"mn\"></em> многие программы не могут реализовываться легко (в частности: циклы, рекурсия)\n",
    "* __динамические__ - динамически исполняют граф потока вычислений. Используют перегрузку операторов для записи. Типичные представители: PyTorch, AutoGradrFlow. Плюсы и минусы:\n",
    "    * <em class=\"pl\"></em> более простая и понятная запись программы\n",
    "    * <em class=\"mn\"></em> накладные расходы интерпретатора\n",
    "    * <em class=\"mn\"></em> невозможно использовать оптимизацию компилятора\n",
    "    * <em class=\"mn\"></em> хуже масштабируемость\n",
    "* статическая на основе разбора промежуточного представления синтаксического разбора исходной программы. Пример фрэймоврк Zygote (язык программирования Julia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямой проход__:\n",
    "* Модули из графа обходятся один за одним начиная с узла входных данных и далее по мере готовности всех необходимых входных данных для очередного модуля, который еще не был обойден\n",
    "* Рассчет функций активации для каждого модуля по входным данным: $a_l=h_l(x_l, w_l)$\n",
    "* Промежуточные значения кэшируются, чтобы не рассчитывать их повторно (в сложном графе сети и при обратном проходе)\n",
    "* Выходы одних модулей становятся входами других модулей: $x_{l+1}=a_l$\n",
    "* Последним модулем рассчитывается сумма потерь для входных данных\n",
    "<center> \n",
    "    \n",
    "__Прямой и обратный проход процедуры обучения многослойной ИНС:__\n",
    "\n",
    "<img src=\"./img/ann_19.png\" alt=\"Прямой и обратный проход \" style=\"width: 300px;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "__Обратный проход__:\n",
    "* Сначала должен быть произведен прямой проход. На входе обратного прохода известна сумма потерь.\n",
    "* Строится обратный порядок обхода графа зависимостей модулей.\n",
    "* Модули из графа обходятся один за одним начиная с узла рассчета функции потерь и далее по мере готовности всех необходимых входных данных для очередного модуля, который еще не был обойден \n",
    "* Для каждого модуля рассчитыватся якобиан функции активации по параметрам слоя $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ и якобиан функции активации по входным значениям слоя: $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}}$ \n",
    "* По пришедшму в модуль градиенту ошибки (полученному из модулей использовавших результаты данного модуля на прямом проходе) $\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$ рассчитывается:\n",
    "    * Градиент для шага градиентного спуска по параметрам модуля $w_l$: $\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\left ( \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$\n",
    "    * Градиент ошбки, который передается в модули, поставившие данные в этот модуль во время прямого прохода: $\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l-1}}}=\\left ( \\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 1__\n",
    "\n",
    "<img src=\"./img/bp_2.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 2__\n",
    "\n",
    "<img src=\"./img/bp_3.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 3__\n",
    "\n",
    "<img src=\"./img/bp_4.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 1__\n",
    "\n",
    "<img src=\"./img/bp_5.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 2__\n",
    "\n",
    "<img src=\"./img/bp_6.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Производные популярных функций активации__\n",
    "\n",
    "<img src=\"./img/ann_17.png\" alt=\"Пример\" style=\"width: 500px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 3__\n",
    "\n",
    "<img src=\"./img/bp_7.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоматическое дифференциирование в PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.21713,  1.01658, -0.62985], requires_grad=True)\n",
      "tensor([2.21713, 3.01658, 1.37015], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x00000238229A9408>\n"
     ]
    }
   ],
   "source": [
    "# The autograd package provides automatic differentiation \n",
    "# for all operations on Tensors\n",
    "\n",
    "# requires_grad = True -> tracks all operations on the tensor. \n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14.74698, 27.29934,  5.63190], grad_fn=<MulBackward0>)\n",
      "tensor(15.89274, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.43426, 6.03317, 2.74029])\n"
     ]
    }
   ],
   "source": [
    "# Let's compute the gradients with backpropagation\n",
    "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "# The gradient for this tensor will be accumulated into .grad attribute.\n",
    "# It is the partial derivate of the function w.r.t. the tensor\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) # dz/dx\n",
    "\n",
    "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "# It computes partial derivates while applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__Примеры рассчета градиента суперпозиции двух функций нескольких переменных:__\n",
    "\n",
    "<img src=\"./img/ann_18.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "Т.е. нам нужны градиенты по всем возможным путям (рассмотренным в обработном порядке) завимиостей переменных.\n",
    "\n",
    "Запись этой же задачи в векторной нотации: \n",
    "* $\\frac{\\mathrm{d} z}{\\mathrm{d} \\mathbf{x}} = \\nabla_x (z)= \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial x_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial x_n} \\end{pmatrix}=\\left ( \\frac{\\mathrm{d} \\mathbf{y}}{\\mathrm{d} \\mathbf{x}} \\right )^T \\cdot \\nabla_y (z) = J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\nabla_y (z)= J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial y_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial y_m} \\end{pmatrix}$    \n",
    "* Где $J$ это Якобиан: $$J(\\mathbf{y}(\\mathbf{x})) = \\begin{pmatrix}\n",
    "    \\dfrac{\\partial y_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_1}{\\partial x_n}\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial y_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_m}{\\partial x_n} \\end{pmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1292.58887, -2687.01343,   244.47513], grad_fn=<MulBackward0>)\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Model with non-scalar output:\n",
    "# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \n",
    "# specify a gradient argument that is a tensor of matching shape.\n",
    "# needed for vector-Jacobian product\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "for _ in range(10):\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.04800e+02, 2.04800e+03, 2.04800e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Stop a tensor from tracking history:\n",
    "For example during our training loop when we want to update our weights\n",
    "then this update operation should not be part of the gradient computation\n",
    "* `x.requires_grad_(False)`\n",
    "* `x.detach()`\n",
    "* wrap in `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad = False\n",
      "b.grad_fn = None\n",
      "a.requires_grad = True\n",
      "b.grad_fn = <SumBackward0 object at 0x00000238229CC988>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "\n",
    "a = torch.randn(2, 2)\n",
    "print(f'a.requires_grad = {a.requires_grad}')\n",
    "\n",
    "b = ((a * 3) / (a - 1))\n",
    "print(f'b.grad_fn = {b.grad_fn}')\n",
    "      \n",
    "a.requires_grad_(True)\n",
    "print(f'a.requires_grad = {a.requires_grad}')\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(f'b.grad_fn = {b.grad_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = a.detach()\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!!\n",
    "# Use .zero_() to empty the gradients before a new optimization step!\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    # 'forward pass'\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "\n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)\n",
    "print(model_output)\n",
    "\n",
    "# Optimizer has zero_grad() method\n",
    "# optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "# During training:\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Автоматическое выполнение обратного прохода с помощью `l.backward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "epoch 0: w = tensor([0.16900, 2.21000], requires_grad=True), y_pred = tensor([0., 0., 0., 0.], grad_fn=<MvBackward>), loss = 980.00000000\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 5: w = tensor([0.13813, 0.24422], requires_grad=True), y_pred = tensor([81.70274, 61.57523, 41.44772, 21.32021], grad_fn=<MvBackward>), loss = 646.58898926\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 10: w = tensor([0.33966, 1.82453], requires_grad=True), y_pred = tensor([15.22920, 11.70164,  8.17407,  4.64651], grad_fn=<MvBackward>), loss = 427.49307251\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 15: w = tensor([0.34364, 0.53338], requires_grad=True), y_pred = tensor([68.78386, 52.09385, 35.40385, 18.71384], grad_fn=<MvBackward>), loss = 283.42614746\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 20: w = tensor([0.49880, 1.56850], requires_grad=True), y_pred = tensor([25.13912, 19.37721, 13.61531,  7.85340], grad_fn=<MvBackward>), loss = 188.61228943\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 25: w = tensor([0.52316, 0.72007], requires_grad=True), y_pred = tensor([60.23326, 45.87370, 31.51413, 17.15457], grad_fn=<MvBackward>), loss = 126.13926697\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 30: w = tensor([0.64554, 1.39771], requires_grad=True), y_pred = tensor([31.56792, 24.41182, 17.25571, 10.09960], grad_fn=<MvBackward>), loss = 84.91012573\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 35: w = tensor([0.68103, 0.83984], requires_grad=True), y_pred = tensor([54.55604, 41.79293, 29.02982, 16.26671], grad_fn=<MvBackward>), loss = 57.64197159\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 40: w = tensor([0.77979, 1.28313], requires_grad=True), y_pred = tensor([35.72054, 27.71401, 19.70748, 11.70095], grad_fn=<MvBackward>), loss = 39.55477905\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 45: w = tensor([0.82057, 0.91600], requires_grad=True), y_pred = tensor([50.77066, 39.11558, 27.46050, 15.80541], grad_fn=<MvBackward>), loss = 27.51074600\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 50: w = tensor([0.90194, 1.20568], requires_grad=True), y_pred = tensor([38.38666, 29.87982, 21.37299, 12.86615], grad_fn=<MvBackward>), loss = 19.44943237\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 55: w = tensor([0.94440, 0.96380], requires_grad=True), y_pred = tensor([48.23264, 37.35892, 26.48521, 15.61149], grad_fn=<MvBackward>), loss = 14.01713943\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 60: w = tensor([1.01265, 1.15282], requires_grad=True), y_pred = tensor([40.08362, 31.30023, 22.51685, 13.73346], grad_fn=<MvBackward>), loss = 10.32425213\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 65: w = tensor([1.05460, 0.99321], requires_grad=True), y_pred = tensor([46.51847, 36.20624, 25.89402, 15.58179], grad_fn=<MvBackward>), loss = 7.78575325\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 70: w = tensor([1.11272, 1.11631], requires_grad=True), y_pred = tensor([41.15015, 32.23169, 23.31322, 14.39476], grad_fn=<MvBackward>), loss = 6.01642036\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 75: w = tensor([1.15289, 1.01076], requires_grad=True), y_pred = tensor([45.34982, 35.44982, 25.54981, 15.64980], grad_fn=<MvBackward>), loss = 4.76234579\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 80: w = tensor([1.20299, 1.09071], requires_grad=True), y_pred = tensor([41.80791, 32.84242, 23.87693, 14.91144], grad_fn=<MvBackward>), loss = 3.85587597\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 85: w = tensor([1.24068, 1.02071], requires_grad=True), y_pred = tensor([44.54358, 34.95336, 25.36314, 15.77292], grad_fn=<MvBackward>), loss = 3.18603969\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 90: w = tensor([1.28429, 1.07244], requires_grad=True), y_pred = tensor([42.20184, 33.24281, 24.28377, 15.32473], grad_fn=<MvBackward>), loss = 2.67915201\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 95: w = tensor([1.31920, 1.02583], requires_grad=True), y_pred = tensor([43.97913, 34.62745, 25.27579, 15.92412], grad_fn=<MvBackward>), loss = 2.28610182\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 100: w = tensor([1.35745, 1.05912], requires_grad=True), y_pred = tensor([42.42656, 33.50523, 24.58389, 15.66256], grad_fn=<MvBackward>), loss = 1.97393394\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 105: w = tensor([1.38948, 1.02793], requires_grad=True), y_pred = tensor([43.57692, 34.41344, 25.24997, 16.08649], grad_fn=<MvBackward>), loss = 1.72041774\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 110: w = tensor([1.42322, 1.04919], requires_grad=True), y_pred = tensor([42.54377, 33.67719, 24.81061, 15.94403], grad_fn=<MvBackward>), loss = 1.51038170\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 115: w = tensor([1.45243, 1.02818], requires_grad=True), y_pred = tensor([43.28445, 34.27287, 25.26129, 16.24971], grad_fn=<MvBackward>), loss = 1.33336782\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 120: w = tensor([1.48233, 1.04161], requires_grad=True), y_pred = tensor([42.59350, 33.78979, 24.98609, 16.18238], grad_fn=<MvBackward>), loss = 1.18204451\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 125: w = tensor([1.50884, 1.02733], requires_grad=True), y_pred = tensor([43.06691, 34.18049, 25.29407, 16.40766], grad_fn=<MvBackward>), loss = 1.05119181\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 130: w = tensor([1.53541, 1.03567], requires_grad=True), y_pred = tensor([42.60181, 33.86351, 25.12522, 16.38693], grad_fn=<MvBackward>), loss = 0.93700927\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 135: w = tensor([1.55940, 1.02587], requires_grad=True), y_pred = tensor([42.90114, 34.11973, 25.33832, 16.55691], grad_fn=<MvBackward>), loss = 0.83668095\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 140: w = tensor([1.58308, 1.03092], requires_grad=True), y_pred = tensor([42.58541, 33.91172, 25.23803, 16.56434], grad_fn=<MvBackward>), loss = 0.74805164\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 145: w = tensor([1.60474, 1.02409], requires_grad=True), y_pred = tensor([42.77173, 34.07975, 25.38776, 16.69578], grad_fn=<MvBackward>), loss = 0.66944206\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 150: w = tensor([1.62588, 1.02703], requires_grad=True), y_pred = tensor([42.55506, 33.94321, 25.33137, 16.71952], grad_fn=<MvBackward>), loss = 0.59950674\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 155: w = tensor([1.64540, 1.02219], requires_grad=True), y_pred = tensor([42.66828, 34.05339, 25.43850, 16.82362], grad_fn=<MvBackward>), loss = 0.53715217\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 160: w = tensor([1.66429, 1.02378], requires_grad=True), y_pred = tensor([42.51758, 33.96376, 25.40994, 16.85612], grad_fn=<MvBackward>), loss = 0.48146015\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 165: w = tensor([1.68186, 1.02029], requires_grad=True), y_pred = tensor([42.58377, 34.03600, 25.48822, 16.94045], grad_fn=<MvBackward>), loss = 0.43166173\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 170: w = tensor([1.69877, 1.02103], requires_grad=True), y_pred = tensor([42.47721, 33.97713, 25.47704, 16.97695], grad_fn=<MvBackward>), loss = 0.38709140\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 175: w = tensor([1.71457, 1.01845], requires_grad=True), y_pred = tensor([42.51338, 34.02448, 25.53557, 17.04667], grad_fn=<MvBackward>), loss = 0.34717295\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 180: w = tensor([1.72971, 1.01867], requires_grad=True), y_pred = tensor([42.43659, 33.98581, 25.53503, 17.08424], grad_fn=<MvBackward>), loss = 0.31140596\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 185: w = tensor([1.74391, 1.01672], requires_grad=True), y_pred = tensor([42.45380, 34.01683, 25.57986, 17.14289], grad_fn=<MvBackward>), loss = 0.27934441\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 190: w = tensor([1.75748, 1.01662], requires_grad=True), y_pred = tensor([42.39725, 33.99142, 25.58560, 17.17977], grad_fn=<MvBackward>), loss = 0.25059801\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 195: w = tensor([1.77024, 1.01511], requires_grad=True), y_pred = tensor([42.40269, 34.01173, 25.62078, 17.22982], grad_fn=<MvBackward>), loss = 0.22481927\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 200: w = tensor([1.78240, 1.01482], requires_grad=True), y_pred = tensor([42.36006, 33.99504, 25.63001, 17.26499], grad_fn=<MvBackward>), loss = 0.20169993\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 205: w = tensor([1.79385, 1.01363], requires_grad=True), y_pred = tensor([42.35838, 34.00832, 25.65827, 17.30822], grad_fn=<MvBackward>), loss = 0.18096074\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 210: w = tensor([1.80476, 1.01324], requires_grad=True), y_pred = tensor([42.32542, 33.99732, 25.66922, 17.34113], grad_fn=<MvBackward>), loss = 0.16235729\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 215: w = tensor([1.81504, 1.01227], requires_grad=True), y_pred = tensor([42.31963, 34.00603, 25.69242, 17.37882], grad_fn=<MvBackward>), loss = 0.14566770\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 220: w = tensor([1.82482, 1.01185], requires_grad=True), y_pred = tensor([42.29353, 33.99876, 25.70400, 17.40924], grad_fn=<MvBackward>), loss = 0.13069558\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 225: w = tensor([1.83405, 1.01104], requires_grad=True), y_pred = tensor([42.28552, 34.00446, 25.72339, 17.44233], grad_fn=<MvBackward>), loss = 0.11726224\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 230: w = tensor([1.84282, 1.01060], requires_grad=True), y_pred = tensor([42.26440, 33.99967, 25.73494, 17.47021], grad_fn=<MvBackward>), loss = 0.10521053\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 235: w = tensor([1.85111, 1.00993], requires_grad=True), y_pred = tensor([42.25534, 34.00337, 25.75140, 17.49942], grad_fn=<MvBackward>), loss = 0.09439759\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 240: w = tensor([1.85897, 1.00950], requires_grad=True), y_pred = tensor([42.23791, 34.00021, 25.76252, 17.52483], grad_fn=<MvBackward>), loss = 0.08469677\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 245: w = tensor([1.86641, 1.00892], requires_grad=True), y_pred = tensor([42.22856, 34.00261, 25.77667, 17.55073], grad_fn=<MvBackward>), loss = 0.07599217\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 250: w = tensor([1.87346, 1.00851], requires_grad=True), y_pred = tensor([42.21390, 34.00052, 25.78715, 17.57377], grad_fn=<MvBackward>), loss = 0.06818256\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 255: w = tensor([1.88014, 1.00801], requires_grad=True), y_pred = tensor([42.20472, 34.00208, 25.79945, 17.59681], grad_fn=<MvBackward>), loss = 0.06117591\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 260: w = tensor([1.88647, 1.00763], requires_grad=True), y_pred = tensor([42.19220, 34.00068, 25.80916, 17.61764], grad_fn=<MvBackward>), loss = 0.05488885\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 265: w = tensor([1.89246, 1.00719], requires_grad=True), y_pred = tensor([42.18345, 34.00169, 25.81994, 17.63818], grad_fn=<MvBackward>), loss = 0.04924808\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 270: w = tensor([1.89813, 1.00684], requires_grad=True), y_pred = tensor([42.17264, 34.00076, 25.82887, 17.65699], grad_fn=<MvBackward>), loss = 0.04418735\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 275: w = tensor([1.90351, 1.00646], requires_grad=True), y_pred = tensor([42.16444, 34.00140, 25.83836, 17.67532], grad_fn=<MvBackward>), loss = 0.03964623\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 280: w = tensor([1.90860, 1.00614], requires_grad=True), y_pred = tensor([42.15502, 34.00077, 25.84652, 17.69227], grad_fn=<MvBackward>), loss = 0.03557184\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 285: w = tensor([1.91342, 1.00580], requires_grad=True), y_pred = tensor([42.14744, 34.00118, 25.85492, 17.70866], grad_fn=<MvBackward>), loss = 0.03191639\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 290: w = tensor([1.91799, 1.00550], requires_grad=True), y_pred = tensor([42.13918, 34.00076, 25.86233, 17.72391], grad_fn=<MvBackward>), loss = 0.02863660\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 295: w = tensor([1.92232, 1.00520], requires_grad=True), y_pred = tensor([42.13222, 34.00101, 25.86980, 17.73858], grad_fn=<MvBackward>), loss = 0.02569385\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 300: w = tensor([1.92642, 1.00494], requires_grad=True), y_pred = tensor([42.12493, 34.00072, 25.87651, 17.75230], grad_fn=<MvBackward>), loss = 0.02305340\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 305: w = tensor([1.93030, 1.00467], requires_grad=True), y_pred = tensor([42.11858, 34.00087, 25.88315, 17.76544], grad_fn=<MvBackward>), loss = 0.02068412\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 310: w = tensor([1.93398, 1.00443], requires_grad=True), y_pred = tensor([42.11213, 34.00067, 25.88922, 17.77776], grad_fn=<MvBackward>), loss = 0.01855873\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 315: w = tensor([1.93747, 1.00419], requires_grad=True), y_pred = tensor([42.10637, 34.00076, 25.89515, 17.78953], grad_fn=<MvBackward>), loss = 0.01665138\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 320: w = tensor([1.94077, 1.00397], requires_grad=True), y_pred = tensor([42.10064, 34.00063, 25.90062, 17.80061], grad_fn=<MvBackward>), loss = 0.01494033\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 325: w = tensor([1.94389, 1.00376], requires_grad=True), y_pred = tensor([42.09541, 34.00066, 25.90591, 17.81116], grad_fn=<MvBackward>), loss = 0.01340491\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 330: w = tensor([1.94685, 1.00356], requires_grad=True), y_pred = tensor([42.09031, 34.00057, 25.91084, 17.82110], grad_fn=<MvBackward>), loss = 0.01202754\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 335: w = tensor([1.94966, 1.00337], requires_grad=True), y_pred = tensor([42.08558, 34.00058, 25.91557, 17.83055], grad_fn=<MvBackward>), loss = 0.01079139\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 340: w = tensor([1.95231, 1.00320], requires_grad=True), y_pred = tensor([42.08104, 34.00053, 25.92001, 17.83949], grad_fn=<MvBackward>), loss = 0.00968240\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 345: w = tensor([1.95483, 1.00303], requires_grad=True), y_pred = tensor([42.07679, 34.00052, 25.92424, 17.84797], grad_fn=<MvBackward>), loss = 0.00868748\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 350: w = tensor([1.95722, 1.00287], requires_grad=True), y_pred = tensor([42.07271, 34.00047, 25.92823, 17.85599], grad_fn=<MvBackward>), loss = 0.00779471\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 355: w = tensor([1.95947, 1.00272], requires_grad=True), y_pred = tensor([42.06890, 34.00047, 25.93203, 17.86359], grad_fn=<MvBackward>), loss = 0.00699359\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 360: w = tensor([1.96161, 1.00257], requires_grad=True), y_pred = tensor([42.06524, 34.00042, 25.93560, 17.87078], grad_fn=<MvBackward>), loss = 0.00627505\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 365: w = tensor([1.96364, 1.00244], requires_grad=True), y_pred = tensor([42.06182, 34.00042, 25.93901, 17.87761], grad_fn=<MvBackward>), loss = 0.00563018\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 370: w = tensor([1.96556, 1.00231], requires_grad=True), y_pred = tensor([42.05854, 34.00038, 25.94222, 17.88406], grad_fn=<MvBackward>), loss = 0.00505164\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 375: w = tensor([1.96737, 1.00219], requires_grad=True), y_pred = tensor([42.05547, 34.00037, 25.94528, 17.89019], grad_fn=<MvBackward>), loss = 0.00453248\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 380: w = tensor([1.96910, 1.00207], requires_grad=True), y_pred = tensor([42.05252, 34.00034, 25.94816, 17.89598], grad_fn=<MvBackward>), loss = 0.00406663\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 385: w = tensor([1.97073, 1.00196], requires_grad=True), y_pred = tensor([42.04977, 34.00034, 25.95090, 17.90147], grad_fn=<MvBackward>), loss = 0.00364873\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 390: w = tensor([1.97227, 1.00186], requires_grad=True), y_pred = tensor([42.04712, 34.00031, 25.95349, 17.90667], grad_fn=<MvBackward>), loss = 0.00327375\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 395: w = tensor([1.97374, 1.00176], requires_grad=True), y_pred = tensor([42.04465, 34.00030, 25.95595, 17.91160], grad_fn=<MvBackward>), loss = 0.00293732\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 400: w = tensor([1.97512, 1.00167], requires_grad=True), y_pred = tensor([42.04229, 34.00028, 25.95827, 17.91626], grad_fn=<MvBackward>), loss = 0.00263548\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 405: w = tensor([1.97643, 1.00158], requires_grad=True), y_pred = tensor([42.04005, 34.00026, 25.96047, 17.92068], grad_fn=<MvBackward>), loss = 0.00236459\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 410: w = tensor([1.97768, 1.00150], requires_grad=True), y_pred = tensor([42.03794, 34.00025, 25.96256, 17.92487], grad_fn=<MvBackward>), loss = 0.00212172\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 415: w = tensor([1.97886, 1.00142], requires_grad=True), y_pred = tensor([42.03594, 34.00024, 25.96453, 17.92883], grad_fn=<MvBackward>), loss = 0.00190360\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 420: w = tensor([1.97997, 1.00134], requires_grad=True), y_pred = tensor([42.03405, 34.00023, 25.96641, 17.93259], grad_fn=<MvBackward>), loss = 0.00170798\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 425: w = tensor([1.98103, 1.00127], requires_grad=True), y_pred = tensor([42.03225, 34.00022, 25.96818, 17.93615], grad_fn=<MvBackward>), loss = 0.00153252\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 430: w = tensor([1.98203, 1.00120], requires_grad=True), y_pred = tensor([42.03054, 34.00020, 25.96986, 17.93951], grad_fn=<MvBackward>), loss = 0.00137501\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 435: w = tensor([1.98298, 1.00114], requires_grad=True), y_pred = tensor([42.02893, 34.00019, 25.97145, 17.94271], grad_fn=<MvBackward>), loss = 0.00123367\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 440: w = tensor([1.98388, 1.00108], requires_grad=True), y_pred = tensor([42.02741, 34.00019, 25.97296, 17.94573], grad_fn=<MvBackward>), loss = 0.00110684\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 445: w = tensor([1.98473, 1.00102], requires_grad=True), y_pred = tensor([42.02594, 34.00016, 25.97437, 17.94859], grad_fn=<MvBackward>), loss = 0.00099315\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 450: w = tensor([1.98553, 1.00097], requires_grad=True), y_pred = tensor([42.02460, 34.00017, 25.97574, 17.95131], grad_fn=<MvBackward>), loss = 0.00089105\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 455: w = tensor([1.98630, 1.00092], requires_grad=True), y_pred = tensor([42.02329, 34.00015, 25.97701, 17.95388], grad_fn=<MvBackward>), loss = 0.00079952\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 460: w = tensor([1.98702, 1.00087], requires_grad=True), y_pred = tensor([42.02205, 34.00014, 25.97823, 17.95631], grad_fn=<MvBackward>), loss = 0.00071731\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 465: w = tensor([1.98771, 1.00082], requires_grad=True), y_pred = tensor([42.02092, 34.00015, 25.97939, 17.95862], grad_fn=<MvBackward>), loss = 0.00064363\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 470: w = tensor([1.98835, 1.00078], requires_grad=True), y_pred = tensor([42.01978, 34.00011, 25.98046, 17.96080], grad_fn=<MvBackward>), loss = 0.00057748\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 475: w = tensor([1.98897, 1.00074], requires_grad=True), y_pred = tensor([42.01876, 34.00014, 25.98150, 17.96288], grad_fn=<MvBackward>), loss = 0.00051811\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 480: w = tensor([1.98955, 1.00070], requires_grad=True), y_pred = tensor([42.01775, 34.00011, 25.98247, 17.96483], grad_fn=<MvBackward>), loss = 0.00046487\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 485: w = tensor([1.99010, 1.00066], requires_grad=True), y_pred = tensor([42.01683, 34.00012, 25.98340, 17.96669], grad_fn=<MvBackward>), loss = 0.00041709\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 490: w = tensor([1.99063, 1.00063], requires_grad=True), y_pred = tensor([42.01592, 34.00009, 25.98427, 17.96844], grad_fn=<MvBackward>), loss = 0.00037425\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 495: w = tensor([1.99112, 1.00060], requires_grad=True), y_pred = tensor([42.01510, 34.00010, 25.98511, 17.97011], grad_fn=<MvBackward>), loss = 0.00033581\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 500: w = tensor([1.99159, 1.00056], requires_grad=True), y_pred = tensor([42.01431, 34.00010, 25.98589, 17.97169], grad_fn=<MvBackward>), loss = 0.00030129\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 505: w = tensor([1.99203, 1.00053], requires_grad=True), y_pred = tensor([42.01354, 34.00008, 25.98663, 17.97318], grad_fn=<MvBackward>), loss = 0.00027033\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 510: w = tensor([1.99245, 1.00051], requires_grad=True), y_pred = tensor([42.01283, 34.00009, 25.98734, 17.97460], grad_fn=<MvBackward>), loss = 0.00024254\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 515: w = tensor([1.99285, 1.00048], requires_grad=True), y_pred = tensor([42.01215, 34.00007, 25.98800, 17.97593], grad_fn=<MvBackward>), loss = 0.00021764\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 520: w = tensor([1.99323, 1.00045], requires_grad=True), y_pred = tensor([42.01151, 34.00008, 25.98864, 17.97721], grad_fn=<MvBackward>), loss = 0.00019526\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 525: w = tensor([1.99359, 1.00043], requires_grad=True), y_pred = tensor([42.01090, 34.00007, 25.98924, 17.97841], grad_fn=<MvBackward>), loss = 0.00017518\n",
      "gradient = tensor([-0.00029,  0.00038])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 530: w = tensor([1.99392, 1.00041], requires_grad=True), y_pred = tensor([42.01033, 34.00007, 25.98981, 17.97955], grad_fn=<MvBackward>), loss = 0.00015719\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 535: w = tensor([1.99425, 1.00039], requires_grad=True), y_pred = tensor([42.00978, 34.00006, 25.99035, 17.98063], grad_fn=<MvBackward>), loss = 0.00014103\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 540: w = tensor([1.99455, 1.00037], requires_grad=True), y_pred = tensor([42.00927, 34.00006, 25.99086, 17.98165], grad_fn=<MvBackward>), loss = 0.00012652\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 545: w = tensor([1.99484, 1.00035], requires_grad=True), y_pred = tensor([42.00878, 34.00006, 25.99134, 17.98262], grad_fn=<MvBackward>), loss = 0.00011353\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 550: w = tensor([1.99511, 1.00033], requires_grad=True), y_pred = tensor([42.00831, 34.00005, 25.99179, 17.98354], grad_fn=<MvBackward>), loss = 0.00010187\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 555: w = tensor([1.99537, 1.00031], requires_grad=True), y_pred = tensor([42.00787, 34.00005, 25.99223, 17.98441], grad_fn=<MvBackward>), loss = 0.00009140\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 560: w = tensor([1.99561, 1.00029], requires_grad=True), y_pred = tensor([42.00747, 34.00006, 25.99264, 17.98523], grad_fn=<MvBackward>), loss = 0.00008202\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 565: w = tensor([1.99584, 1.00028], requires_grad=True), y_pred = tensor([42.00705, 34.00004, 25.99302, 17.98600], grad_fn=<MvBackward>), loss = 0.00007359\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 570: w = tensor([1.99606, 1.00026], requires_grad=True), y_pred = tensor([42.00670, 34.00005, 25.99340, 17.98675], grad_fn=<MvBackward>), loss = 0.00006602\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 575: w = tensor([1.99627, 1.00025], requires_grad=True), y_pred = tensor([42.00633, 34.00003, 25.99374, 17.98744], grad_fn=<MvBackward>), loss = 0.00005925\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 580: w = tensor([1.99647, 1.00024], requires_grad=True), y_pred = tensor([42.00601, 34.00005, 25.99408, 17.98811], grad_fn=<MvBackward>), loss = 0.00005316\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 585: w = tensor([1.99665, 1.00022], requires_grad=True), y_pred = tensor([42.00569, 34.00003, 25.99439, 17.98874], grad_fn=<MvBackward>), loss = 0.00004769\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 590: w = tensor([1.99683, 1.00021], requires_grad=True), y_pred = tensor([42.00538, 34.00003, 25.99468, 17.98933], grad_fn=<MvBackward>), loss = 0.00004279\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 595: w = tensor([1.99700, 1.00020], requires_grad=True), y_pred = tensor([42.00511, 34.00004, 25.99497, 17.98989], grad_fn=<MvBackward>), loss = 0.00003839\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 600: w = tensor([1.99716, 1.00019], requires_grad=True), y_pred = tensor([42.00483, 34.00003, 25.99523, 17.99043], grad_fn=<MvBackward>), loss = 0.00003444\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 605: w = tensor([1.99731, 1.00018], requires_grad=True), y_pred = tensor([42.00458, 34.00003, 25.99548, 17.99093], grad_fn=<MvBackward>), loss = 0.00003091\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 610: w = tensor([1.99745, 1.00017], requires_grad=True), y_pred = tensor([42.00435, 34.00003, 25.99572, 17.99141], grad_fn=<MvBackward>), loss = 0.00002773\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 615: w = tensor([1.99758, 1.00016], requires_grad=True), y_pred = tensor([42.00410, 34.00002, 25.99594, 17.99186], grad_fn=<MvBackward>), loss = 0.00002489\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 620: w = tensor([1.99771, 1.00015], requires_grad=True), y_pred = tensor([42.00390, 34.00003, 25.99616, 17.99229], grad_fn=<MvBackward>), loss = 0.00002233\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 625: w = tensor([1.99783, 1.00015], requires_grad=True), y_pred = tensor([42.00368, 34.00002, 25.99636, 17.99270], grad_fn=<MvBackward>), loss = 0.00002004\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 630: w = tensor([1.99795, 1.00014], requires_grad=True), y_pred = tensor([42.00349, 34.00002, 25.99655, 17.99308], grad_fn=<MvBackward>), loss = 0.00001798\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 635: w = tensor([1.99805, 1.00013], requires_grad=True), y_pred = tensor([42.00330, 34.00002, 25.99673, 17.99345], grad_fn=<MvBackward>), loss = 0.00001613\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 640: w = tensor([1.99816, 1.00012], requires_grad=True), y_pred = tensor([42.00313, 34.00002, 25.99691, 17.99379], grad_fn=<MvBackward>), loss = 0.00001448\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 645: w = tensor([1.99825, 1.00012], requires_grad=True), y_pred = tensor([42.00298, 34.00003, 25.99708, 17.99413], grad_fn=<MvBackward>), loss = 0.00001298\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 650: w = tensor([1.99835, 1.00011], requires_grad=True), y_pred = tensor([42.00280, 34.00001, 25.99722, 17.99443], grad_fn=<MvBackward>), loss = 0.00001165\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 655: w = tensor([1.99843, 1.00010], requires_grad=True), y_pred = tensor([42.00267, 34.00002, 25.99738, 17.99473], grad_fn=<MvBackward>), loss = 0.00001045\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 660: w = tensor([1.99852, 1.00010], requires_grad=True), y_pred = tensor([42.00251, 34.00001, 25.99751, 17.99500], grad_fn=<MvBackward>), loss = 0.00000938\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 665: w = tensor([1.99859, 1.00009], requires_grad=True), y_pred = tensor([42.00239, 34.00002, 25.99764, 17.99527], grad_fn=<MvBackward>), loss = 0.00000841\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 670: w = tensor([1.99867, 1.00009], requires_grad=True), y_pred = tensor([42.00225, 34.00001, 25.99776, 17.99552], grad_fn=<MvBackward>), loss = 0.00000755\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 675: w = tensor([1.99874, 1.00008], requires_grad=True), y_pred = tensor([42.00216, 34.00003, 25.99789, 17.99576], grad_fn=<MvBackward>), loss = 0.00000678\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 680: w = tensor([1.99881, 1.00008], requires_grad=True), y_pred = tensor([42.00201, 34.00000, 25.99799, 17.99597], grad_fn=<MvBackward>), loss = 0.00000608\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 685: w = tensor([1.99887, 1.00008], requires_grad=True), y_pred = tensor([42.00194, 34.00002, 25.99811, 17.99619], grad_fn=<MvBackward>), loss = 0.00000546\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 690: w = tensor([1.99893, 1.00007], requires_grad=True), y_pred = tensor([42.00181, 34.00000, 25.99820, 17.99639], grad_fn=<MvBackward>), loss = 0.00000489\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 695: w = tensor([1.99898, 1.00007], requires_grad=True), y_pred = tensor([42.00173, 34.00002, 25.99830, 17.99658], grad_fn=<MvBackward>), loss = 0.00000439\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 700: w = tensor([1.99904, 1.00006], requires_grad=True), y_pred = tensor([42.00164, 34.00002, 25.99839, 17.99677], grad_fn=<MvBackward>), loss = 0.00000394\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 705: w = tensor([1.99909, 1.00006], requires_grad=True), y_pred = tensor([42.00154, 34.00001, 25.99847, 17.99693], grad_fn=<MvBackward>), loss = 0.00000353\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 710: w = tensor([1.99914, 1.00006], requires_grad=True), y_pred = tensor([42.00146, 34.00000, 25.99855, 17.99709], grad_fn=<MvBackward>), loss = 0.00000317\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 715: w = tensor([1.99918, 1.00005], requires_grad=True), y_pred = tensor([42.00139, 34.00002, 25.99863, 17.99725], grad_fn=<MvBackward>), loss = 0.00000284\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 720: w = tensor([1.99923, 1.00005], requires_grad=True), y_pred = tensor([42.00131, 34.00000, 25.99870, 17.99739], grad_fn=<MvBackward>), loss = 0.00000255\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 725: w = tensor([1.99927, 1.00005], requires_grad=True), y_pred = tensor([42.00126, 34.00002, 25.99878, 17.99753], grad_fn=<MvBackward>), loss = 0.00000229\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 730: w = tensor([1.99931, 1.00005], requires_grad=True), y_pred = tensor([42.00117, 34.00000, 25.99883, 17.99766], grad_fn=<MvBackward>), loss = 0.00000206\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 735: w = tensor([1.99934, 1.00004], requires_grad=True), y_pred = tensor([42.00113, 34.00002, 25.99890, 17.99779], grad_fn=<MvBackward>), loss = 0.00000184\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 740: w = tensor([1.99938, 1.00004], requires_grad=True), y_pred = tensor([42.00105, 34.00000, 25.99895, 17.99790], grad_fn=<MvBackward>), loss = 0.00000166\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 745: w = tensor([1.99941, 1.00004], requires_grad=True), y_pred = tensor([42.00101, 34.00001, 25.99901, 17.99801], grad_fn=<MvBackward>), loss = 0.00000149\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 750: w = tensor([1.99944, 1.00004], requires_grad=True), y_pred = tensor([42.00096, 34.00001, 25.99907, 17.99812], grad_fn=<MvBackward>), loss = 0.00000133\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 755: w = tensor([1.99947, 1.00004], requires_grad=True), y_pred = tensor([42.00090, 34.00000, 25.99911, 17.99822], grad_fn=<MvBackward>), loss = 0.00000119\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 760: w = tensor([1.99950, 1.00003], requires_grad=True), y_pred = tensor([42.00086, 34.00001, 25.99916, 17.99831], grad_fn=<MvBackward>), loss = 0.00000107\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 765: w = tensor([1.99952, 1.00003], requires_grad=True), y_pred = tensor([42.00081, 34.00000, 25.99920, 17.99840], grad_fn=<MvBackward>), loss = 0.00000096\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 770: w = tensor([1.99955, 1.00003], requires_grad=True), y_pred = tensor([42.00076, 34.00000, 25.99924, 17.99848], grad_fn=<MvBackward>), loss = 0.00000086\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 775: w = tensor([1.99957, 1.00003], requires_grad=True), y_pred = tensor([42.00073, 34.00001, 25.99929, 17.99857], grad_fn=<MvBackward>), loss = 0.00000077\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 780: w = tensor([1.99960, 1.00003], requires_grad=True), y_pred = tensor([42.00068, 34.00000, 25.99932, 17.99864], grad_fn=<MvBackward>), loss = 0.00000069\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 785: w = tensor([1.99962, 1.00003], requires_grad=True), y_pred = tensor([42.00065, 34.00001, 25.99936, 17.99871], grad_fn=<MvBackward>), loss = 0.00000062\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 790: w = tensor([1.99964, 1.00002], requires_grad=True), y_pred = tensor([42.00062, 34.00001, 25.99940, 17.99878], grad_fn=<MvBackward>), loss = 0.00000056\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 795: w = tensor([1.99966, 1.00002], requires_grad=True), y_pred = tensor([42.00059, 34.00001, 25.99943, 17.99885], grad_fn=<MvBackward>), loss = 0.00000050\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 800: w = tensor([1.99967, 1.00002], requires_grad=True), y_pred = tensor([42.00055, 34.00000, 25.99945, 17.99891], grad_fn=<MvBackward>), loss = 0.00000045\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 805: w = tensor([1.99969, 1.00002], requires_grad=True), y_pred = tensor([42.00053, 34.00001, 25.99949, 17.99896], grad_fn=<MvBackward>), loss = 0.00000040\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 810: w = tensor([1.99971, 1.00002], requires_grad=True), y_pred = tensor([42.00049, 34.00000, 25.99951, 17.99902], grad_fn=<MvBackward>), loss = 0.00000036\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 815: w = tensor([1.99972, 1.00002], requires_grad=True), y_pred = tensor([42.00047, 34.00000, 25.99954, 17.99907], grad_fn=<MvBackward>), loss = 0.00000033\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 820: w = tensor([1.99974, 1.00002], requires_grad=True), y_pred = tensor([42.00045, 34.00000, 25.99956, 17.99912], grad_fn=<MvBackward>), loss = 0.00000029\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 825: w = tensor([1.99975, 1.00002], requires_grad=True), y_pred = tensor([42.00042, 34.00000, 25.99958, 17.99916], grad_fn=<MvBackward>), loss = 0.00000026\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 830: w = tensor([1.99977, 1.00002], requires_grad=True), y_pred = tensor([42.00040, 34.00000, 25.99960, 17.99921], grad_fn=<MvBackward>), loss = 0.00000024\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 835: w = tensor([1.99978, 1.00001], requires_grad=True), y_pred = tensor([42.00038, 34.00000, 25.99963, 17.99925], grad_fn=<MvBackward>), loss = 0.00000021\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 840: w = tensor([1.99979, 1.00001], requires_grad=True), y_pred = tensor([42.00035, 34.00000, 25.99965, 17.99929], grad_fn=<MvBackward>), loss = 0.00000019\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 845: w = tensor([1.99980, 1.00001], requires_grad=True), y_pred = tensor([42.00034, 34.00000, 25.99967, 17.99933], grad_fn=<MvBackward>), loss = 0.00000017\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 850: w = tensor([1.99981, 1.00001], requires_grad=True), y_pred = tensor([42.00032, 34.00000, 25.99968, 17.99936], grad_fn=<MvBackward>), loss = 0.00000015\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 855: w = tensor([1.99982, 1.00001], requires_grad=True), y_pred = tensor([42.00030, 34.00000, 25.99970, 17.99940], grad_fn=<MvBackward>), loss = 0.00000014\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 860: w = tensor([1.99983, 1.00001], requires_grad=True), y_pred = tensor([42.00028, 33.99999, 25.99971, 17.99943], grad_fn=<MvBackward>), loss = 0.00000012\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 865: w = tensor([1.99984, 1.00001], requires_grad=True), y_pred = tensor([42.00027, 34.00000, 25.99973, 17.99946], grad_fn=<MvBackward>), loss = 0.00000011\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 870: w = tensor([1.99985, 1.00001], requires_grad=True), y_pred = tensor([42.00026, 34.00000, 25.99975, 17.99949], grad_fn=<MvBackward>), loss = 0.00000010\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 875: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00023, 33.99999, 25.99975, 17.99951], grad_fn=<MvBackward>), loss = 0.00000009\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 880: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00001, 25.99978, 17.99954], grad_fn=<MvBackward>), loss = 0.00000008\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 885: w = tensor([1.99987, 1.00001], requires_grad=True), y_pred = tensor([42.00021, 33.99999, 25.99978, 17.99956], grad_fn=<MvBackward>), loss = 0.00000007\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 890: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00002, 25.99981, 17.99960], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 895: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 33.99998, 25.99980, 17.99961], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 900: w = tensor([1.99989, 1.00001], requires_grad=True), y_pred = tensor([42.00020, 34.00001, 25.99982, 17.99963], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 905: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 33.99999, 25.99982, 17.99965], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 910: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 34.00001, 25.99984, 17.99967], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 915: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 34.00000, 25.99985, 17.99969], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 920: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00015, 34.00000, 25.99985, 17.99970], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 925: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00014, 34.00000, 25.99986, 17.99972], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 930: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00013, 33.99999, 25.99986, 17.99973], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 935: w = tensor([1.99992, 1.00000], requires_grad=True), y_pred = tensor([42.00013, 34.00000, 25.99988, 17.99975], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 940: w = tensor([1.99993, 1.00001], requires_grad=True), y_pred = tensor([42.00011, 33.99999, 25.99988, 17.99976], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 945: w = tensor([1.99993, 1.00000], requires_grad=True), y_pred = tensor([42.00012, 34.00000, 25.99989, 17.99977], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 950: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99989, 17.99978], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 955: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00011, 34.00000, 25.99990, 17.99980], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 960: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99981], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 965: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 970: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 33.99999, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 975: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00001, 25.99993, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 980: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99992, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 985: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 34.00001, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 990: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 995: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00006, 34.00000, 25.99993, 17.99987], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n"
     ]
    }
   ],
   "source": [
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "#!!! requires_grad=True\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.0013\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "# основной цикл:\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    #!!! backward pass        \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Использование встроенного оптимизатора `optimizer = torch.optim.SGD([w], lr=learning_rate)` и функции потерь `loss = nn.MSELoss()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "epoch 0: w = tensor([0.16900, 2.21000], requires_grad=True), y_pred = tensor([0., 0., 0., 0.], grad_fn=<MvBackward>), loss = 980.00000000\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 5: w = tensor([0.13813, 0.24422], requires_grad=True), y_pred = tensor([81.70274, 61.57523, 41.44772, 21.32021], grad_fn=<MvBackward>), loss = 646.58898926\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 10: w = tensor([0.33966, 1.82453], requires_grad=True), y_pred = tensor([15.22920, 11.70164,  8.17407,  4.64651], grad_fn=<MvBackward>), loss = 427.49307251\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 15: w = tensor([0.34364, 0.53338], requires_grad=True), y_pred = tensor([68.78386, 52.09385, 35.40385, 18.71384], grad_fn=<MvBackward>), loss = 283.42614746\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 20: w = tensor([0.49880, 1.56850], requires_grad=True), y_pred = tensor([25.13912, 19.37721, 13.61531,  7.85340], grad_fn=<MvBackward>), loss = 188.61228943\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 25: w = tensor([0.52316, 0.72007], requires_grad=True), y_pred = tensor([60.23326, 45.87370, 31.51413, 17.15457], grad_fn=<MvBackward>), loss = 126.13926697\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 30: w = tensor([0.64554, 1.39771], requires_grad=True), y_pred = tensor([31.56792, 24.41182, 17.25571, 10.09960], grad_fn=<MvBackward>), loss = 84.91012573\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 35: w = tensor([0.68103, 0.83984], requires_grad=True), y_pred = tensor([54.55604, 41.79293, 29.02982, 16.26671], grad_fn=<MvBackward>), loss = 57.64197159\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 40: w = tensor([0.77979, 1.28313], requires_grad=True), y_pred = tensor([35.72054, 27.71401, 19.70748, 11.70095], grad_fn=<MvBackward>), loss = 39.55477905\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 45: w = tensor([0.82057, 0.91600], requires_grad=True), y_pred = tensor([50.77066, 39.11558, 27.46050, 15.80541], grad_fn=<MvBackward>), loss = 27.51074600\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 50: w = tensor([0.90194, 1.20568], requires_grad=True), y_pred = tensor([38.38666, 29.87982, 21.37299, 12.86615], grad_fn=<MvBackward>), loss = 19.44943237\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 55: w = tensor([0.94440, 0.96380], requires_grad=True), y_pred = tensor([48.23264, 37.35892, 26.48521, 15.61149], grad_fn=<MvBackward>), loss = 14.01713943\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 60: w = tensor([1.01265, 1.15282], requires_grad=True), y_pred = tensor([40.08362, 31.30023, 22.51685, 13.73346], grad_fn=<MvBackward>), loss = 10.32425213\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 65: w = tensor([1.05460, 0.99321], requires_grad=True), y_pred = tensor([46.51847, 36.20624, 25.89402, 15.58179], grad_fn=<MvBackward>), loss = 7.78575325\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 70: w = tensor([1.11272, 1.11631], requires_grad=True), y_pred = tensor([41.15015, 32.23169, 23.31322, 14.39476], grad_fn=<MvBackward>), loss = 6.01642036\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 75: w = tensor([1.15289, 1.01076], requires_grad=True), y_pred = tensor([45.34982, 35.44982, 25.54981, 15.64980], grad_fn=<MvBackward>), loss = 4.76234579\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 80: w = tensor([1.20299, 1.09071], requires_grad=True), y_pred = tensor([41.80791, 32.84242, 23.87693, 14.91144], grad_fn=<MvBackward>), loss = 3.85587597\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 85: w = tensor([1.24068, 1.02071], requires_grad=True), y_pred = tensor([44.54358, 34.95336, 25.36314, 15.77292], grad_fn=<MvBackward>), loss = 3.18603969\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 90: w = tensor([1.28429, 1.07244], requires_grad=True), y_pred = tensor([42.20184, 33.24281, 24.28377, 15.32473], grad_fn=<MvBackward>), loss = 2.67915201\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 95: w = tensor([1.31920, 1.02583], requires_grad=True), y_pred = tensor([43.97913, 34.62745, 25.27579, 15.92412], grad_fn=<MvBackward>), loss = 2.28610182\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 100: w = tensor([1.35745, 1.05912], requires_grad=True), y_pred = tensor([42.42656, 33.50523, 24.58389, 15.66256], grad_fn=<MvBackward>), loss = 1.97393394\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 105: w = tensor([1.38948, 1.02793], requires_grad=True), y_pred = tensor([43.57692, 34.41344, 25.24997, 16.08649], grad_fn=<MvBackward>), loss = 1.72041774\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 110: w = tensor([1.42322, 1.04919], requires_grad=True), y_pred = tensor([42.54377, 33.67719, 24.81061, 15.94403], grad_fn=<MvBackward>), loss = 1.51038170\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 115: w = tensor([1.45243, 1.02818], requires_grad=True), y_pred = tensor([43.28445, 34.27287, 25.26129, 16.24971], grad_fn=<MvBackward>), loss = 1.33336782\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 120: w = tensor([1.48233, 1.04161], requires_grad=True), y_pred = tensor([42.59350, 33.78979, 24.98609, 16.18238], grad_fn=<MvBackward>), loss = 1.18204451\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 125: w = tensor([1.50884, 1.02733], requires_grad=True), y_pred = tensor([43.06691, 34.18049, 25.29407, 16.40766], grad_fn=<MvBackward>), loss = 1.05119181\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 130: w = tensor([1.53541, 1.03567], requires_grad=True), y_pred = tensor([42.60181, 33.86351, 25.12522, 16.38693], grad_fn=<MvBackward>), loss = 0.93700927\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 135: w = tensor([1.55940, 1.02587], requires_grad=True), y_pred = tensor([42.90114, 34.11973, 25.33832, 16.55691], grad_fn=<MvBackward>), loss = 0.83668095\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 140: w = tensor([1.58308, 1.03092], requires_grad=True), y_pred = tensor([42.58541, 33.91172, 25.23803, 16.56434], grad_fn=<MvBackward>), loss = 0.74805164\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 145: w = tensor([1.60474, 1.02409], requires_grad=True), y_pred = tensor([42.77173, 34.07975, 25.38776, 16.69578], grad_fn=<MvBackward>), loss = 0.66944206\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 150: w = tensor([1.62588, 1.02703], requires_grad=True), y_pred = tensor([42.55506, 33.94321, 25.33137, 16.71952], grad_fn=<MvBackward>), loss = 0.59950674\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 155: w = tensor([1.64540, 1.02219], requires_grad=True), y_pred = tensor([42.66828, 34.05339, 25.43850, 16.82362], grad_fn=<MvBackward>), loss = 0.53715217\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 160: w = tensor([1.66429, 1.02378], requires_grad=True), y_pred = tensor([42.51758, 33.96376, 25.40994, 16.85612], grad_fn=<MvBackward>), loss = 0.48146015\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 165: w = tensor([1.68186, 1.02029], requires_grad=True), y_pred = tensor([42.58377, 34.03600, 25.48822, 16.94045], grad_fn=<MvBackward>), loss = 0.43166173\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 170: w = tensor([1.69877, 1.02103], requires_grad=True), y_pred = tensor([42.47721, 33.97713, 25.47704, 16.97695], grad_fn=<MvBackward>), loss = 0.38709140\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 175: w = tensor([1.71457, 1.01845], requires_grad=True), y_pred = tensor([42.51338, 34.02448, 25.53557, 17.04667], grad_fn=<MvBackward>), loss = 0.34717295\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 180: w = tensor([1.72971, 1.01867], requires_grad=True), y_pred = tensor([42.43659, 33.98581, 25.53503, 17.08424], grad_fn=<MvBackward>), loss = 0.31140596\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 185: w = tensor([1.74391, 1.01672], requires_grad=True), y_pred = tensor([42.45380, 34.01683, 25.57986, 17.14289], grad_fn=<MvBackward>), loss = 0.27934441\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 190: w = tensor([1.75748, 1.01662], requires_grad=True), y_pred = tensor([42.39725, 33.99142, 25.58560, 17.17977], grad_fn=<MvBackward>), loss = 0.25059801\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 195: w = tensor([1.77024, 1.01511], requires_grad=True), y_pred = tensor([42.40269, 34.01173, 25.62078, 17.22982], grad_fn=<MvBackward>), loss = 0.22481927\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 200: w = tensor([1.78240, 1.01482], requires_grad=True), y_pred = tensor([42.36006, 33.99504, 25.63001, 17.26499], grad_fn=<MvBackward>), loss = 0.20169993\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 205: w = tensor([1.79385, 1.01363], requires_grad=True), y_pred = tensor([42.35838, 34.00832, 25.65827, 17.30822], grad_fn=<MvBackward>), loss = 0.18096074\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 210: w = tensor([1.80476, 1.01324], requires_grad=True), y_pred = tensor([42.32542, 33.99732, 25.66922, 17.34113], grad_fn=<MvBackward>), loss = 0.16235729\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 215: w = tensor([1.81504, 1.01227], requires_grad=True), y_pred = tensor([42.31963, 34.00603, 25.69242, 17.37882], grad_fn=<MvBackward>), loss = 0.14566770\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 220: w = tensor([1.82482, 1.01185], requires_grad=True), y_pred = tensor([42.29353, 33.99876, 25.70400, 17.40924], grad_fn=<MvBackward>), loss = 0.13069558\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 225: w = tensor([1.83405, 1.01104], requires_grad=True), y_pred = tensor([42.28552, 34.00446, 25.72339, 17.44233], grad_fn=<MvBackward>), loss = 0.11726224\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 230: w = tensor([1.84282, 1.01060], requires_grad=True), y_pred = tensor([42.26440, 33.99967, 25.73494, 17.47021], grad_fn=<MvBackward>), loss = 0.10521053\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 235: w = tensor([1.85111, 1.00993], requires_grad=True), y_pred = tensor([42.25534, 34.00337, 25.75140, 17.49942], grad_fn=<MvBackward>), loss = 0.09439759\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 240: w = tensor([1.85897, 1.00950], requires_grad=True), y_pred = tensor([42.23791, 34.00021, 25.76252, 17.52483], grad_fn=<MvBackward>), loss = 0.08469677\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 245: w = tensor([1.86641, 1.00892], requires_grad=True), y_pred = tensor([42.22856, 34.00261, 25.77667, 17.55073], grad_fn=<MvBackward>), loss = 0.07599217\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 250: w = tensor([1.87346, 1.00851], requires_grad=True), y_pred = tensor([42.21390, 34.00052, 25.78715, 17.57377], grad_fn=<MvBackward>), loss = 0.06818256\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 255: w = tensor([1.88014, 1.00801], requires_grad=True), y_pred = tensor([42.20472, 34.00208, 25.79945, 17.59681], grad_fn=<MvBackward>), loss = 0.06117591\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 260: w = tensor([1.88647, 1.00763], requires_grad=True), y_pred = tensor([42.19220, 34.00068, 25.80916, 17.61764], grad_fn=<MvBackward>), loss = 0.05488885\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 265: w = tensor([1.89246, 1.00719], requires_grad=True), y_pred = tensor([42.18345, 34.00169, 25.81994, 17.63818], grad_fn=<MvBackward>), loss = 0.04924808\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 270: w = tensor([1.89813, 1.00684], requires_grad=True), y_pred = tensor([42.17264, 34.00076, 25.82887, 17.65699], grad_fn=<MvBackward>), loss = 0.04418735\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 275: w = tensor([1.90351, 1.00646], requires_grad=True), y_pred = tensor([42.16444, 34.00140, 25.83836, 17.67532], grad_fn=<MvBackward>), loss = 0.03964623\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 280: w = tensor([1.90860, 1.00614], requires_grad=True), y_pred = tensor([42.15502, 34.00077, 25.84652, 17.69227], grad_fn=<MvBackward>), loss = 0.03557184\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 285: w = tensor([1.91342, 1.00580], requires_grad=True), y_pred = tensor([42.14744, 34.00118, 25.85492, 17.70866], grad_fn=<MvBackward>), loss = 0.03191639\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 290: w = tensor([1.91799, 1.00550], requires_grad=True), y_pred = tensor([42.13918, 34.00076, 25.86233, 17.72391], grad_fn=<MvBackward>), loss = 0.02863660\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 295: w = tensor([1.92232, 1.00520], requires_grad=True), y_pred = tensor([42.13222, 34.00101, 25.86980, 17.73858], grad_fn=<MvBackward>), loss = 0.02569385\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 300: w = tensor([1.92642, 1.00494], requires_grad=True), y_pred = tensor([42.12493, 34.00072, 25.87651, 17.75230], grad_fn=<MvBackward>), loss = 0.02305340\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 305: w = tensor([1.93030, 1.00467], requires_grad=True), y_pred = tensor([42.11858, 34.00087, 25.88315, 17.76544], grad_fn=<MvBackward>), loss = 0.02068412\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 310: w = tensor([1.93398, 1.00443], requires_grad=True), y_pred = tensor([42.11213, 34.00067, 25.88922, 17.77776], grad_fn=<MvBackward>), loss = 0.01855873\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 315: w = tensor([1.93747, 1.00419], requires_grad=True), y_pred = tensor([42.10637, 34.00076, 25.89515, 17.78953], grad_fn=<MvBackward>), loss = 0.01665138\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 320: w = tensor([1.94077, 1.00397], requires_grad=True), y_pred = tensor([42.10064, 34.00063, 25.90062, 17.80061], grad_fn=<MvBackward>), loss = 0.01494033\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 325: w = tensor([1.94389, 1.00376], requires_grad=True), y_pred = tensor([42.09541, 34.00066, 25.90591, 17.81116], grad_fn=<MvBackward>), loss = 0.01340491\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 330: w = tensor([1.94685, 1.00356], requires_grad=True), y_pred = tensor([42.09031, 34.00057, 25.91084, 17.82110], grad_fn=<MvBackward>), loss = 0.01202754\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 335: w = tensor([1.94966, 1.00337], requires_grad=True), y_pred = tensor([42.08558, 34.00058, 25.91557, 17.83055], grad_fn=<MvBackward>), loss = 0.01079139\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 340: w = tensor([1.95231, 1.00320], requires_grad=True), y_pred = tensor([42.08104, 34.00053, 25.92001, 17.83949], grad_fn=<MvBackward>), loss = 0.00968240\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 345: w = tensor([1.95483, 1.00303], requires_grad=True), y_pred = tensor([42.07679, 34.00052, 25.92424, 17.84797], grad_fn=<MvBackward>), loss = 0.00868748\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 350: w = tensor([1.95722, 1.00287], requires_grad=True), y_pred = tensor([42.07271, 34.00047, 25.92823, 17.85599], grad_fn=<MvBackward>), loss = 0.00779471\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 355: w = tensor([1.95947, 1.00272], requires_grad=True), y_pred = tensor([42.06890, 34.00047, 25.93203, 17.86359], grad_fn=<MvBackward>), loss = 0.00699359\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 360: w = tensor([1.96161, 1.00257], requires_grad=True), y_pred = tensor([42.06524, 34.00042, 25.93560, 17.87078], grad_fn=<MvBackward>), loss = 0.00627505\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 365: w = tensor([1.96364, 1.00244], requires_grad=True), y_pred = tensor([42.06182, 34.00042, 25.93901, 17.87761], grad_fn=<MvBackward>), loss = 0.00563018\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 370: w = tensor([1.96556, 1.00231], requires_grad=True), y_pred = tensor([42.05854, 34.00038, 25.94222, 17.88406], grad_fn=<MvBackward>), loss = 0.00505164\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 375: w = tensor([1.96737, 1.00219], requires_grad=True), y_pred = tensor([42.05547, 34.00037, 25.94528, 17.89019], grad_fn=<MvBackward>), loss = 0.00453248\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 380: w = tensor([1.96910, 1.00207], requires_grad=True), y_pred = tensor([42.05252, 34.00034, 25.94816, 17.89598], grad_fn=<MvBackward>), loss = 0.00406663\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 385: w = tensor([1.97073, 1.00196], requires_grad=True), y_pred = tensor([42.04977, 34.00034, 25.95090, 17.90147], grad_fn=<MvBackward>), loss = 0.00364873\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 390: w = tensor([1.97227, 1.00186], requires_grad=True), y_pred = tensor([42.04712, 34.00031, 25.95349, 17.90667], grad_fn=<MvBackward>), loss = 0.00327375\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 395: w = tensor([1.97374, 1.00176], requires_grad=True), y_pred = tensor([42.04465, 34.00030, 25.95595, 17.91160], grad_fn=<MvBackward>), loss = 0.00293732\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 400: w = tensor([1.97512, 1.00167], requires_grad=True), y_pred = tensor([42.04229, 34.00028, 25.95827, 17.91626], grad_fn=<MvBackward>), loss = 0.00263548\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 405: w = tensor([1.97643, 1.00158], requires_grad=True), y_pred = tensor([42.04005, 34.00026, 25.96047, 17.92068], grad_fn=<MvBackward>), loss = 0.00236459\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 410: w = tensor([1.97768, 1.00150], requires_grad=True), y_pred = tensor([42.03794, 34.00025, 25.96256, 17.92487], grad_fn=<MvBackward>), loss = 0.00212172\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 415: w = tensor([1.97886, 1.00142], requires_grad=True), y_pred = tensor([42.03594, 34.00024, 25.96453, 17.92883], grad_fn=<MvBackward>), loss = 0.00190360\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 420: w = tensor([1.97997, 1.00134], requires_grad=True), y_pred = tensor([42.03405, 34.00023, 25.96641, 17.93259], grad_fn=<MvBackward>), loss = 0.00170798\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 425: w = tensor([1.98103, 1.00127], requires_grad=True), y_pred = tensor([42.03225, 34.00022, 25.96818, 17.93615], grad_fn=<MvBackward>), loss = 0.00153252\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 430: w = tensor([1.98203, 1.00120], requires_grad=True), y_pred = tensor([42.03054, 34.00020, 25.96986, 17.93951], grad_fn=<MvBackward>), loss = 0.00137501\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 435: w = tensor([1.98298, 1.00114], requires_grad=True), y_pred = tensor([42.02893, 34.00019, 25.97145, 17.94271], grad_fn=<MvBackward>), loss = 0.00123367\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 440: w = tensor([1.98388, 1.00108], requires_grad=True), y_pred = tensor([42.02741, 34.00019, 25.97296, 17.94573], grad_fn=<MvBackward>), loss = 0.00110684\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 445: w = tensor([1.98473, 1.00102], requires_grad=True), y_pred = tensor([42.02594, 34.00016, 25.97437, 17.94859], grad_fn=<MvBackward>), loss = 0.00099315\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 450: w = tensor([1.98553, 1.00097], requires_grad=True), y_pred = tensor([42.02460, 34.00017, 25.97574, 17.95131], grad_fn=<MvBackward>), loss = 0.00089105\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 455: w = tensor([1.98630, 1.00092], requires_grad=True), y_pred = tensor([42.02329, 34.00015, 25.97701, 17.95388], grad_fn=<MvBackward>), loss = 0.00079952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 460: w = tensor([1.98702, 1.00087], requires_grad=True), y_pred = tensor([42.02205, 34.00014, 25.97823, 17.95631], grad_fn=<MvBackward>), loss = 0.00071731\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 465: w = tensor([1.98771, 1.00082], requires_grad=True), y_pred = tensor([42.02092, 34.00015, 25.97939, 17.95862], grad_fn=<MvBackward>), loss = 0.00064363\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 470: w = tensor([1.98835, 1.00078], requires_grad=True), y_pred = tensor([42.01978, 34.00011, 25.98046, 17.96080], grad_fn=<MvBackward>), loss = 0.00057748\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 475: w = tensor([1.98897, 1.00074], requires_grad=True), y_pred = tensor([42.01876, 34.00014, 25.98150, 17.96288], grad_fn=<MvBackward>), loss = 0.00051811\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 480: w = tensor([1.98955, 1.00070], requires_grad=True), y_pred = tensor([42.01775, 34.00011, 25.98247, 17.96483], grad_fn=<MvBackward>), loss = 0.00046487\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 485: w = tensor([1.99010, 1.00066], requires_grad=True), y_pred = tensor([42.01683, 34.00012, 25.98340, 17.96669], grad_fn=<MvBackward>), loss = 0.00041709\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 490: w = tensor([1.99063, 1.00063], requires_grad=True), y_pred = tensor([42.01592, 34.00009, 25.98427, 17.96844], grad_fn=<MvBackward>), loss = 0.00037425\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 495: w = tensor([1.99112, 1.00060], requires_grad=True), y_pred = tensor([42.01510, 34.00010, 25.98511, 17.97011], grad_fn=<MvBackward>), loss = 0.00033581\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 500: w = tensor([1.99159, 1.00056], requires_grad=True), y_pred = tensor([42.01431, 34.00010, 25.98589, 17.97169], grad_fn=<MvBackward>), loss = 0.00030129\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 505: w = tensor([1.99203, 1.00053], requires_grad=True), y_pred = tensor([42.01354, 34.00008, 25.98663, 17.97318], grad_fn=<MvBackward>), loss = 0.00027033\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 510: w = tensor([1.99245, 1.00051], requires_grad=True), y_pred = tensor([42.01283, 34.00009, 25.98734, 17.97460], grad_fn=<MvBackward>), loss = 0.00024254\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 515: w = tensor([1.99285, 1.00048], requires_grad=True), y_pred = tensor([42.01215, 34.00007, 25.98800, 17.97593], grad_fn=<MvBackward>), loss = 0.00021764\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 520: w = tensor([1.99323, 1.00045], requires_grad=True), y_pred = tensor([42.01151, 34.00008, 25.98864, 17.97721], grad_fn=<MvBackward>), loss = 0.00019526\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 525: w = tensor([1.99359, 1.00043], requires_grad=True), y_pred = tensor([42.01090, 34.00007, 25.98924, 17.97841], grad_fn=<MvBackward>), loss = 0.00017518\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 530: w = tensor([1.99392, 1.00041], requires_grad=True), y_pred = tensor([42.01033, 34.00007, 25.98981, 17.97955], grad_fn=<MvBackward>), loss = 0.00015719\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 535: w = tensor([1.99425, 1.00039], requires_grad=True), y_pred = tensor([42.00978, 34.00006, 25.99035, 17.98063], grad_fn=<MvBackward>), loss = 0.00014103\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 540: w = tensor([1.99455, 1.00037], requires_grad=True), y_pred = tensor([42.00927, 34.00006, 25.99086, 17.98165], grad_fn=<MvBackward>), loss = 0.00012652\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 545: w = tensor([1.99484, 1.00035], requires_grad=True), y_pred = tensor([42.00878, 34.00006, 25.99134, 17.98262], grad_fn=<MvBackward>), loss = 0.00011353\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 550: w = tensor([1.99511, 1.00033], requires_grad=True), y_pred = tensor([42.00831, 34.00005, 25.99179, 17.98354], grad_fn=<MvBackward>), loss = 0.00010187\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 555: w = tensor([1.99537, 1.00031], requires_grad=True), y_pred = tensor([42.00787, 34.00005, 25.99223, 17.98441], grad_fn=<MvBackward>), loss = 0.00009140\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 560: w = tensor([1.99561, 1.00029], requires_grad=True), y_pred = tensor([42.00747, 34.00006, 25.99264, 17.98523], grad_fn=<MvBackward>), loss = 0.00008202\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 565: w = tensor([1.99584, 1.00028], requires_grad=True), y_pred = tensor([42.00705, 34.00004, 25.99302, 17.98600], grad_fn=<MvBackward>), loss = 0.00007359\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 570: w = tensor([1.99606, 1.00026], requires_grad=True), y_pred = tensor([42.00670, 34.00005, 25.99340, 17.98675], grad_fn=<MvBackward>), loss = 0.00006602\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 575: w = tensor([1.99627, 1.00025], requires_grad=True), y_pred = tensor([42.00633, 34.00003, 25.99374, 17.98744], grad_fn=<MvBackward>), loss = 0.00005925\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 580: w = tensor([1.99647, 1.00024], requires_grad=True), y_pred = tensor([42.00601, 34.00005, 25.99408, 17.98811], grad_fn=<MvBackward>), loss = 0.00005316\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 585: w = tensor([1.99665, 1.00022], requires_grad=True), y_pred = tensor([42.00569, 34.00003, 25.99439, 17.98874], grad_fn=<MvBackward>), loss = 0.00004769\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 590: w = tensor([1.99683, 1.00021], requires_grad=True), y_pred = tensor([42.00538, 34.00003, 25.99468, 17.98933], grad_fn=<MvBackward>), loss = 0.00004279\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 595: w = tensor([1.99700, 1.00020], requires_grad=True), y_pred = tensor([42.00511, 34.00004, 25.99497, 17.98989], grad_fn=<MvBackward>), loss = 0.00003839\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 600: w = tensor([1.99716, 1.00019], requires_grad=True), y_pred = tensor([42.00483, 34.00003, 25.99523, 17.99043], grad_fn=<MvBackward>), loss = 0.00003444\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 605: w = tensor([1.99731, 1.00018], requires_grad=True), y_pred = tensor([42.00458, 34.00003, 25.99548, 17.99093], grad_fn=<MvBackward>), loss = 0.00003091\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 610: w = tensor([1.99745, 1.00017], requires_grad=True), y_pred = tensor([42.00435, 34.00003, 25.99572, 17.99141], grad_fn=<MvBackward>), loss = 0.00002773\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 615: w = tensor([1.99758, 1.00016], requires_grad=True), y_pred = tensor([42.00410, 34.00002, 25.99594, 17.99186], grad_fn=<MvBackward>), loss = 0.00002489\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 620: w = tensor([1.99771, 1.00015], requires_grad=True), y_pred = tensor([42.00390, 34.00003, 25.99616, 17.99229], grad_fn=<MvBackward>), loss = 0.00002233\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 625: w = tensor([1.99783, 1.00015], requires_grad=True), y_pred = tensor([42.00368, 34.00002, 25.99636, 17.99270], grad_fn=<MvBackward>), loss = 0.00002004\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 630: w = tensor([1.99795, 1.00014], requires_grad=True), y_pred = tensor([42.00349, 34.00002, 25.99655, 17.99308], grad_fn=<MvBackward>), loss = 0.00001798\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 635: w = tensor([1.99805, 1.00013], requires_grad=True), y_pred = tensor([42.00330, 34.00002, 25.99673, 17.99345], grad_fn=<MvBackward>), loss = 0.00001613\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 640: w = tensor([1.99816, 1.00012], requires_grad=True), y_pred = tensor([42.00313, 34.00002, 25.99691, 17.99379], grad_fn=<MvBackward>), loss = 0.00001448\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 645: w = tensor([1.99825, 1.00012], requires_grad=True), y_pred = tensor([42.00298, 34.00003, 25.99708, 17.99413], grad_fn=<MvBackward>), loss = 0.00001298\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 650: w = tensor([1.99835, 1.00011], requires_grad=True), y_pred = tensor([42.00280, 34.00001, 25.99722, 17.99443], grad_fn=<MvBackward>), loss = 0.00001165\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 655: w = tensor([1.99843, 1.00010], requires_grad=True), y_pred = tensor([42.00267, 34.00002, 25.99738, 17.99473], grad_fn=<MvBackward>), loss = 0.00001045\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 660: w = tensor([1.99852, 1.00010], requires_grad=True), y_pred = tensor([42.00251, 34.00001, 25.99751, 17.99500], grad_fn=<MvBackward>), loss = 0.00000938\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 665: w = tensor([1.99859, 1.00009], requires_grad=True), y_pred = tensor([42.00239, 34.00002, 25.99764, 17.99527], grad_fn=<MvBackward>), loss = 0.00000841\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 670: w = tensor([1.99867, 1.00009], requires_grad=True), y_pred = tensor([42.00225, 34.00001, 25.99776, 17.99552], grad_fn=<MvBackward>), loss = 0.00000755\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 675: w = tensor([1.99874, 1.00008], requires_grad=True), y_pred = tensor([42.00216, 34.00003, 25.99789, 17.99576], grad_fn=<MvBackward>), loss = 0.00000678\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 680: w = tensor([1.99881, 1.00008], requires_grad=True), y_pred = tensor([42.00201, 34.00000, 25.99799, 17.99597], grad_fn=<MvBackward>), loss = 0.00000608\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 685: w = tensor([1.99887, 1.00008], requires_grad=True), y_pred = tensor([42.00194, 34.00002, 25.99811, 17.99619], grad_fn=<MvBackward>), loss = 0.00000546\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 690: w = tensor([1.99893, 1.00007], requires_grad=True), y_pred = tensor([42.00181, 34.00000, 25.99820, 17.99639], grad_fn=<MvBackward>), loss = 0.00000489\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 695: w = tensor([1.99898, 1.00007], requires_grad=True), y_pred = tensor([42.00173, 34.00002, 25.99830, 17.99658], grad_fn=<MvBackward>), loss = 0.00000439\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 700: w = tensor([1.99904, 1.00006], requires_grad=True), y_pred = tensor([42.00164, 34.00002, 25.99839, 17.99677], grad_fn=<MvBackward>), loss = 0.00000394\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 705: w = tensor([1.99909, 1.00006], requires_grad=True), y_pred = tensor([42.00154, 34.00001, 25.99847, 17.99693], grad_fn=<MvBackward>), loss = 0.00000353\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 710: w = tensor([1.99914, 1.00006], requires_grad=True), y_pred = tensor([42.00146, 34.00000, 25.99855, 17.99709], grad_fn=<MvBackward>), loss = 0.00000317\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 715: w = tensor([1.99918, 1.00005], requires_grad=True), y_pred = tensor([42.00139, 34.00002, 25.99863, 17.99725], grad_fn=<MvBackward>), loss = 0.00000284\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 720: w = tensor([1.99923, 1.00005], requires_grad=True), y_pred = tensor([42.00131, 34.00000, 25.99870, 17.99739], grad_fn=<MvBackward>), loss = 0.00000255\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 725: w = tensor([1.99927, 1.00005], requires_grad=True), y_pred = tensor([42.00126, 34.00002, 25.99878, 17.99753], grad_fn=<MvBackward>), loss = 0.00000229\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 730: w = tensor([1.99931, 1.00005], requires_grad=True), y_pred = tensor([42.00117, 34.00000, 25.99883, 17.99766], grad_fn=<MvBackward>), loss = 0.00000206\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 735: w = tensor([1.99934, 1.00004], requires_grad=True), y_pred = tensor([42.00113, 34.00002, 25.99890, 17.99779], grad_fn=<MvBackward>), loss = 0.00000184\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 740: w = tensor([1.99938, 1.00004], requires_grad=True), y_pred = tensor([42.00105, 34.00000, 25.99895, 17.99790], grad_fn=<MvBackward>), loss = 0.00000166\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 745: w = tensor([1.99941, 1.00004], requires_grad=True), y_pred = tensor([42.00101, 34.00001, 25.99901, 17.99801], grad_fn=<MvBackward>), loss = 0.00000149\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 750: w = tensor([1.99944, 1.00004], requires_grad=True), y_pred = tensor([42.00096, 34.00001, 25.99907, 17.99812], grad_fn=<MvBackward>), loss = 0.00000133\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 755: w = tensor([1.99947, 1.00004], requires_grad=True), y_pred = tensor([42.00090, 34.00000, 25.99911, 17.99822], grad_fn=<MvBackward>), loss = 0.00000119\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 760: w = tensor([1.99950, 1.00003], requires_grad=True), y_pred = tensor([42.00086, 34.00001, 25.99916, 17.99831], grad_fn=<MvBackward>), loss = 0.00000107\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 765: w = tensor([1.99952, 1.00003], requires_grad=True), y_pred = tensor([42.00081, 34.00000, 25.99920, 17.99840], grad_fn=<MvBackward>), loss = 0.00000096\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 770: w = tensor([1.99955, 1.00003], requires_grad=True), y_pred = tensor([42.00076, 34.00000, 25.99924, 17.99848], grad_fn=<MvBackward>), loss = 0.00000086\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 775: w = tensor([1.99957, 1.00003], requires_grad=True), y_pred = tensor([42.00073, 34.00001, 25.99929, 17.99857], grad_fn=<MvBackward>), loss = 0.00000077\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 780: w = tensor([1.99960, 1.00003], requires_grad=True), y_pred = tensor([42.00068, 34.00000, 25.99932, 17.99864], grad_fn=<MvBackward>), loss = 0.00000069\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 785: w = tensor([1.99962, 1.00003], requires_grad=True), y_pred = tensor([42.00065, 34.00001, 25.99936, 17.99871], grad_fn=<MvBackward>), loss = 0.00000062\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 790: w = tensor([1.99964, 1.00002], requires_grad=True), y_pred = tensor([42.00062, 34.00001, 25.99940, 17.99878], grad_fn=<MvBackward>), loss = 0.00000056\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 795: w = tensor([1.99966, 1.00002], requires_grad=True), y_pred = tensor([42.00059, 34.00001, 25.99943, 17.99885], grad_fn=<MvBackward>), loss = 0.00000050\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 800: w = tensor([1.99967, 1.00002], requires_grad=True), y_pred = tensor([42.00055, 34.00000, 25.99945, 17.99891], grad_fn=<MvBackward>), loss = 0.00000045\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 805: w = tensor([1.99969, 1.00002], requires_grad=True), y_pred = tensor([42.00053, 34.00001, 25.99949, 17.99896], grad_fn=<MvBackward>), loss = 0.00000040\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 810: w = tensor([1.99971, 1.00002], requires_grad=True), y_pred = tensor([42.00049, 34.00000, 25.99951, 17.99902], grad_fn=<MvBackward>), loss = 0.00000036\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 815: w = tensor([1.99972, 1.00002], requires_grad=True), y_pred = tensor([42.00047, 34.00000, 25.99954, 17.99907], grad_fn=<MvBackward>), loss = 0.00000033\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 820: w = tensor([1.99974, 1.00002], requires_grad=True), y_pred = tensor([42.00045, 34.00000, 25.99956, 17.99912], grad_fn=<MvBackward>), loss = 0.00000029\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 825: w = tensor([1.99975, 1.00002], requires_grad=True), y_pred = tensor([42.00042, 34.00000, 25.99958, 17.99916], grad_fn=<MvBackward>), loss = 0.00000026\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 830: w = tensor([1.99977, 1.00002], requires_grad=True), y_pred = tensor([42.00040, 34.00000, 25.99960, 17.99921], grad_fn=<MvBackward>), loss = 0.00000024\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 835: w = tensor([1.99978, 1.00001], requires_grad=True), y_pred = tensor([42.00038, 34.00000, 25.99963, 17.99925], grad_fn=<MvBackward>), loss = 0.00000021\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 840: w = tensor([1.99979, 1.00001], requires_grad=True), y_pred = tensor([42.00035, 34.00000, 25.99965, 17.99929], grad_fn=<MvBackward>), loss = 0.00000019\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 845: w = tensor([1.99980, 1.00001], requires_grad=True), y_pred = tensor([42.00034, 34.00000, 25.99967, 17.99933], grad_fn=<MvBackward>), loss = 0.00000017\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 850: w = tensor([1.99981, 1.00001], requires_grad=True), y_pred = tensor([42.00032, 34.00000, 25.99968, 17.99936], grad_fn=<MvBackward>), loss = 0.00000015\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 855: w = tensor([1.99982, 1.00001], requires_grad=True), y_pred = tensor([42.00030, 34.00000, 25.99970, 17.99940], grad_fn=<MvBackward>), loss = 0.00000014\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 860: w = tensor([1.99983, 1.00001], requires_grad=True), y_pred = tensor([42.00028, 33.99999, 25.99971, 17.99943], grad_fn=<MvBackward>), loss = 0.00000012\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 865: w = tensor([1.99984, 1.00001], requires_grad=True), y_pred = tensor([42.00027, 34.00000, 25.99973, 17.99946], grad_fn=<MvBackward>), loss = 0.00000011\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 870: w = tensor([1.99985, 1.00001], requires_grad=True), y_pred = tensor([42.00026, 34.00000, 25.99975, 17.99949], grad_fn=<MvBackward>), loss = 0.00000010\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 875: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00023, 33.99999, 25.99975, 17.99951], grad_fn=<MvBackward>), loss = 0.00000009\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 880: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00001, 25.99978, 17.99954], grad_fn=<MvBackward>), loss = 0.00000008\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 885: w = tensor([1.99987, 1.00001], requires_grad=True), y_pred = tensor([42.00021, 33.99999, 25.99978, 17.99956], grad_fn=<MvBackward>), loss = 0.00000007\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 890: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00002, 25.99981, 17.99960], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 895: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 33.99998, 25.99980, 17.99961], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 900: w = tensor([1.99989, 1.00001], requires_grad=True), y_pred = tensor([42.00020, 34.00001, 25.99982, 17.99963], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 905: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 33.99999, 25.99982, 17.99965], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 910: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 34.00001, 25.99984, 17.99967], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 915: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 34.00000, 25.99985, 17.99969], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 920: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00015, 34.00000, 25.99985, 17.99970], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 925: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00014, 34.00000, 25.99986, 17.99972], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 930: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00013, 33.99999, 25.99986, 17.99973], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 935: w = tensor([1.99992, 1.00000], requires_grad=True), y_pred = tensor([42.00013, 34.00000, 25.99988, 17.99975], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 940: w = tensor([1.99993, 1.00001], requires_grad=True), y_pred = tensor([42.00011, 33.99999, 25.99988, 17.99976], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 945: w = tensor([1.99993, 1.00000], requires_grad=True), y_pred = tensor([42.00012, 34.00000, 25.99989, 17.99977], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 950: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99989, 17.99978], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 955: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00011, 34.00000, 25.99990, 17.99980], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 960: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99981], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 965: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 970: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 33.99999, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 975: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00001, 25.99993, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 980: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99992, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 985: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 34.00001, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 990: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 995: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00006, 34.00000, 25.99993, 17.99987], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "#--------------------\n",
    "# 0) Training samples\n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "#--------------------\n",
    "# 1) Design Model: Weights to optimize and forward function\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "#--------------------\n",
    "# 2) Define loss and optimizer\n",
    "\n",
    "# callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "# def loss(y, y_pred):\n",
    "#     return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "learning_rate = 0.0013\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "\n",
    "#--------------------\n",
    "# 3) Training loop\n",
    "# основной цикл:\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "         \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "Использование модели:\n",
    "\n",
    "`torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "Applies a linear transformation to the incoming data: $y = xA^T + b$\n",
    "\n",
    "Parameters:\n",
    "* `in_features` – size of each input sample\n",
    "* `out_features` – size of each output sample\n",
    "* `bias` – If set to False, the layer will not learn an additive bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "Prediction before training: f(5) = tensor([[6.26964],\n",
      "        [4.58868],\n",
      "        [2.90772],\n",
      "        [1.22675]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# X_test = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) # torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(n_samples, n_features)\n",
    "\n",
    "# n_samples, n_features = X_test.shape\n",
    "# input_size = n_features\n",
    "# output_size = n_features\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(n_features, 1)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-cf290153e199>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([4, 2])\n",
      "Y.shape = torch.Size([4])\n",
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "Prediction before training: f(tensor([[ 1., 40.],\n",
      "        [ 2., 30.],\n",
      "        [ 3., 20.],\n",
      "        [ 4., 10.]])) = tensor([[-24.89012],\n",
      "        [-18.63989],\n",
      "        [-12.38966],\n",
      "        [ -6.13942]], grad_fn=<MmBackward>)\n",
      "epoch 0: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 5: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 10: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 15: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 20: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 25: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 30: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 35: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 40: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 45: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 50: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 55: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 60: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 65: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 70: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 75: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 80: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 85: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 90: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 95: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 100: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 105: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 110: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 115: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 120: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 125: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 130: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 135: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 140: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 145: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 150: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 155: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 160: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 165: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 170: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 175: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 180: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 185: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 190: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 195: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 200: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 205: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 210: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 215: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 220: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 225: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 230: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 235: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 240: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 245: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 250: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 255: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 260: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 265: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 270: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 275: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 280: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 285: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 290: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 295: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 300: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 305: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 310: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 315: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 320: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 325: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 330: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 335: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 340: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 345: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 350: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 355: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 360: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 365: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 370: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 375: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 380: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 385: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 390: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 395: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 400: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 405: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 410: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 415: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 420: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 425: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 430: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 435: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 440: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 445: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 450: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 455: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 460: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpha\\.conda\\envs\\pyTorch_1_5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:432: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 465: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 470: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 475: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 480: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 485: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 490: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 495: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 500: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 505: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 510: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 515: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 520: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 525: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 530: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 535: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 540: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 545: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 550: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 555: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 560: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 565: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 570: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 575: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 580: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 585: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 590: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 595: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 600: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 605: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 610: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 615: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 620: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 625: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 630: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 635: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 640: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 645: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 650: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 655: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 660: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 665: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 670: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 675: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 680: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 685: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 690: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 695: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 700: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 705: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 710: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 715: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 720: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 725: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 730: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 735: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 740: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 745: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 750: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 755: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 760: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 765: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 770: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 775: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 780: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 785: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 790: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 795: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 800: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 805: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 810: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 815: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 820: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 825: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 830: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 835: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 840: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 845: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 850: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 855: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 860: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 865: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 870: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 875: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 880: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 885: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 890: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 895: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 900: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 905: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 910: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 915: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 920: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 925: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 930: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 935: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 940: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 945: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 950: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 955: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 960: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 965: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 970: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 975: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 980: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 985: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 990: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 995: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00029,  0.00038])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "#--------------------\n",
    "# 0) Training samples\n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "print(f'X.shape = {X.shape}')\n",
    "X_samples, X_features = X.shape\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "print(f'Y.shape = {Y.shape}')\n",
    "Y_features = 1\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "#--------------------\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(X_features, Y_features)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f({X}) = {model(X)}')\n",
    "\n",
    "\n",
    "# # model (модель, в нашем случае: линейная регрессия)\n",
    "# # прямое распространение:\n",
    "# def forward(X):\n",
    "#     return X @ w # Size([4])\n",
    "\n",
    "#--------------------\n",
    "# 2) Define loss and optimizer\n",
    "\n",
    "# callable function\n",
    "criterion  = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.0013\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "#--------------------\n",
    "# 3) Training loop\n",
    "# основной цикл:\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, Y)\n",
    "\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Спасибо за внимание!\n",
    "\n",
    "---\n",
    "### Технический раздел:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * И Введение в искусственные нейронные сети\n",
    "     * Базовые понятия и история\n",
    " * И Машинное обучение и концепция глубокого обучения\n",
    " * И Почему глубокое обучение начало приносить плоды и активно использоваться только после 2010 г?\n",
    "     * Производительность оборудования\n",
    "     * Доступность наборов данных и тестов\n",
    "     * Алгоритмические достижения в области глубокого обучения\n",
    "         * Улчшенные подходы к регуляризации\n",
    "         * Улучшенные схемы инициализации весов\n",
    "         * (повтор) Усовершенствованные методы градиентного супска\n",
    "         \n",
    "\n",
    "* Обратное распространение ошибки\n",
    " * Оптимизация\n",
    "     * Стохастический градиентный спуск\n",
    "     * Усовершенствованные методы градиентного супска\n",
    "* Введение в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> next <em class=\"qs\"></em> qs line \n",
    "<br/> next <em class=\"an\"></em> an line \n",
    "<br/> next <em class=\"nt\"></em> an line \n",
    "<br/> next <em class=\"df\"></em> df line \n",
    "<br/> next <em class=\"ex\"></em> ex line \n",
    "<br/> next <em class=\"pl\"></em> pl line \n",
    "<br/> next <em class=\"mn\"></em> mn line \n",
    "<br/> next <em class=\"plmn\"></em> plmn line \n",
    "<br/> next <em class=\"hn\"></em> hn line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Работа с графом потока вычислений нужна  для того, чтобы решить __задачу обучения многослойной ИНС__. А эта задача требует после получения резуьтатов и оценки ошибки __выполнения обратного прохода__ дающего градиент ошибки для весов (параметров) модели и последующей процедуры оптимизации весов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<img src=\"./img/ker_7.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "<img src=\"./img/ker_8.png\" alt=\"\" style=\"width: 500px;\"/>    \n",
    "<img src=\"./img/ker_9.png\" alt=\"\" style=\"width: 500px;\"/>        \n",
    "<img src=\"./img/ker_10.png\" alt=\"\" style=\"width: 500px;\"/>        \n",
    "<img src=\"./img/ker_11.png\" alt=\"\" style=\"width: 500px;\"/>            \n",
    "<img src=\"./img/ker_12.png\" alt=\"\" style=\"width: 500px;\"/>                \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch_1_5v2",
   "language": "python",
   "name": "pytorch_1_5v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
